{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083c9776",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604cc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae1095",
   "metadata": {},
   "source": [
    "### Text Splitter (chunking strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a85dd34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and chunked the book content from the PDF with page numbers.\n",
      "Filtered chunks for BERT pre-training quality.\n",
      "Total number of valid chunks created: 134\n",
      "\n",
      "Here is the content of the first chunk:\n",
      "---------------------------------------\n",
      "Upon a laboring day without the sign\n",
      " Of your profession?—Speak, what trade art thou?\n",
      "  Why, sir, a carpenter.\n",
      " \n",
      " Where is thy leather apron and thy rule?\n",
      " What dost thou with thy best apparel on?—\n",
      " You, sir, what trade are you?\n",
      "  Truly, sir, in respect of a fine workman, I am\n",
      " but, as you would say, a cobbler.\n",
      " \n",
      " But what trade art thou? Answer me directly.\n",
      "  A trade, sir, that I hope I may use with a safe\n",
      " conscience, which is indeed, sir, a mender of bad\n",
      " soles.\n",
      "---------------------------------------\n",
      "First chunk metadata: {'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 9, 'c': 'rule_based', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "file_path = \"../julius-caesar_PDF_FolgerShakespeare.pdf\"\n",
    "docs_rule = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "\n",
    "            # Initialize a text splitter for this page.\n",
    "            # We will split the text from one page and add the page number as metadata to each chunk.\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=200,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            \n",
    "            # Split the text from the current page into chunks\n",
    "            page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "            # Filter and add metadata to each valid chunk\n",
    "            for chunk in page_chunks:\n",
    "                # Validate chunk for BERT pre-training\n",
    "                if is_valid_chunk_for_bert(chunk.page_content):\n",
    "                    chunk.metadata.update({\n",
    "                        \"source\": file_path, \n",
    "                        \"page_number\": page_num + 1,\n",
    "                        \"c\": \"rule_based\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    })\n",
    "                    docs_rule.append(chunk)\n",
    "\n",
    "    print(\"Successfully loaded and chunked the book content from the PDF with page numbers.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please make sure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Let's print some information about the chunks to verify\n",
    "print(f\"Total number of valid chunks created: {len(docs_rule)}\")\n",
    "print(\"\\nHere is the content of the first chunk:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(docs_rule[0].page_content)\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"First chunk metadata: {docs_rule[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7dbd9c",
   "metadata": {},
   "source": [
    "### Semantic Aware Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f99f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484958d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and chunked the book content from the PDF with semantic awareness + page numbers.\n",
      "Filtered chunks for BERT pre-training quality.\n",
      "Total number of valid chunks created: 126\n",
      "\n",
      "Here is the content of the first chunk:\n",
      "---------------------------------------\n",
      "Michael Witmore\n",
      "Director, Folger Shakespeare Library\n",
      "It is hard to imagine a world without Shakespeare. Since their\n",
      "composition four hundred years ago, Shakespeare’s plays and poems\n",
      "have traveled the globe, inviting those who see and read his works to\n",
      "make them their own. Readers of the New Folger Editions are part of this ongoing process\n",
      "of “taking up Shakespeare,” finding our own thoughts and feelings\n",
      "in language that strikes us as old or unusual and, for that very reason,\n",
      "new. We still struggle to keep up with a writer who could think a\n",
      "mile a minute, whose words paint pictures that shift like clouds. These expertly edited texts are presented to the public as a resource\n",
      "for study, artistic adaptation, and enjoyment. By making the classic\n",
      "texts of the New Folger Editions available in electronic form as The\n",
      "Folger Shakespeare (formerly Folger Digital Texts), we place a\n",
      "trusted resource in the hands of anyone who wants them. The New Folger Editions of Shakespeare’s plays, which are the basis\n",
      "for the texts realized here in digital form, are special because of their\n",
      "origin. The Folger Shakespeare Library in Washington, DC, is the\n",
      "single greatest documentary source of Shakespeare’s works. An\n",
      "unparalleled collection of early modern books, manuscripts, and\n",
      "artwork connected to Shakespeare, the Folger’s holdings have been\n",
      "consulted extensively in the preparation of these texts. The Editions\n",
      "also reflect the expertise gained through the regular performance of\n",
      "Shakespeare’s works in the Folger’s Elizabethan Theatre. I want to express my deep thanks to editors Barbara Mowat and Paul\n",
      "Werstine for creating these indispensable editions of Shakespeare’s\n",
      "works, which incorporate the best of textual scholarship with a\n",
      "richness of commentary that is both inspired and engaging. Readers\n",
      "who want to know more about Shakespeare and his plays can follow\n",
      "the paths these distinguished scholars have tread by visiting the\n",
      "Folger either in-person or online, where a range of physical and\n",
      "digital resources exists to supplement the material in these texts. I\n",
      "commend to you these words, and hope that they inspire.\n",
      "---------------------------------------\n",
      "First chunk metadata: {'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 3, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "import fitz  # Add this import\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# File path to your PDF\n",
    "file_path = \"../julius-caesar_PDF_FolgerShakespeare.pdf\"\n",
    "\n",
    "# A list to store chunks\n",
    "docs_semantic = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        # Initialize HuggingFace embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Initialize semantic chunker with embeddings\n",
    "        text_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "\n",
    "            # Skip empty pages\n",
    "            if not page_text.strip():\n",
    "                continue\n",
    "\n",
    "            # Split the text into semantic chunks\n",
    "            page_chunks = text_splitter.create_documents([page_text])\n",
    "\n",
    "            # Filter and add metadata to each valid chunk\n",
    "            for chunk in page_chunks:\n",
    "                # Validate chunk for BERT pre-training\n",
    "                if is_valid_chunk_for_bert(chunk.page_content):\n",
    "                    chunk.metadata.update({\n",
    "                        \"source\": file_path, \n",
    "                        \"page_number\": page_num + 1,\n",
    "                        \"c\": \"semantic\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    })\n",
    "                    docs_semantic.append(chunk)\n",
    "\n",
    "    print(\"✅ Successfully loaded and chunked the book content from the PDF with semantic awareness + page numbers.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{file_path}' was not found. Please make sure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# Print some information about the chunks to verify\n",
    "print(f\"Total number of valid chunks created: {len(docs_semantic)}\")\n",
    "print(\"\\nHere is the content of the first chunk:\")\n",
    "print(\"---------------------------------------\")\n",
    "print(docs_semantic[0].page_content)\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"First chunk metadata: {docs_semantic[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27ecdf",
   "metadata": {},
   "source": [
    "### Hierarchical Chunking Strategy\n",
    "This strategy creates chunks at multiple levels of granularity and maintains parent-child relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7857c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully created hierarchical chunks from the PDF.\n",
      "Filtered chunks for BERT pre-training quality.\n",
      "\\nTotal valid hierarchical chunks created: 160\n",
      "\\nBreakdown by level:\n",
      "  Level 1 (section): 1 chunks\n",
      "  Level 2 (paragraph): 53 chunks\n",
      "  Level 3 (sentence): 106 chunks\n",
      "\\n==================================================\n",
      "EXAMPLE CHUNKS FROM EACH LEVEL:\n",
      "==================================================\n",
      "\\nLevel 1 (section) Example:\n",
      "------------------------------\n",
      "Content: Hamlet: “O farewell, honest soldier.  Who hath relieved/you?”). At\n",
      "any point in the text, you can hover your cursor over a bracket for\n",
      "more information.\n",
      "Because the Folger Shakespeare texts are edited...\n",
      "Metadata: {'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 5, 'chunk_type': 'section', 'chunk_level': 1, 'section_id': 'page_5_section_0', 'parent_id': 'page_5', 'chunk_index': 0, 'c': 'hierarchical_section', 'ischunk': True}\n",
      "\\nLevel 2 (paragraph) Example:\n",
      "------------------------------\n",
      "Content: Hamlet: “O farewell, honest soldier.  Who hath relieved/you?”). At\n",
      "any point in the text, you can hover your cursor over a bracket for\n",
      "more information.\n",
      "Because the Folger Shakespeare texts are edited...\n",
      "Metadata: {'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 5, 'chunk_type': 'paragraph', 'chunk_level': 2, 'paragraph_id': 'page_5_section_0_para_0', 'parent_id': 'page_5_section_0', 'section_id': 'page_5_section_0', 'chunk_index': 0, 'c': 'hierarchical_paragraph', 'ischunk': True}\n",
      "\\nLevel 3 (sentence) Example:\n",
      "------------------------------\n",
      "Content: ? What, know you not,\n",
      " Being mechanical, you ought not walk\n",
      " Upon a laboring day without the sign\n",
      " Of your profession?—Speak, what trade art thou?\n",
      "  Why, sir, a carpenter.\n",
      " \n",
      " Where is thy leather apro...\n",
      "Metadata: {'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 9, 'chunk_type': 'sentence', 'chunk_level': 3, 'sentence_id': 'page_9_section_0_para_0_sent_2', 'parent_id': 'page_9_section_0_para_0', 'paragraph_id': 'page_9_section_0_para_0', 'section_id': 'page_9_section_0', 'chunk_index': 2, 'c': 'hierarchical_sentence', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "\n",
    "def is_valid_chunk_for_bert(text):\n",
    "    \"\"\"\n",
    "    Check if a chunk is valid for BERT pre-training.\n",
    "    - Should have at least 2 complete sentences\n",
    "    - Should not be a half-cut sentence\n",
    "    - Should have minimum length for meaningful content\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check minimum length (at least 100 characters for meaningful content)\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # Count sentences (look for sentence endings)\n",
    "    sentence_endings = re.findall(r'[.!?]+', text)\n",
    "    if len(sentence_endings) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if text ends with a complete sentence\n",
    "    if not re.search(r'[.!?]\\s*$', text):\n",
    "        return False\n",
    "    \n",
    "    # Check if text starts properly (not a fragment)\n",
    "    if text[0].islower():  # Likely a sentence fragment\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_hierarchical_chunks(text, page_num, source_file):\n",
    "    \"\"\"\n",
    "    Create hierarchical chunks with multiple levels of granularity.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to chunk\n",
    "        page_num: Page number for metadata\n",
    "        source_file: Source file path for metadata\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects with hierarchical metadata\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Level 1: Large sections (based on multiple paragraphs)\n",
    "    section_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Level 2: Medium chunks (paragraphs/subsections)\n",
    "    paragraph_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Level 3: Small chunks (sentences/phrases)\n",
    "    sentence_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\". \", \"! \", \"? \", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Create level 1 chunks (sections)\n",
    "    level1_chunks = section_splitter.split_text(text)\n",
    "    \n",
    "    for i, section_text in enumerate(level1_chunks):\n",
    "        section_id = f\"page_{page_num}_section_{i}\"\n",
    "        \n",
    "        # Validate and create section-level chunk\n",
    "        if is_valid_chunk_for_bert(section_text):\n",
    "            section_chunk = Document(\n",
    "                page_content=section_text,\n",
    "                metadata={\n",
    "                    \"source\": source_file,\n",
    "                    \"page_number\": page_num,\n",
    "                    \"chunk_type\": \"section\",\n",
    "                    \"chunk_level\": 1,\n",
    "                    \"section_id\": section_id,\n",
    "                    \"parent_id\": f\"page_{page_num}\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"c\": \"hierarchical_section\",  # Added metadata field 'c'\n",
    "                    \"ischunk\": True  # Added ischunk field\n",
    "                }\n",
    "            )\n",
    "            chunks.append(section_chunk)\n",
    "        \n",
    "        # Create level 2 chunks (paragraphs) within this section\n",
    "        level2_chunks = paragraph_splitter.split_text(section_text)\n",
    "        \n",
    "        for j, paragraph_text in enumerate(level2_chunks):\n",
    "            paragraph_id = f\"{section_id}_para_{j}\"\n",
    "            \n",
    "            # Validate and create paragraph-level chunk\n",
    "            if is_valid_chunk_for_bert(paragraph_text):\n",
    "                paragraph_chunk = Document(\n",
    "                    page_content=paragraph_text,\n",
    "                    metadata={\n",
    "                        \"source\": source_file,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_type\": \"paragraph\",\n",
    "                        \"chunk_level\": 2,\n",
    "                        \"paragraph_id\": paragraph_id,\n",
    "                        \"parent_id\": section_id,\n",
    "                        \"section_id\": section_id,\n",
    "                        \"chunk_index\": j,\n",
    "                        \"c\": \"hierarchical_paragraph\",  # Added metadata field 'c'\n",
    "                        \"ischunk\": True  # Added ischunk field\n",
    "                    }\n",
    "                )\n",
    "                chunks.append(paragraph_chunk)\n",
    "            \n",
    "            # Create level 3 chunks (sentences) within this paragraph\n",
    "            level3_chunks = sentence_splitter.split_text(paragraph_text)\n",
    "            \n",
    "            for k, sentence_text in enumerate(level3_chunks):\n",
    "                sentence_id = f\"{paragraph_id}_sent_{k}\"\n",
    "                \n",
    "                # Validate and create sentence-level chunk\n",
    "                if is_valid_chunk_for_bert(sentence_text):\n",
    "                    sentence_chunk = Document(\n",
    "                        page_content=sentence_text,\n",
    "                        metadata={\n",
    "                            \"source\": source_file,\n",
    "                            \"page_number\": page_num,\n",
    "                            \"chunk_type\": \"sentence\",\n",
    "                            \"chunk_level\": 3,\n",
    "                            \"sentence_id\": sentence_id,\n",
    "                            \"parent_id\": paragraph_id,\n",
    "                            \"paragraph_id\": paragraph_id,\n",
    "                            \"section_id\": section_id,\n",
    "                            \"chunk_index\": k,\n",
    "                            \"c\": \"hierarchical_sentence\",  # Added metadata field 'c'\n",
    "                            \"ischunk\": True  # Added ischunk field\n",
    "                        }\n",
    "                    )\n",
    "                    chunks.append(sentence_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Apply hierarchical chunking to the PDF\n",
    "file_path = \"../julius-caesar_PDF_FolgerShakespeare.pdf\"\n",
    "docs_hierarchical = []\n",
    "\n",
    "try:\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        for page_num, page in enumerate(pdf_doc):\n",
    "            # Extract text from the current page\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            # Skip empty pages\n",
    "            if not page_text.strip():\n",
    "                continue\n",
    "            \n",
    "            # Create hierarchical chunks for this page\n",
    "            page_chunks = create_hierarchical_chunks(page_text, page_num + 1, file_path)\n",
    "            docs_hierarchical.extend(page_chunks)\n",
    "\n",
    "    print(\"✅ Successfully created hierarchical chunks from the PDF.\")\n",
    "    print(f\"Filtered chunks for BERT pre-training quality.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: The file '{file_path}' was not found.\")\n",
    "    \n",
    "# Print statistics about the hierarchical chunks\n",
    "level_counts = {}\n",
    "for chunk in docs_hierarchical:\n",
    "    level = chunk.metadata.get(\"chunk_level\", \"unknown\")\n",
    "    chunk_type = chunk.metadata.get(\"chunk_type\", \"unknown\")\n",
    "    key = f\"Level {level} ({chunk_type})\"\n",
    "    level_counts[key] = level_counts.get(key, 0) + 1\n",
    "\n",
    "print(f\"\\\\nTotal valid hierarchical chunks created: {len(docs_hierarchical)}\")\n",
    "print(\"\\\\nBreakdown by level:\")\n",
    "for level, count in sorted(level_counts.items()):\n",
    "    print(f\"  {level}: {count} chunks\")\n",
    "\n",
    "# Show example chunks from each level\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"EXAMPLE CHUNKS FROM EACH LEVEL:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for level in [1, 2, 3]:\n",
    "    example_chunk = next((chunk for chunk in docs_hierarchical \n",
    "                         if chunk.metadata.get(\"chunk_level\") == level), None)\n",
    "    if example_chunk:\n",
    "        chunk_type = example_chunk.metadata.get(\"chunk_type\", \"unknown\")\n",
    "        print(f\"\\\\nLevel {level} ({chunk_type}) Example:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Content: {example_chunk.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {example_chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292c80f",
   "metadata": {},
   "source": [
    "### Saving the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "270a1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 134 rule-based chunks to 'Chunk_files/julius-caesar_chunks_rule.pkl'.\n",
      "Successfully saved 126 semantic chunks to 'Chunk_files/julius-caesar_chunks_semantic.pkl'.\n",
      "Successfully saved 160 hierarchical chunks to 'Chunk_files/julius-caesar_chunks_hierarchical.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Create Chunk_files directory if it doesn't exist\n",
    "os.makedirs(\"Chunk_files\", exist_ok=True)\n",
    "HIERARCHAIL_CHUNK_FILE_SAVE_NAME = \"Chunk_files/julius-caesar_chunks_hierarchical.pkl\"\n",
    "SEMANTIC_CHUNK_FILE_SAVE_NAME = \"Chunk_files/julius-caesar_chunks_semantic.pkl\"\n",
    "RULE_BASED_CHUNK_FILE_SAVE_NAME = \"Chunk_files/julius-caesar_chunks_rule.pkl\"\n",
    "\n",
    "# Save rule-based chunks (if they exist)\n",
    "if 'docs_rule' in locals() and docs_rule:\n",
    "    file_path_rule = RULE_BASED_CHUNK_FILE_SAVE_NAME\n",
    "    try:\n",
    "        with open(file_path_rule, \"wb\") as f:\n",
    "            pickle.dump(docs_rule, f)\n",
    "        print(f\"Successfully saved {len(docs_rule)} rule-based chunks to '{file_path_rule}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving rule-based chunks: {e}\")\n",
    "else:\n",
    "    print(\"No rule-based chunks to save (docs_rule not defined or empty).\")\n",
    "\n",
    "# Save semantic chunks (if they exist)\n",
    "if 'docs_semantic' in locals() and docs_semantic:\n",
    "    file_path_semantic = SEMANTIC_CHUNK_FILE_SAVE_NAME\n",
    "    try:\n",
    "        with open(file_path_semantic, \"wb\") as f:\n",
    "            pickle.dump(docs_semantic, f)\n",
    "        print(f\"Successfully saved {len(docs_semantic)} semantic chunks to '{file_path_semantic}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving semantic chunks: {e}\")\n",
    "else:\n",
    "    print(\"No semantic chunks to save (docs_semantic not defined or empty).\")\n",
    "\n",
    "# Save hierarchical chunks (if they exist)\n",
    "if 'docs_hierarchical' in locals() and docs_hierarchical:\n",
    "    file_path_hierarchical = HIERARCHAIL_CHUNK_FILE_SAVE_NAME\n",
    "    try:\n",
    "        with open(file_path_hierarchical, \"wb\") as f:\n",
    "            pickle.dump(docs_hierarchical, f)\n",
    "        print(f\"Successfully saved {len(docs_hierarchical)} hierarchical chunks to '{file_path_hierarchical}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving hierarchical chunks: {e}\")\n",
    "else:\n",
    "    print(\"No hierarchical chunks to save (docs_hierarchical not defined or empty).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
