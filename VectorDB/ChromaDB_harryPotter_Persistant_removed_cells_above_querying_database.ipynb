{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd26bf01",
   "metadata": {},
   "source": [
    "### Imports and Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e0d607ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/harry_potter_chunks_hierarchical.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"harry_potter_collection\"\n",
    "# # Set API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b927c",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4b9b8be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: C:\\Users\\Valmik Belgaonkar\\OneDrive\\Desktop\\RAG-A2\\VectorDB\\chroma_Data\n",
      "Existing collections: ['harry_potter_collection']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "788e6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 160 chunks from '../Chunking/Chunk_files/harry_potter_chunks_hierarchical.pkl'.\n",
      "\n",
      "Here is the metadata of a loaded chunk:\n",
      "{'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 5, 'chunk_type': 'section', 'chunk_level': 1, 'section_id': 'page_5_section_0', 'parent_id': 'page_5', 'chunk_index': 0, 'c': 'hierarchical_section', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# No need for fitz or RecursiveCharacterTextSplitter here, as we are loading from a file.\n",
    "\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f: # 'rb' mode for reading in binary\n",
    "        loaded_docs = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Now you can inspect the loaded documents to verify.\n",
    "print(\"\\nHere is the metadata of a loaded chunk:\")\n",
    "if loaded_docs:\n",
    "    print(loaded_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cbb00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use default SentenceTransformer for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "928a1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding function initialized successfully with custom SentenceTransformer model.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# ✅ Load the model once\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Define embedding class that supports Chroma ≥0.5.x\n",
    "class SentenceTransformerEmbeddingFunction:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self.embed_documents(input)\n",
    "\n",
    "\n",
    "    # For inserting documents\n",
    "    def embed_documents(self, input):\n",
    "        return self.model.encode(input, show_progress_bar=False).tolist()\n",
    "\n",
    "    # For querying (Chroma calls this inside .query)\n",
    "    def embed_query(self, input):\n",
    "        return self.model.encode(input, show_progress_bar=False).tolist()\n",
    "\n",
    "    # Optional metadata function\n",
    "    def name(self):\n",
    "        return \"custom-sentence-transformer\"\n",
    "\n",
    "# ✅ Instantiate your embedding function object\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model)\n",
    "\n",
    "print(\"✅ Embedding function initialized successfully with custom SentenceTransformer model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f44b",
   "metadata": {},
   "source": [
    "### Creating new Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3b34eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'harry_potter_collection' created or accessed successfully\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get or create the collection\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)  \n",
    "except Exception as e:\n",
    "    print(f\"Error = {e}\")\n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\n",
    "        \"description\": \"Harry Potter book chunks\",\n",
    "        \"created\": str(datetime.now())\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' created or accessed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b449ba",
   "metadata": {},
   "source": [
    "### Add data to collection\n",
    "The chunks have to be given an id and added to the collection now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eaa83664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch: 0 to 159 (160 items)\n",
      "Successfully added 160 documents to collection 'harry_potter_collection'\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Prepare documents for ChromaDB\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "# Process each loaded document chunk\n",
    "for i, doc in enumerate(loaded_docs):\n",
    "    # Generate a unique ID (you could use a more deterministic approach if needed)\n",
    "    doc_id = f\"hp_chunk_{i}\"\n",
    "    \n",
    "    # Get the document text\n",
    "    document_text = doc.page_content\n",
    "    \n",
    "    # Get the document metadata\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Add to our lists\n",
    "    ids.append(doc_id)\n",
    "    documents.append(document_text)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Add documents in batches to avoid memory issues\n",
    "batch_size = 500\n",
    "total_added = 0\n",
    "\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    end_idx = min(i + batch_size, len(ids))\n",
    "    \n",
    "    # collection.update(\n",
    "    #     ids=ids[i:end_idx],\n",
    "    #     documents=documents[i:end_idx],\n",
    "    #     metadatas=metadatas[i:end_idx]\n",
    "    # )\n",
    "    collection.add(\n",
    "        ids=ids[i:end_idx],\n",
    "        documents=documents[i:end_idx],\n",
    "        metadatas=metadatas[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    total_added += end_idx - i\n",
    "    print(f\"Added batch: {i} to {end_idx-1} ({end_idx-i} items)\")\n",
    "\n",
    "print(f\"Successfully added {total_added} documents to collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b1fd6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 160\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "--- Document 1 ---\n",
      "ID: hp_chunk_0\n",
      "Text: Hamlet: “O farewell, honest soldier.  Who hath relieved/you?”). At\n",
      "any point in the text, you can ho...\n",
      "Metadata: {'section_id': 'page_5_section_0', 'parent_id': 'page_5', 'ischunk': True, 'c': 'hierarchical_section', 'page_number': 5, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'chunk_level': 1, 'chunk_type': 'section', 'chunk_index': 0}\n",
      "\n",
      "--- Document 2 ---\n",
      "ID: hp_chunk_1\n",
      "Text: Hamlet: “O farewell, honest soldier.  Who hath relieved/you?”). At\n",
      "any point in the text, you can ho...\n",
      "Metadata: {'section_id': 'page_5_section_0', 'parent_id': 'page_5_section_0', 'chunk_type': 'paragraph', 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'chunk_index': 0, 'c': 'hierarchical_paragraph', 'page_number': 5, 'paragraph_id': 'page_5_section_0_para_0', 'chunk_level': 2, 'ischunk': True}\n",
      "\n",
      "--- Document 3 ---\n",
      "ID: hp_chunk_2\n",
      "Text: FLAVIUS\n",
      "CARPENTER\n",
      "MARULLUS\n",
      "COBBLER\n",
      "MARULLUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "Enter Flavius, Marullus, and certain Com...\n",
      "Metadata: {'parent_id': 'page_9_section_0', 'page_number': 9, 'paragraph_id': 'page_9_section_0_para_0', 'c': 'hierarchical_paragraph', 'chunk_type': 'paragraph', 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'chunk_level': 2, 'section_id': 'page_9_section_0', 'ischunk': True, 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Check collection count\n",
    "count = collection.count()\n",
    "print(f\"Total documents in collection: {count}\")\n",
    "\n",
    "# Peek at the first few entries\n",
    "peek = collection.peek(limit=3)\n",
    "print(\"\\nSample entries:\")\n",
    "for i, (doc_id, doc_text, metadata) in enumerate(zip(\n",
    "    peek['ids'], peek['documents'], peek['metadatas']\n",
    ")):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Text: {doc_text[:100]}...\")\n",
    "    print(f\"Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ce07b",
   "metadata": {},
   "source": [
    "## Quantitative Analysis using RAGAs: Faithfulness and Answer Relevency: using OLLaMA for RAG and HuggingFace Zephyr for RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d2c56031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8: Evaluate RAG pipeline using RAGAS\n",
    "# # Faithfulness & Answer Relevancy\n",
    "# # Ollama for RAG generation | Zephyr for offline RAGAS evaluation\n",
    "\n",
    "# # ==== INSTALL DEPENDENCIES ====\n",
    "# # !pip install ragas datasets transformers accelerate sentence-transformers tqdm\n",
    "# # Make sure you have Ollama installed: https://ollama.ai/download\n",
    "# # Example to pull a model: `ollama pull llama3` or `ollama pull mistral`\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import subprocess\n",
    "# from tqdm import tqdm\n",
    "# from datetime import datetime\n",
    "# from datasets import Dataset\n",
    "\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics import faithfulness, answer_relevancy\n",
    "# from ragas.llms import HuggingfaceLLM\n",
    "# from ragas.embeddings import HuggingfaceEmbeddings\n",
    "\n",
    "# # ==== CONFIG ====\n",
    "# testbed_path = \"../RAG Results/test_bed.json\"\n",
    "# output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "# TOP_K = 3\n",
    "\n",
    "# # --- LLMs ---\n",
    "# OLLAMA_MODEL = \"llama3\"                     # for RAG generation (local via Ollama)\n",
    "# LLM_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"  # for RAGAS evaluation (offline)\n",
    "# EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # for embeddings\n",
    "# # =================\n",
    "\n",
    "# # 1️⃣ Load test data\n",
    "# with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "# print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "# # 2️⃣ Function to generate RAG answers using Ollama\n",
    "# def generate_with_ollama(prompt, model_name=OLLAMA_MODEL):\n",
    "#     \"\"\"\n",
    "#     Generate a response using a local Ollama model.\n",
    "#     Assumes Ollama is installed and the model is already pulled.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Run the Ollama CLI\n",
    "#         result = subprocess.run(\n",
    "#             [\"ollama\", \"run\", model_name],\n",
    "#             input=prompt.encode(\"utf-8\"),\n",
    "#             capture_output=True,\n",
    "#             text=True,\n",
    "#             timeout=120\n",
    "#         )\n",
    "#         if result.returncode == 0:\n",
    "#             return result.stdout.strip()\n",
    "#         else:\n",
    "#             print(f\"[WARN] Ollama returned error: {result.stderr}\")\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Ollama call failed: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # 3️⃣ Prepare evaluation records\n",
    "# records = []\n",
    "# for item in tqdm(test_data, desc=\"Generating Ollama RAG answers\"):\n",
    "#     question = item[\"question\"]\n",
    "#     ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "#     # --- Retrieve from Chroma ---\n",
    "#     retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "#     retrieved_docs = retrieved[\"documents\"][0]\n",
    "#     retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "#     # --- Build RAG prompt ---\n",
    "#     prompt = (\n",
    "#         f\"You are a helpful assistant. \"\n",
    "#         f\"Use only the information provided in the context below to answer the question.\\n\\n\"\n",
    "#         f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "#         f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "#     )\n",
    "\n",
    "#     # --- Generate answer using Ollama ---\n",
    "#     generated_answer = generate_with_ollama(prompt)\n",
    "#     if not generated_answer:\n",
    "#         generated_answer = f\"[Fallback mock answer] {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "#     # --- Add record for RAGAS evaluation ---\n",
    "#     records.append({\n",
    "#         \"question\": question,\n",
    "#         \"contexts\": retrieved_docs,\n",
    "#         \"answer\": generated_answer,\n",
    "#         \"ground_truth\": [ideal_answer],\n",
    "#     })\n",
    "\n",
    "# # 4️⃣ Convert to Hugging Face Dataset\n",
    "# dataset = Dataset.from_list(records)\n",
    "\n",
    "# # 5️⃣ Initialize Zephyr & embedding models for RAGAS (offline)\n",
    "# llm = HuggingfaceLLM(model=LLM_MODEL)\n",
    "# embeddings = HuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# # 6️⃣ Evaluate using RAGAS\n",
    "# print(f\"\\n[INFO] Evaluating with RAGAS (Faithfulness & Answer Relevancy) using {LLM_MODEL} ...\")\n",
    "# results = evaluate(\n",
    "#     dataset=dataset,\n",
    "#     metrics=[faithfulness, answer_relevancy],\n",
    "#     llm=llm,\n",
    "#     embeddings=embeddings\n",
    "# )\n",
    "\n",
    "# # 7️⃣ Extract scores\n",
    "# faithfulness_score = results[\"faithfulness\"]\n",
    "# answer_relevancy_score = results[\"answer_relevancy\"]\n",
    "\n",
    "# # 8️⃣ Save results to file\n",
    "# with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"=== RAG Evaluation Metrics (Ollama + RAGAS Offline) ===\\n\")\n",
    "#     f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "#     f.write(f\"Faithfulness: {faithfulness_score:.4f}\\n\")\n",
    "#     f.write(f\"Answer Relevancy: {answer_relevancy_score:.4f}\\n\\n\")\n",
    "#     f.write(\"Full Results:\\n\")\n",
    "#     f.write(str(results))\n",
    "\n",
    "# print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "# print(f\"Faithfulness: {faithfulness_score:.4f} | Answer Relevancy: {answer_relevancy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c47d8",
   "metadata": {},
   "source": [
    "## Using Gemini for RAG and RAGAs both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "83b8b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8: Evaluate RAG pipeline using RAGAS\n",
    "# # Faithfulness & Answer Relevancy\n",
    "# # Gemini for RAG generation | Gemini for RAGAS evaluation (using RAGAS's API integration)\n",
    "\n",
    "# # ==== INSTALL DEPENDENCIES ====\n",
    "# # !pip install ragas datasets google-genai tqdm\n",
    "# # Make sure you have your Gemini API key set as an environment variable (e.g., GEMINI_API_KEY)\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import subprocess\n",
    "# from tqdm import tqdm\n",
    "# from datetime import datetime\n",
    "# from datasets import Dataset\n",
    "\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics import faithfulness, answer_relevancy\n",
    "# # New imports for Gemini/Google\n",
    "# from google import genai\n",
    "# from ragas.llms import RagasLLM\n",
    "# from ragas.embeddings import HuggingfaceEmbeddings # Reusing a local embedding model as it's efficient\n",
    "\n",
    "# # ==== CONFIG ====\n",
    "# testbed_path = \"../RAG Results/test_bed.json\"\n",
    "# output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "# TOP_K = 3\n",
    "\n",
    "# # --- LLMs ---\n",
    "# # Using specific Gemini models for their respective roles\n",
    "# GEMINI_RAG_MODEL = \"gemini-2.5-flash\" # for RAG generation (Fast, modern LLM)\n",
    "# GEMINI_RAGAS_MODEL = \"gemini-2.5-pro\"# for RAGAS evaluation (Prefer a more capable model for reasoning tasks)\n",
    "# EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\" # for embeddings (RAGAS requires an embedding model)\n",
    "# # =================\n",
    "\n",
    "# # Initialize the Gemini client\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"sk-AIzaSyAoBDShyFr3QWxKiSVc59n_MkCjSNrZHKg\"\n",
    "\n",
    "# # Initialize the Gemini client — it will automatically pick up GEMINI_API_KEY\n",
    "# try:\n",
    "#     client = genai.Client()\n",
    "#     print(\"[INFO] Gemini client initialized successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"[ERROR] Failed to initialize Gemini Client: {e}\")\n",
    "#     raise\n",
    "\n",
    "# # 1️⃣ Load test data\n",
    "# with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "# print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "# # 2️⃣ Function to generate RAG answers using Gemini\n",
    "# def generate_with_gemini(prompt, model_name=GEMINI_RAG_MODEL):\n",
    "#     \"\"\"\n",
    "#     Generate a response using the Gemini API.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         response = client.models.generate_content(\n",
    "#             model=model_name,\n",
    "#             contents=prompt,\n",
    "#             config={\"system_instruction\": \"You are a helpful assistant. Use only the provided context to answer the question.\"}\n",
    "#         )\n",
    "#         return response.text.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Gemini API call failed: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # 3️⃣ Prepare evaluation records\n",
    "# records = []\n",
    "# # NOTE: The 'collection' object (ChromaDB) is missing in this script, \n",
    "# # assuming it exists in the user's environment before this cell runs.\n",
    "# try:\n",
    "#     # Attempt a mock query to ensure 'collection' exists, otherwise provide a warning.\n",
    "#     collection.query(query_texts=[\"test\"], n_results=1)\n",
    "# except NameError:\n",
    "#     print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB client) is NOT defined.\")\n",
    "#     print(\"This script will fail when trying to retrieve documents (collection.query).\")\n",
    "#     print(\"Please ensure your ChromaDB client/collection is initialized before running this cell.\")\n",
    "#     exit()\n",
    "\n",
    "\n",
    "# for item in tqdm(test_data, desc=\"Generating Gemini RAG answers\"):\n",
    "#     question = item[\"question\"]\n",
    "#     ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "#     # --- Retrieve from Chroma ---\n",
    "#     # NOTE: Assuming 'collection' is defined in the execution environment\n",
    "#     retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "#     retrieved_docs = retrieved[\"documents\"][0]\n",
    "#     retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "#     # --- Build RAG prompt ---\n",
    "#     prompt = (\n",
    "#         f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "#         f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "#     )\n",
    "\n",
    "#     # --- Generate answer using Gemini ---\n",
    "#     generated_answer = generate_with_gemini(prompt)\n",
    "#     if not generated_answer:\n",
    "#         # Fallback in case of API failure\n",
    "#         generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "#     # --- Add record for RAGAS evaluation ---\n",
    "#     records.append({\n",
    "#         \"question\": question,\n",
    "#         \"contexts\": retrieved_docs,\n",
    "#         \"answer\": generated_answer,\n",
    "#         \"ground_truth\": [ideal_answer],\n",
    "#     })\n",
    "\n",
    "# # 4️⃣ Convert to Hugging Face Dataset\n",
    "# dataset = Dataset.from_list(records)\n",
    "\n",
    "# # 5️⃣ Initialize RAGAS components with Gemini & embedding models\n",
    "# # RagasLLM is used to interface with the google-genai SDK for RAGAS evaluation\n",
    "# llm = RagasLLM(model=GEMINI_RAGAS_MODEL)\n",
    "# embeddings = HuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# # 6️⃣ Evaluate using RAGAS\n",
    "# print(f\"\\n[INFO] Evaluating with RAGAS (Faithfulness & Answer Relevancy) using {GEMINI_RAGAS_MODEL} ...\")\n",
    "# results = evaluate(\n",
    "#     dataset=dataset,\n",
    "#     metrics=[faithfulness, answer_relevancy],\n",
    "#     llm=llm,\n",
    "#     embeddings=embeddings\n",
    "# )\n",
    "\n",
    "# # 7️⃣ Extract scores\n",
    "# faithfulness_score = results[\"faithfulness\"]\n",
    "# answer_relevancy_score = results[\"answer_relevancy\"]\n",
    "\n",
    "# # 8️⃣ Save results to file\n",
    "# with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"=== RAG Evaluation Metrics (Gemini + RAGAS with Gemini) ===\\n\")\n",
    "#     f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "#     f.write(f\"RAG Generation Model: {GEMINI_RAG_MODEL}\\n\")\n",
    "#     f.write(f\"RAGAS Evaluation Model: {GEMINI_RAGAS_MODEL}\\n\\n\")\n",
    "#     f.write(f\"Faithfulness: {faithfulness_score:.4f}\\n\")\n",
    "#     f.write(f\"Answer Relevancy: {answer_relevancy_score:.4f}\\n\\n\")\n",
    "#     f.write(\"Full Results:\\n\")\n",
    "#     f.write(str(results))\n",
    "\n",
    "# print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "# print(f\"Faithfulness: {faithfulness_score:.4f} | Answer Relevancy: {answer_relevancy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f3242",
   "metadata": {},
   "source": [
    "## Joshi Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a21448fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001 ['embedText', 'countTextTokens']\n",
      "models/gemini-2.5-pro-preview-03-25 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-06-17 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-05-06 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-06-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-preview-image-generation ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts ['countTokens', 'generateContent']\n",
      "models/learnlm-2.0-flash-experimental ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it ['generateContent', 'countTokens']\n",
      "models/gemini-flash-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-flash-lite-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-pro-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image-preview ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-09-2025 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-09-2025 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-robotics-er-1.5-preview ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-computer-use-preview-10-2025 ['generateContent', 'countTokens']\n",
      "models/embedding-001 ['embedContent']\n",
      "models/text-embedding-004 ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-exp ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-001 ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
      "models/aqa ['generateAnswer']\n",
      "models/imagen-4.0-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-generate-001 ['predict']\n",
      "models/imagen-4.0-ultra-generate-001 ['predict']\n",
      "models/imagen-4.0-fast-generate-001 ['predict']\n",
      "models/veo-2.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-generate-preview ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-preview ['predictLongRunning']\n",
      "models/veo-3.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-001 ['predictLongRunning']\n",
      "models/veo-3.1-generate-preview ['predictLongRunning']\n",
      "models/veo-3.1-fast-generate-preview ['predictLongRunning']\n",
      "models/gemini-2.0-flash-live-001 ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-live-2.5-flash-preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-live-preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-native-audio-latest ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025 ['countTokens', 'bidiGenerateContent']\n",
      "Exists: True\n",
      "Size: 3632 bytes\n",
      "First few characters:\n",
      " [\n",
      "    {\n",
      "        \"question\": \"How does Caesar first enter the play?\",\n",
      "        \"ideal_answer\": \"In a triumphal procession; he has defeated the sons of his deceased rival, Pompey\"\n",
      "    },\n",
      "{\n",
      "\"question\": \"W\n",
      "[INFO] Loaded 25 QA pairs from testbed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:   8%|▊         | 2/25 [00:00<00:04,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  12%|█▏        | 3/25 [00:00<00:04,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  20%|██        | 5/25 [00:01<00:04,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  24%|██▍       | 6/25 [00:01<00:06,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  36%|███▌      | 9/25 [00:02<00:04,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  40%|████      | 10/25 [00:02<00:03,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  48%|████▊     | 12/25 [00:03<00:03,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  56%|█████▌    | 14/25 [00:03<00:02,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  64%|██████▍   | 16/25 [00:04<00:02,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  68%|██████▊   | 17/25 [00:04<00:02,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  76%|███████▌  | 19/25 [00:05<00:01,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  84%|████████▍ | 21/25 [00:05<00:01,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  88%|████████▊ | 22/25 [00:05<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:  96%|█████████▌| 24/25 [00:06<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers: 100%|██████████| 25/25 [00:06<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Gemini API call failed: 404 models/gemini-1.5-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Evaluating with RAGAS using gemini-1.5-pro ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]Exception raised in Job[0]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[1]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[2]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[3]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[4]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[5]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[6]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[7]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[8]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[9]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[10]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[11]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[12]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[13]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[14]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[15]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[16]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[17]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[18]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[19]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[20]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[21]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[22]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[23]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[24]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[25]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[26]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[27]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[28]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[29]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[30]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[31]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[32]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[33]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[34]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[35]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[36]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[37]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[38]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[39]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[40]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[41]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[42]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[43]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[44]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[45]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[46]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[47]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[48]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[49]: TypeError(object list can't be used in 'await' expression)\n",
      "Evaluating: 100%|██████████| 50/50 [00:21<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation complete! Metrics saved to '../RAG Results/multiquery_rag_metrics.txt'\n",
      "Faithfulness (avg): nan | Answer Relevancy (avg): nan\n"
     ]
    }
   ],
   "source": [
    "# === Gemini + RAG + RAGAS Evaluation (no LangChain) ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets google-generativeai tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import google.generativeai as genai\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# ==== CONFIG ====\n",
    "genai.configure(api_key=\"AIzaSyAoBDShyFr3QWxKiSVc59n_MkCjSNrZHKg\")  # ⚠️ Replace with your real Gemini key\n",
    "\n",
    "for m in genai.list_models():\n",
    "    print(m.name, m.supported_generation_methods)\n",
    "\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "TOP_K = 3\n",
    "\n",
    "GEMINI_RAG_MODEL = \"gemini-1.5-pro\"\n",
    "GEMINI_RAGAS_MODEL = \"gemini-1.5-pro\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_200 = f.read(200)\n",
    "print(\"First few characters:\\n\", first_200)\n",
    "\n",
    "\n",
    "# ==== 1️⃣ Load test data ====\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "\n",
    "# ==== 2️⃣ Gemini generation ====\n",
    "def generate_with_gemini(prompt, model_name=GEMINI_RAG_MODEL):\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Gemini API call failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==== 3️⃣ Gemini wrapper for RAGAS ====\n",
    "class GeminiRagasLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def generate(self, prompts, **kwargs):  # ✅ Accept extra args like n=1\n",
    "        model = genai.GenerativeModel(self.model_name)\n",
    "        outputs = []\n",
    "        for p in prompts:\n",
    "            try:\n",
    "                r = model.generate_content(p)\n",
    "                outputs.append(r.text.strip())\n",
    "            except Exception as e:\n",
    "                outputs.append(f\"[Error: {e}]\")\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# ==== 4️⃣ Check collection availability ====\n",
    "try:\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB) is NOT defined.\")\n",
    "    print(\"Please initialize your ChromaDB client/collection before running this cell.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# ==== 5️⃣ Generate records ====\n",
    "records = []\n",
    "for item in tqdm(test_data, desc=\"Generating Gemini RAG answers\"):\n",
    "    question = item[\"question\"]\n",
    "    ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "    retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "    retrieved_docs = retrieved[\"documents\"][0]\n",
    "    retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    generated_answer = generate_with_gemini(prompt)\n",
    "    if not generated_answer:\n",
    "        generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "    records.append({\n",
    "        \"question\": question,\n",
    "        \"contexts\": retrieved_docs,\n",
    "        \"answer\": generated_answer,\n",
    "        \"ground_truth\": ideal_answer,  # ✅ single string is fine\n",
    "    })\n",
    "\n",
    "\n",
    "# ==== 6️⃣ Convert to HF Dataset ====\n",
    "dataset = Dataset.from_list(records)\n",
    "\n",
    "\n",
    "# ==== 7️⃣ Custom HuggingFace Embedding Wrapper ====\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    \"\"\"Implements both sync + async embedding methods for latest RAGAS.\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        # ✅ Do not call super()\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    # --- Sync methods ---\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False).tolist()[0]\n",
    "\n",
    "    # --- Async methods ---\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "\n",
    "# ==== 8️⃣ Evaluate with RAGAS ====\n",
    "llm = GeminiRagasLLM(GEMINI_RAGAS_MODEL)\n",
    "embeddings = CustomHuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "print(f\"\\n[INFO] Evaluating with RAGAS using {GEMINI_RAGAS_MODEL} ...\")\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "\n",
    "# ==== 9️⃣ Save Results ====\n",
    "faithfulness_scores = results[\"faithfulness\"]\n",
    "answer_relevancy_scores = results[\"answer_relevancy\"]\n",
    "\n",
    "# ✅ Compute mean values\n",
    "faithfulness_mean = float(np.mean(faithfulness_scores))\n",
    "answer_relevancy_mean = float(np.mean(answer_relevancy_scores))\n",
    "\n",
    "os.makedirs(os.path.dirname(output_metrics_path), exist_ok=True)\n",
    "\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Gemini + RAGAS) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GEMINI_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GEMINI_RAGAS_MODEL}\\n\\n\")\n",
    "    f.write(f\"Faithfulness (avg): {faithfulness_mean:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy (avg): {answer_relevancy_mean:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness (avg): {faithfulness_mean:.4f} | Answer Relevancy (avg): {answer_relevancy_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a2201b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google import generativeai as genai\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "# print(llm.invoke(\"Say hello in one line\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
