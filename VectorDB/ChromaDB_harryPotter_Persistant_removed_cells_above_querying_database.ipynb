{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd26bf01",
   "metadata": {},
   "source": [
    "### Imports and Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d607ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/julius-caesar_chunks_semantic.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"anlp_rag_collection\"\n",
    "# # Set API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b927c",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9b8be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: C:\\Users\\micro\\Desktop\\Abhinav college\\Resources\\Sem 7\\Advanced NLP\\Assignment 2\\RAG-A2\\VectorDB\\chroma_Data\n",
      "Existing collections: ['anlp_rag_collection']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788e6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 126 chunks from '../Chunking/Chunk_files/julius-caesar_chunks_semantic.pkl'.\n",
      "\n",
      "Here is the metadata of a loaded chunk:\n",
      "{'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 3, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# No need for fitz or RecursiveCharacterTextSplitter here, as we are loading from a file.\n",
    "\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f: # 'rb' mode for reading in binary\n",
    "        loaded_docs = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Now you can inspect the loaded documents to verify.\n",
    "print(\"\\nHere is the metadata of a loaded chunk:\")\n",
    "if loaded_docs:\n",
    "    print(loaded_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cbb00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use default SentenceTransformer for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928a1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function initialized with model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Install if needed\n",
    "# !pip install sentence_transformers\n",
    "\n",
    "# Set up embedding function\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding function initialized with model: all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f44b",
   "metadata": {},
   "source": [
    "### Creating new Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b34eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleted existing collection 'anlp_rag_collection'\n",
      "[SUCCESS] Fresh collection 'anlp_rag_collection' created successfully\n",
      "Current count in collection: 0\n",
      "[SUCCESS] Fresh collection 'anlp_rag_collection' created successfully\n",
      "Current count in collection: 0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# FORCE DELETE the collection if it exists\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(f\"[INFO] Deleted existing collection '{collection_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] No existing collection named '{collection_name}' to delete.\")\n",
    "\n",
    "# Create a FRESH collection\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\n",
    "        \"description\": \"Julius Caesar Chunks collection for RAG\",\n",
    "        \"created\": str(datetime.now())\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"[SUCCESS] Fresh collection '{collection_name}' created successfully\")\n",
    "print(f\"Current count in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b449ba",
   "metadata": {},
   "source": [
    "### Add data to collection\n",
    "The chunks have to be given an id and added to the collection now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa83664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating IDs with prefix: julius-caesar\n",
      "[INFO] Prepared 126 documents with IDs like: julius-caesar_chunk_0, julius-caesar_chunk_1...\n",
      "[INFO] Added batch: 0 to 125 (126 documents)\n",
      "\n",
      "[SUCCESS] Added 126 documents to collection 'anlp_rag_collection'\n",
      "[INFO] All chunks have IDs in format: julius-caesar_chunk_<number>\n",
      "[INFO] Added batch: 0 to 125 (126 documents)\n",
      "\n",
      "[SUCCESS] Added 126 documents to collection 'anlp_rag_collection'\n",
      "[INFO] All chunks have IDs in format: julius-caesar_chunk_<number>\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "# Extract document name from file path for ID generation\n",
    "# This will give us \"julius-caesar\" from the file path\n",
    "doc_name = file_path.split('/')[-1].split('_chunks')[0]\n",
    "\n",
    "# Prepare documents for ChromaDB\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "print(f\"[INFO] Generating IDs with prefix: {doc_name}\")\n",
    "\n",
    "# Process each loaded document chunk\n",
    "for i, doc in enumerate(loaded_docs):\n",
    "    # Generate a deterministic ID based on document name and index\n",
    "    # This ensures all chunks from the same document have consistent IDs\n",
    "    doc_id = f\"{doc_name}_chunk_{i}\"\n",
    "    \n",
    "    # Get the document text\n",
    "    document_text = doc.page_content\n",
    "    \n",
    "    # Get the document metadata\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Add to our lists\n",
    "    ids.append(doc_id)\n",
    "    documents.append(document_text)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "print(f\"[INFO] Prepared {len(ids)} documents with IDs like: {ids[0]}, {ids[1] if len(ids) > 1 else 'N/A'}...\")\n",
    "\n",
    "# Add documents in batches to avoid memory issues\n",
    "batch_size = 500\n",
    "total_added = 0\n",
    "\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    end_idx = min(i + batch_size, len(ids))\n",
    "    \n",
    "    # Simply add all documents (collection is fresh, no need to update)\n",
    "    collection.add(\n",
    "        ids=ids[i:end_idx],\n",
    "        documents=documents[i:end_idx],\n",
    "        metadatas=metadatas[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    total_added += end_idx - i\n",
    "    print(f\"[INFO] Added batch: {i} to {end_idx-1} ({end_idx-i} documents)\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Added {total_added} documents to collection '{collection_name}'\")\n",
    "print(f\"[INFO] All chunks have IDs in format: {doc_name}_chunk_<number>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fd6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 126\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "--- Document 1 ---\n",
      "ID: julius-caesar_chunk_0\n",
      "Text: Michael Witmore\n",
      "Director, Folger Shakespeare Library\n",
      "It is hard to imagine a world without Shakespea...\n",
      "Metadata: {'ischunk': True, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'c': 'semantic', 'page_number': 3}\n",
      "\n",
      "--- Document 2 ---\n",
      "ID: julius-caesar_chunk_1\n",
      "Text: Until now, with the release of The Folger Shakespeare (formerly\n",
      "Folger Digital Texts), readers in se...\n",
      "Metadata: {'c': 'semantic', 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 4, 'ischunk': True}\n",
      "\n",
      "--- Document 3 ---\n",
      "ID: julius-caesar_chunk_2\n",
      "Text: At\n",
      "any point in the text, you can hover your cursor over a bracket for\n",
      "more information. Because the...\n",
      "Metadata: {'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 5, 'ischunk': True, 'c': 'semantic'}\n"
     ]
    }
   ],
   "source": [
    "# Check collection count\n",
    "count = collection.count()\n",
    "print(f\"Total documents in collection: {count}\")\n",
    "\n",
    "# Peek at the first few entries\n",
    "peek = collection.peek(limit=3)\n",
    "print(\"\\nSample entries:\")\n",
    "for i, (doc_id, doc_text, metadata) in enumerate(zip(\n",
    "    peek['ids'], peek['documents'], peek['metadatas']\n",
    ")):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Text: {doc_text[:100]}...\")\n",
    "    print(f\"Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc47242",
   "metadata": {},
   "source": [
    "### Querying the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ee0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rich table for displaying results (optional but nice)\n",
    "try:\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    \n",
    "    console = Console()\n",
    "    use_rich = True\n",
    "except ImportError:\n",
    "    use_rich = False\n",
    "    print(\"Rich package not found. Using standard print.\")\n",
    "\n",
    "# Function to display query results\n",
    "def print_results(results, use_rich=use_rich):\n",
    "    if use_rich:\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Rank\", width=6)\n",
    "        table.add_column(\"Document ID\")\n",
    "        table.add_column(\"Document Text\", width=60)\n",
    "        table.add_column(\"Page\")\n",
    "        table.add_column(\"Distance\")\n",
    "        \n",
    "        docs = results['documents'][0]\n",
    "        ids = results['ids'][0]\n",
    "        metas = results['metadatas'][0]\n",
    "        distances = results['distances'][0]\n",
    "        \n",
    "        for i, (doc, doc_id, meta, dist) in enumerate(zip(docs, ids, metas, distances)):\n",
    "            table.add_row(\n",
    "                str(i+1),\n",
    "                doc_id,\n",
    "                (doc[:100] + \"...\") if len(doc) > 100 else doc,\n",
    "                str(meta.get('page_number', 'N/A')),\n",
    "                f\"{dist:.4f}\"\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "    else:\n",
    "        # Standard print version\n",
    "        for i, (doc, meta, dist) in enumerate(zip(\n",
    "            results['documents'][0], \n",
    "            results['metadatas'][0], \n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            print(f\"\\n--- Result {i+1} ---\")\n",
    "            print(f\"Text: {doc[:100]}...\")\n",
    "            print(f\"Metadata: {meta}\")\n",
    "            print(f\"Distance: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac82c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for query: 'What themes are explored in Julius Caesar?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Rank   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Document ID           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Document Text                                                </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Page </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Distance </span>┃\n",
       "┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━┩\n",
       "│ 1      │ julius-caesar_chunk_… │ 95                                                           │ 51   │ 0.3517   │\n",
       "│        │                       │ Julius Caesar                                                │      │          │\n",
       "│        │                       │ ACT 3. SC. 1                                                 │      │          │\n",
       "│        │                       │ POPILIUS                                                     │      │          │\n",
       "│        │                       │ CASSIUS                                                      │      │          │\n",
       "│        │                       │ POPILIUS                                                     │      │          │\n",
       "│        │                       │ He walks away. BRUTUS                                        │      │          │\n",
       "│        │                       │ CASSIUS                                                      │      │          │\n",
       "│        │                       │ BRUTUS                                                       │      │          │\n",
       "│        │                       │ CASSIUS...                                                   │      │          │\n",
       "│ 2      │ julius-caesar_chunk_… │ CAESAR                                                       │ 50   │ 0.3766   │\n",
       "│        │                       │ SOOTHSAYER                                                   │      │          │\n",
       "│        │                       │ ARTEMIDORUS                                                  │      │          │\n",
       "│        │                       │ DECIUS                                                       │      │          │\n",
       "│        │                       │ ARTEMIDORUS                                                  │      │          │\n",
       "│        │                       │ CAESAR                                                       │      │          │\n",
       "│        │                       │ ARTEMIDORUS                                                  │      │          │\n",
       "│        │                       │ CAESAR                                                       │      │          │\n",
       "│        │                       │ PUBLIUS                                                      │      │          │\n",
       "│        │                       │ CASSIUS                                                      │      │          │\n",
       "│        │                       │ Caesar go...                                                 │      │          │\n",
       "│ 3      │ julius-caesar_chunk_… │ 85                                                           │ 47   │ 0.3788   │\n",
       "│        │                       │ Julius Caesar                                                │      │          │\n",
       "│        │                       │ ACT 2. SC. 4                                                 │      │          │\n",
       "│        │                       │ ARTEMIDORUS                                                  │      │          │\n",
       "│        │                       │  Thy lover,                                                  │      │          │\n",
       "│        │                       │  Artemidorus                                                 │      │          │\n",
       "│        │                       │ He exits. PORTIA                                             │      │          │\n",
       "│        │                       │ LUCIUS                                                       │      │          │\n",
       "│        │                       │ PORTIA                                                       │      │          │\n",
       "│        │                       │ En...                                                        │      │          │\n",
       "└────────┴───────────────────────┴──────────────────────────────────────────────────────────────┴──────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mRank  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDocument ID          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDocument Text                                               \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mPage\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDistance\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━┩\n",
       "│ 1      │ julius-caesar_chunk_… │ 95                                                           │ 51   │ 0.3517   │\n",
       "│        │                       │ Julius Caesar                                                │      │          │\n",
       "│        │                       │ ACT 3. SC. 1                                                 │      │          │\n",
       "│        │                       │ POPILIUS                                                     │      │          │\n",
       "│        │                       │ CASSIUS                                                      │      │          │\n",
       "│        │                       │ POPILIUS                                                     │      │          │\n",
       "│        │                       │ He walks away. BRUTUS                                        │      │          │\n",
       "│        │                       │ CASSIUS                                                      │      │          │\n",
       "│        │                       │ BRUTUS                                                       │      │          │\n",
       "│        │                       │ CASSIUS...                                                   │      │          │\n",
       "│ 2      │ julius-caesar_chunk_… │ CAESAR                                                       │ 50   │ 0.3766   │\n",
       "│        │                       │ SOOTHSAYER                                                   │      │          │\n",
       "│        │                       │ ARTEMIDORUS                                                  │      │          │\n",
       "│        │                       │ DECIUS                                                       │      │          │\n",
       "│        │                       │ ARTEMIDORUS                                                  │      │          │\n",
       "│        │                       │ CAESAR                                                       │      │          │\n",
       "│        │                       │ ARTEMIDORUS                                                  │      │          │\n",
       "│        │                       │ CAESAR                                                       │      │          │\n",
       "│        │                       │ PUBLIUS                                                      │      │          │\n",
       "│        │                       │ CASSIUS                                                      │      │          │\n",
       "│        │                       │ Caesar go...                                                 │      │          │\n",
       "│ 3      │ julius-caesar_chunk_… │ 85                                                           │ 47   │ 0.3788   │\n",
       "│        │                       │ Julius Caesar                                                │      │          │\n",
       "│        │                       │ ACT 2. SC. 4                                                 │      │          │\n",
       "│        │                       │ ARTEMIDORUS                                                  │      │          │\n",
       "│        │                       │  Thy lover,                                                  │      │          │\n",
       "│        │                       │  Artemidorus                                                 │      │          │\n",
       "│        │                       │ He exits. PORTIA                                             │      │          │\n",
       "│        │                       │ LUCIUS                                                       │      │          │\n",
       "│        │                       │ PORTIA                                                       │      │          │\n",
       "│        │                       │ En...                                                        │      │          │\n",
       "└────────┴───────────────────────┴──────────────────────────────────────────────────────────────┴──────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run a sample query\n",
    "query = \"What themes are explored in Julius Caesar?\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nResults for query: '{query}'\")\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb314ce",
   "metadata": {},
   "source": [
    "### Natural Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dbc9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20e7f9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001 ['embedText', 'countTextTokens']\n",
      "models/gemini-2.5-pro-preview-03-25 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-05-20 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-06-17 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-05-06 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro-preview-06-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-preview-image-generation ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite-preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-pro-exp-02-05 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts ['countTokens', 'generateContent']\n",
      "models/learnlm-2.0-flash-experimental ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it ['generateContent', 'countTokens']\n",
      "models/gemini-flash-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-flash-lite-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-pro-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image-preview ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-09-2025 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-09-2025 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-robotics-er-1.5-preview ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-computer-use-preview-10-2025 ['generateContent', 'countTokens']\n",
      "models/embedding-001 ['embedContent']\n",
      "models/text-embedding-004 ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-exp ['embedContent', 'countTextTokens', 'countTokens']\n",
      "models/gemini-embedding-001 ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
      "models/aqa ['generateAnswer']\n",
      "models/imagen-4.0-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-generate-001 ['predict']\n",
      "models/imagen-4.0-ultra-generate-001 ['predict']\n",
      "models/imagen-4.0-fast-generate-001 ['predict']\n",
      "models/veo-2.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-generate-preview ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-preview ['predictLongRunning']\n",
      "models/veo-3.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-001 ['predictLongRunning']\n",
      "models/veo-3.1-generate-preview ['predictLongRunning']\n",
      "models/veo-3.1-fast-generate-preview ['predictLongRunning']\n",
      "models/gemini-2.0-flash-live-001 ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-live-2.5-flash-preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-live-preview ['bidiGenerateContent', 'countTokens']\n",
      "models/gemini-2.5-flash-native-audio-latest ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025 ['countTokens', 'bidiGenerateContent']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Set the API key for both genai and LangChain\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAoBDShyFr3QWxKiSVc59n_MkCjSNrZHKg\"\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "for m in genai.list_models():\n",
    "    print(m.name, m.supported_generation_methods)\n",
    "# Initialize Gemini (fixed the model name - using a valid Gemini model)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab6b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Better prompt template for Julius Caesar\n",
    "rag_prompt_template = \"\"\"\n",
    "You are an expert on Shakespeare's Julius Caesar. Answer questions using ONLY the context below.\n",
    "If you can't find a complete answer in the context but see partial information, try to provide what you can find and acknowledge the limitations of the available information.\n",
    "If there is NO relevant information at all in the context, respond with \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (based only on the context provided):\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddfe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb979b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "def answer_with_hybrid_rag(query, n_results=5):\n",
    "    # 1. Semantic search with ChromaDB\n",
    "    semantic_results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # 2. Perform keyword search with BM25\n",
    "    # First get all documents to search across\n",
    "    all_docs = collection.get(\n",
    "        limit=100,  # Adjust based on your collection size\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    # Tokenize for BM25\n",
    "    tokenized_docs = [doc.split() for doc in all_docs[\"documents\"]]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top BM25 results\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[-n_results:][::-1]\n",
    "    \n",
    "    # 3. Combine results (simple union)\n",
    "    combined_docs = []\n",
    "    combined_meta = []\n",
    "    combined_ids = [] \n",
    "    seen_ids = set()\n",
    "    \n",
    "    # Add semantic results\n",
    "    for doc, meta, doc_id in zip(\n",
    "        semantic_results[\"documents\"][0], \n",
    "        semantic_results[\"metadatas\"][0],\n",
    "        semantic_results[\"ids\"][0]\n",
    "    ):\n",
    "        if doc_id not in seen_ids:\n",
    "            combined_docs.append(doc)\n",
    "            combined_meta.append(meta)\n",
    "            combined_ids.append(doc_id)\n",
    "            seen_ids.add(doc_id)\n",
    "    \n",
    "    # Add keyword results\n",
    "    for idx in top_bm25_indices:\n",
    "        doc_id = all_docs[\"ids\"][idx]\n",
    "        if doc_id not in seen_ids:\n",
    "            combined_docs.append(all_docs[\"documents\"][idx])\n",
    "            combined_meta.append(all_docs[\"metadatas\"][idx])\n",
    "            combined_ids.append(doc_id)\n",
    "            seen_ids.add(doc_id)\n",
    "    \n",
    "    # Limit to n_results total\n",
    "    combined_docs = combined_docs[:n_results]\n",
    "    combined_meta = combined_meta[:n_results]\n",
    "    combined_ids = combined_ids[:n_results]\n",
    "    \n",
    "    # Format context and complete RAG as before\n",
    "    formatted_docs = []\n",
    "    for doc, meta in zip(combined_docs, combined_meta):\n",
    "        page_num = meta.get(\"page_number\", \"unknown\")\n",
    "        formatted_docs.append(f\"[Page {page_num}]: {doc}\")\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(formatted_docs)\n",
    "    filled_prompt = prompt.format(context=context, query=query)\n",
    "    response = llm.invoke(filled_prompt)\n",
    "    \n",
    "    # Create a mock results object for print_results compatibility\n",
    "    mock_results = {\n",
    "        \"documents\": [combined_docs],\n",
    "        \"metadatas\": [combined_meta],\n",
    "        \"distances\": [[0.0] * len(combined_docs)],  # Placeholder distances\n",
    "        \"ids\": [combined_ids]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": response.content if hasattr(response, 'content') else str(response),\n",
    "        \"source_documents\": mock_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "960bba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\n",
      "Please retry in 17.797377267s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\nPlease retry in 15.705771989s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 15\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test our RAG pipeline with a question\u001b[39;00m\n\u001b[32m      2\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mWhat is the relationship between Brutus and Caesar?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43manswer_with_hybrid_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36manswer_with_hybrid_rag\u001b[39m\u001b[34m(query, n_results)\u001b[39m\n\u001b[32m     68\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(formatted_docs)\n\u001b[32m     69\u001b[39m filled_prompt = prompt.format(context=context, query=query)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilled_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Create a mock results object for print_results compatibility\u001b[39;00m\n\u001b[32m     73\u001b[39m mock_results = {\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: [combined_docs],\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m: [combined_meta],\n\u001b[32m     76\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdistances\u001b[39m\u001b[33m\"\u001b[39m: [[\u001b[32m0.0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(combined_docs)],  \u001b[38;5;66;03m# Placeholder distances\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m: [combined_ids]\n\u001b[32m     78\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:961\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    936\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     **kwargs: Any,\n\u001b[32m    949\u001b[39m ) -> ChatResult:\n\u001b[32m    950\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    951\u001b[39m         messages,\n\u001b[32m    952\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    960\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:194\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    191\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\nPlease retry in 15.705771989s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 15\n}\n]"
     ]
    }
   ],
   "source": [
    "# Test our RAG pipeline with a question\n",
    "test_query = \"What is the relationship between Brutus and Caesar?\"\n",
    "response = answer_with_hybrid_rag(test_query)\n",
    "\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"\\nAnswer: {response['answer']}\")\n",
    "print(\"\\nSources:\")\n",
    "print_results(response[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Question: What are the main themes in Julius Caesar?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\n",
      "Please retry in 28.807957528s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash-exp\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\nPlease retry in 26.639998217s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 26\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43manswer_with_hybrid_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTop source:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36manswer_with_hybrid_rag\u001b[39m\u001b[34m(query, n_results)\u001b[39m\n\u001b[32m     68\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(formatted_docs)\n\u001b[32m     69\u001b[39m filled_prompt = prompt.format(context=context, query=query)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilled_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Create a mock results object for print_results compatibility\u001b[39;00m\n\u001b[32m     73\u001b[39m mock_results = {\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: [combined_docs],\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m: [combined_meta],\n\u001b[32m     76\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdistances\u001b[39m\u001b[33m\"\u001b[39m: [[\u001b[32m0.0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(combined_docs)],  \u001b[38;5;66;03m# Placeholder distances\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m: [combined_ids]\n\u001b[32m     78\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:961\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    936\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     **kwargs: Any,\n\u001b[32m    949\u001b[39m ) -> ChatResult:\n\u001b[32m    950\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    951\u001b[39m         messages,\n\u001b[32m    952\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    960\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:194\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    191\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\nPlease retry in 26.639998217s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-flash-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 26\n}\n]"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions to evaluate system\n",
    "results_for_export = []\n",
    "\n",
    "test_questions = [\n",
    "    \"What are the main themes in Julius Caesar?\",\n",
    "    \"How does Brutus justify killing Caesar?\",\n",
    "    \"What role does Cassius play in the conspiracy?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Question: {question}\")\n",
    "    response = answer_with_hybrid_rag(question)\n",
    "    print(f\"\\nAnswer: {response['answer']}\")\n",
    "    print(\"\\nTop source:\")\n",
    "    if len(response[\"source_documents\"][\"documents\"][0]) > 0:\n",
    "        top_doc = response[\"source_documents\"][\"documents\"][0][0]\n",
    "        top_meta = response[\"source_documents\"][\"metadatas\"][0][0]\n",
    "        page = top_meta.get(\"page_number\", \"N/A\")\n",
    "        print(f\"[Page {page}]:\\n{top_doc[:200]}...\")  # Print first 200 chars\n",
    "        # Save for export\n",
    "        results_for_export.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response['answer'],\n",
    "            \"page\": page,\n",
    "            \"chunk\": top_doc\n",
    "        })\n",
    "    else:\n",
    "        print(\"No sources found.\")\n",
    "        results_for_export.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response['answer'],\n",
    "            \"page\": None,\n",
    "            \"chunk\": None\n",
    "        })\n",
    "\n",
    "# Export results to a well-formatted text file\n",
    "with open(multiquery_rag_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"RAG Multi-Query Evaluation Results\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    for idx, res in enumerate(results_for_export, 1):\n",
    "        f.write(f\"Question {idx}: {res['question']}\\n\")\n",
    "        f.write(f\"Answer:\\n{res['answer']}\\n\\n\")\n",
    "        if res[\"chunk\"]:\n",
    "            f.write(f\"Top Source Chunk (Page {res['page']}):\\n{res['chunk']}\\n\")\n",
    "        else:\n",
    "            f.write(\"Top Source Chunk: No sources found.\\n\")\n",
    "        f.write(\"-\"*60 + \"\\n\\n\")\n",
    "print(f\"\\nResults exported to {multiquery_rag_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c3905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets ragas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2c56031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Size: 3530 bytes\n",
      "First few characters:\n",
      " [\n",
      "    {\n",
      "        \"question\": \"How does Caesar first enter the play?\",\n",
      "        \"ideal_answer\": \"In a triumphal procession; he has defeated the sons of his deceased rival, Pompey\"\n",
      "    },\n",
      "{\n",
      "\"question\": \"W\n",
      "[INFO] Loaded 25 QA pairs from testbed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Gemini RAG answers: 100%|██████████| 25/25 [05:16<00:00, 12.67s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Evaluating with RAGAS using gemini-2.5-flash ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0c5a0c616947ee89d5d261ae2b8679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[0]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[1]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[2]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[3]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[4]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[5]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[1]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[2]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[3]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[4]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[5]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[6]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[7]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[8]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[9]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[10]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[11]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[12]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[13]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[6]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[7]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[8]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[9]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[10]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[11]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[12]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[13]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[14]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[15]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[14]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[15]: TypeError(object list can't be used in 'await' expression)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 58.475816334s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"When Antony arrives at Caesar\\'s body (in Act 3, Scene 1), he does the following:\\\\n\\\\n*   He expresses deep grief and sorrow, directly addressing Caesar\\'s corpse (\\\\\"Pardon me, Julius!\\\\\").\\\\n*   He laments Caesar\\'s death with poetic and emotional language, comparing him to a \\\\\"bayed hart\\\\\" (a hunted deer) and stating how he lies \\\\\"stricken by many princes.\\\\\"\\\\n*   He expresses regret for shaking the bloody hands of the conspirators in the presence of Caesar\\'s body.\\\\n*   He looks down on Caesar\\'s body, which sways him from his initial purpose of making peace.\\\\n*   He demands reasons from the conspirators for why Caesar was dangerous, stating that otherwise, his death is a \\\\\"savage spectacle.\\\\\"\\\\n*   Later, in Act 3, Scene 2, he instructs his servant that he will personally bear Caesar\\'s corpse into the marketplace for his oration.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 58.112757166s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"After the assassination of Caesar, which of the conspirators addresses the plebeians first?\",\\n    \"answer\": \"Based on the provided text, Antony addresses the plebeians first after Caesar\\'s assassination. The text does not show any of the conspirators addressing the plebeians first.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 58.112757166s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"After the assassination of Caesar, which of the conspirators addresses the plebeians first?\",\\n    \"answer\": \"Based on the provided text, Antony addresses the plebeians first after Caesar\\'s assassination. The text does not show any of the conspirators addressing the plebeians first.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 57.792398598s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the provided text, Antony addresses the plebeians first after Caesar\\'s assassination. The text does not show any of the conspirators addressing the plebeians first.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 57.792398598s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the provided text, Antony addresses the plebeians first after Caesar\\'s assassination. The text does not show any of the conspirators addressing the plebeians first.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 57.46692918s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What is Brutus\\'s explanation for killing Caesar?\",\\n    \"answer\": \"Based on the provided context, Brutus\\'s explanation for killing Caesar is that he **\\\\\"fears that Caesar will become king, destroying the republic.\\\\\"**\\\\n\\\\nThe play excerpts themselves do not contain Brutus\\'s direct explanation for the assassination from his own mouth. The second long speech provided, starting \\\\\"If you have tears, prepare to shed them now,\\\\\" is actually **Antony\\'s famous funeral oration**, where he subtly turns the Roman citizens against the conspirators, rather than Brutus explaining his actions.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 57.46692918s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What is Brutus\\'s explanation for killing Caesar?\",\\n    \"answer\": \"Based on the provided context, Brutus\\'s explanation for killing Caesar is that he **\\\\\"fears that Caesar will become king, destroying the republic.\\\\\"**\\\\n\\\\nThe play excerpts themselves do not contain Brutus\\'s direct explanation for the assassination from his own mouth. The second long speech provided, starting \\\\\"If you have tears, prepare to shed them now,\\\\\" is actually **Antony\\'s famous funeral oration**, where he subtly turns the Roman citizens against the conspirators, rather than Brutus explaining his actions.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 57.150693838s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the provided context, Brutus\\'s explanation for killing Caesar is that he **\\\\\"fears that Caesar will become king, destroying the republic.\\\\\"**\\\\n\\\\nThe play excerpts themselves do not contain Brutus\\'s direct explanation for the assassination from his own mouth. The second long speech provided, starting \\\\\"If you have tears, prepare to shed them now,\\\\\" is actually **Antony\\'s famous funeral oration**, where he subtly turns the Roman citizens against the conspirators, rather than Brutus explaining his actions.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 57.150693838s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the provided context, Brutus\\'s explanation for killing Caesar is that he **\\\\\"fears that Caesar will become king, destroying the republic.\\\\\"**\\\\n\\\\nThe play excerpts themselves do not contain Brutus\\'s direct explanation for the assassination from his own mouth. The second long speech provided, starting \\\\\"If you have tears, prepare to shed them now,\\\\\" is actually **Antony\\'s famous funeral oration**, where he subtly turns the Roman citizens against the conspirators, rather than Brutus explaining his actions.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 56.829890427s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What does Antony tell the crowd?\",\\n    \"answer\": \"Based on the provided text, Antony tells the crowd two main things:\\\\n\\\\n1.  He commands them to stay and listen to him, saying, \\\\\"I do entreat you, not a man depart, Save I alone, till Antony have spoke. Stay, ho, and let us hear Mark Antony!\\\\\"\\\\n2.  He recounts the death of a woman, explaining that she became distraught (\\\\\"fell distract\\\\\") and \\\\\"swallowed fire.\\\\\" He attributes her distress to \\\\\"Impatient of my absence\\\\\" and \\\\\"grief that young Octavius with Mark Antony have made themselves so strong.\\\\\" He concludes by confirming, \\\\\"And died so?\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 56.829890427s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What does Antony tell the crowd?\",\\n    \"answer\": \"Based on the provided text, Antony tells the crowd two main things:\\\\n\\\\n1.  He commands them to stay and listen to him, saying, \\\\\"I do entreat you, not a man depart, Save I alone, till Antony have spoke. Stay, ho, and let us hear Mark Antony!\\\\\"\\\\n2.  He recounts the death of a woman, explaining that she became distraught (\\\\\"fell distract\\\\\") and \\\\\"swallowed fire.\\\\\" He attributes her distress to \\\\\"Impatient of my absence\\\\\" and \\\\\"grief that young Octavius with Mark Antony have made themselves so strong.\\\\\" He concludes by confirming, \\\\\"And died so?\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 56.488014858s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the provided text, Antony tells the crowd two main things:\\\\n\\\\n1.  He commands them to stay and listen to him, saying, \\\\\"I do entreat you, not a man depart, Save I alone, till Antony have spoke. Stay, ho, and let us hear Mark Antony!\\\\\"\\\\n2.  He recounts the death of a woman, explaining that she became distraught (\\\\\"fell distract\\\\\") and \\\\\"swallowed fire.\\\\\" He attributes her distress to \\\\\"Impatient of my absence\\\\\" and \\\\\"grief that young Octavius with Mark Antony have made themselves so strong.\\\\\" He concludes by confirming, \\\\\"And died so?\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 56.488014858s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the provided text, Antony tells the crowd two main things:\\\\n\\\\n1.  He commands them to stay and listen to him, saying, \\\\\"I do entreat you, not a man depart, Save I alone, till Antony have spoke. Stay, ho, and let us hear Mark Antony!\\\\\"\\\\n2.  He recounts the death of a woman, explaining that she became distraught (\\\\\"fell distract\\\\\") and \\\\\"swallowed fire.\\\\\" He attributes her distress to \\\\\"Impatient of my absence\\\\\" and \\\\\"grief that young Octavius with Mark Antony have made themselves so strong.\\\\\" He concludes by confirming, \\\\\"And died so?\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 56.155037293s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What is the crowd\\'s response to Antony\\'s speech?\",\\n    \"answer\": \"The provided text does not describe Antony\\'s speech or the crowd\\'s response to it.\\\\n\\\\nThe first part of the text shows Brutus granting Antony permission to speak at Caesar\\'s funeral, with Cassius expressing concern about it. However, the actual speech and the crowd\\'s reaction are not included.\\\\n\\\\nThe second part of the text involves Antony, Octavius, and a Messenger discussing military strategy for an upcoming battle, not the funeral.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 56.155037293s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What is the crowd\\'s response to Antony\\'s speech?\",\\n    \"answer\": \"The provided text does not describe Antony\\'s speech or the crowd\\'s response to it.\\\\n\\\\nThe first part of the text shows Brutus granting Antony permission to speak at Caesar\\'s funeral, with Cassius expressing concern about it. However, the actual speech and the crowd\\'s reaction are not included.\\\\n\\\\nThe second part of the text involves Antony, Octavius, and a Messenger discussing military strategy for an upcoming battle, not the funeral.\"\\n}\\nOutput: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[16]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[17]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[18]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[19]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[20]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[17]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[18]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[19]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[20]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[21]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[22]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[21]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[22]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[23]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[24]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[25]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[26]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[27]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[28]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[29]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[30]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[23]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[24]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[25]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[26]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[27]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[28]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[29]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[30]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[31]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[31]: TypeError(object list can't be used in 'await' expression)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 55.827858461s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"The provided text does not describe Antony\\'s speech or the crowd\\'s response to it.\\\\n\\\\nThe first part of the text shows Brutus granting Antony permission to speak at Caesar\\'s funeral, with Cassius expressing concern about it. However, the actual speech and the crowd\\'s reaction are not included.\\\\n\\\\nThe second part of the text involves Antony, Octavius, and a Messenger discussing military strategy for an upcoming battle, not the funeral.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 55.220259013s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"Who is Octavius?\",\\n    \"answer\": \"Based on the text provided, Octavius is:\\\\n\\\\n*   **A young, strong leader** who has allied with Mark Antony.\\\\n*   **A co-leader with Antony** in the post-Caesar power structure, involved in making decisions about who will be \\\\\"pricked to die in our black sentence and proscription.\\\\\"\\\\n*   **Part of the triumvirate** (along with Antony and Lepidus, though Antony has a low opinion of Lepidus), currently in Rome.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 55.220259013s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"Who is Octavius?\",\\n    \"answer\": \"Based on the text provided, Octavius is:\\\\n\\\\n*   **A young, strong leader** who has allied with Mark Antony.\\\\n*   **A co-leader with Antony** in the post-Caesar power structure, involved in making decisions about who will be \\\\\"pricked to die in our black sentence and proscription.\\\\\"\\\\n*   **Part of the triumvirate** (along with Antony and Lepidus, though Antony has a low opinion of Lepidus), currently in Rome.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 54.863148033s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the text provided, Octavius is:\\\\n\\\\n*   **A young, strong leader** who has allied with Mark Antony.\\\\n*   **A co-leader with Antony** in the post-Caesar power structure, involved in making decisions about who will be \\\\\"pricked to die in our black sentence and proscription.\\\\\"\\\\n*   **Part of the triumvirate** (along with Antony and Lepidus, though Antony has a low opinion of Lepidus), currently in Rome.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 54.863148033s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the text provided, Octavius is:\\\\n\\\\n*   **A young, strong leader** who has allied with Mark Antony.\\\\n*   **A co-leader with Antony** in the post-Caesar power structure, involved in making decisions about who will be \\\\\"pricked to die in our black sentence and proscription.\\\\\"\\\\n*   **Part of the triumvirate** (along with Antony and Lepidus, though Antony has a low opinion of Lepidus), currently in Rome.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 54.519059018s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"Octavius and Antony join together with whom?\",\\n    \"answer\": \"Octavius and Antony join together with **Lepidus**.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 54.519059018s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"Octavius and Antony join together with whom?\",\\n    \"answer\": \"Octavius and Antony join together with **Lepidus**.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 54.205503861s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Octavius and Antony join together with **Lepidus**.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 54.205503861s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Octavius and Antony join together with **Lepidus**.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 53.584077921s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"Why do Brutus and Cassius argue?\",\\n    \"answer\": \"Brutus and Cassius argue over two main issues:\\\\n\\\\n1.  **Lucius Pella:** Cassius is upset that Brutus condemned Lucius Pella for taking bribes from the Sardians, and that Brutus \\\\\"slighted off\\\\\" (ignored) Cassius\\'s letters defending Pella. Brutus believes Cassius \\\\\"wronged himself to write in such a case\\\\\" (by defending a corrupt man).\\\\n2.  **Denial of Gold:** Cassius accuses Brutus of denying him \\\\\"certain sums of gold\\\\\" which he needed to pay his legions, stating he cannot raise money by \\\\\"vile means\\\\\" like extorting peasants. Brutus vehemently denies this accusation, stating, \\\\\"I denied you not.\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 53.584077921s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"Why do Brutus and Cassius argue?\",\\n    \"answer\": \"Brutus and Cassius argue over two main issues:\\\\n\\\\n1.  **Lucius Pella:** Cassius is upset that Brutus condemned Lucius Pella for taking bribes from the Sardians, and that Brutus \\\\\"slighted off\\\\\" (ignored) Cassius\\'s letters defending Pella. Brutus believes Cassius \\\\\"wronged himself to write in such a case\\\\\" (by defending a corrupt man).\\\\n2.  **Denial of Gold:** Cassius accuses Brutus of denying him \\\\\"certain sums of gold\\\\\" which he needed to pay his legions, stating he cannot raise money by \\\\\"vile means\\\\\" like extorting peasants. Brutus vehemently denies this accusation, stating, \\\\\"I denied you not.\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 53.24695321s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Brutus and Cassius argue over two main issues:\\\\n\\\\n1.  **Lucius Pella:** Cassius is upset that Brutus condemned Lucius Pella for taking bribes from the Sardians, and that Brutus \\\\\"slighted off\\\\\" (ignored) Cassius\\'s letters defending Pella. Brutus believes Cassius \\\\\"wronged himself to write in such a case\\\\\" (by defending a corrupt man).\\\\n2.  **Denial of Gold:** Cassius accuses Brutus of denying him \\\\\"certain sums of gold\\\\\" which he needed to pay his legions, stating he cannot raise money by \\\\\"vile means\\\\\" like extorting peasants. Brutus vehemently denies this accusation, stating, \\\\\"I denied you not.\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 53.24695321s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Brutus and Cassius argue over two main issues:\\\\n\\\\n1.  **Lucius Pella:** Cassius is upset that Brutus condemned Lucius Pella for taking bribes from the Sardians, and that Brutus \\\\\"slighted off\\\\\" (ignored) Cassius\\'s letters defending Pella. Brutus believes Cassius \\\\\"wronged himself to write in such a case\\\\\" (by defending a corrupt man).\\\\n2.  **Denial of Gold:** Cassius accuses Brutus of denying him \\\\\"certain sums of gold\\\\\" which he needed to pay his legions, stating he cannot raise money by \\\\\"vile means\\\\\" like extorting peasants. Brutus vehemently denies this accusation, stating, \\\\\"I denied you not.\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 52.888309615s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What news do Brutus and Cassius receive from Rome?\",\\n    \"answer\": \"Based on the provided text excerpts, there is **no mention of Brutus and Cassius receiving any news from Rome.**\\\\n\\\\nThe excerpts show:\\\\n*   Act 5, Scene 4: Brutus reacting to the death of Cato during battle and preparing for another fight.\\\\n*   Act 2, Scene 1: Brutus and Cassius (and other conspirators) planning to bring Caesar to the Capitol.\\\\n*   Act 4, Scene 3: Brutus and Cassius arguing in Brutus\\'s tent about the condemnation of Lucius Pella.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 52.888309615s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What news do Brutus and Cassius receive from Rome?\",\\n    \"answer\": \"Based on the provided text excerpts, there is **no mention of Brutus and Cassius receiving any news from Rome.**\\\\n\\\\nThe excerpts show:\\\\n*   Act 5, Scene 4: Brutus reacting to the death of Cato during battle and preparing for another fight.\\\\n*   Act 2, Scene 1: Brutus and Cassius (and other conspirators) planning to bring Caesar to the Capitol.\\\\n*   Act 4, Scene 3: Brutus and Cassius arguing in Brutus\\'s tent about the condemnation of Lucius Pella.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 52.559143792s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the provided text excerpts, there is **no mention of Brutus and Cassius receiving any news from Rome.**\\\\n\\\\nThe excerpts show:\\\\n*   Act 5, Scene 4: Brutus reacting to the death of Cato during battle and preparing for another fight.\\\\n*   Act 2, Scene 1: Brutus and Cassius (and other conspirators) planning to bring Caesar to the Capitol.\\\\n*   Act 4, Scene 3: Brutus and Cassius arguing in Brutus\\'s tent about the condemnation of Lucius Pella.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 52.559143792s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the provided text excerpts, there is **no mention of Brutus and Cassius receiving any news from Rome.**\\\\n\\\\nThe excerpts show:\\\\n*   Act 5, Scene 4: Brutus reacting to the death of Cato during battle and preparing for another fight.\\\\n*   Act 2, Scene 1: Brutus and Cassius (and other conspirators) planning to bring Caesar to the Capitol.\\\\n*   Act 4, Scene 3: Brutus and Cassius arguing in Brutus\\'s tent about the condemnation of Lucius Pella.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 52.218479194s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What appears at Brutus\\'s bedside in camp?\",\\n    \"answer\": \"At Brutus\\'s bedside in his tent appears:\\\\n*   **Lucius**, his page, who gives him an \\\\\"instrument\\\\\" (likely a musical one).\\\\n*   **Varro and Claudius**, two of Brutus\\'s men, whom he asks to sleep on cushions in his tent. They lie down there.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 52.218479194s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What appears at Brutus\\'s bedside in camp?\",\\n    \"answer\": \"At Brutus\\'s bedside in his tent appears:\\\\n*   **Lucius**, his page, who gives him an \\\\\"instrument\\\\\" (likely a musical one).\\\\n*   **Varro and Claudius**, two of Brutus\\'s men, whom he asks to sleep on cushions in his tent. They lie down there.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 51.789925216s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"At Brutus\\'s bedside in his tent appears:\\\\n*   **Lucius**, his page, who gives him an \\\\\"instrument\\\\\" (likely a musical one).\\\\n*   **Varro and Claudius**, two of Brutus\\'s men, whom he asks to sleep on cushions in his tent. They lie down there.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 51.789925216s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"At Brutus\\'s bedside in his tent appears:\\\\n*   **Lucius**, his page, who gives him an \\\\\"instrument\\\\\" (likely a musical one).\\\\n*   **Varro and Claudius**, two of Brutus\\'s men, whom he asks to sleep on cushions in his tent. They lie down there.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 51.460633645s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What does Cassius think has happened to his and Brutus\\'s armies?\",\\n    \"answer\": \"Based on the text:\\\\n\\\\nCassius knows his army has arrived with him (\\\\\"Enter Cassius and his powers\\\\\"). His primary concern regarding the armies, as revealed in the text, is the **lack of funds to pay his soldiers**. He recounts sending Brutus a request for gold because he could not raise money by \\\\\"vile means,\\\\\" implicitly to pay his legions (which Brutus later confirms was his own purpose for asking for gold).\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 51.460633645s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What does Cassius think has happened to his and Brutus\\'s armies?\",\\n    \"answer\": \"Based on the text:\\\\n\\\\nCassius knows his army has arrived with him (\\\\\"Enter Cassius and his powers\\\\\"). His primary concern regarding the armies, as revealed in the text, is the **lack of funds to pay his soldiers**. He recounts sending Brutus a request for gold because he could not raise money by \\\\\"vile means,\\\\\" implicitly to pay his legions (which Brutus later confirms was his own purpose for asking for gold).\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 51.133966278s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the text:\\\\n\\\\nCassius knows his army has arrived with him (\\\\\"Enter Cassius and his powers\\\\\"). His primary concern regarding the armies, as revealed in the text, is the **lack of funds to pay his soldiers**. He recounts sending Brutus a request for gold because he could not raise money by \\\\\"vile means,\\\\\" implicitly to pay his legions (which Brutus later confirms was his own purpose for asking for gold).\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 51.133966278s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Based on the text:\\\\n\\\\nCassius knows his army has arrived with him (\\\\\"Enter Cassius and his powers\\\\\"). His primary concern regarding the armies, as revealed in the text, is the **lack of funds to pay his soldiers**. He recounts sending Brutus a request for gold because he could not raise money by \\\\\"vile means,\\\\\" implicitly to pay his legions (which Brutus later confirms was his own purpose for asking for gold).\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 50.576827985s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What is Cassius\\'s response to this situation?\",\\n    \"answer\": \"Cassius\\'s response is one of deep grievance and accusation. He feels Brutus has wronged him in two significant ways:\\\\n\\\\n1.  **Regarding Lucius Pella:** He states that Brutus wronged him by condemning Lucius Pella for taking bribes and, more importantly, by \\\\\"slight[ing] off\\\\\" Cassius\\'s letters that prayed on Pella\\'s side. Cassius feels personally disrespected and his influence disregarded.\\\\n2.  **Regarding Gold:** He vehemently complains that Brutus denied him the \\\\\"certain sums of gold\\\\\" he requested to pay his legions. Cassius feels this denial was a serious slight, questioning Brutus\\'s actions with \\\\\"Was that done like Cassius? Should I have answered Caius Cassius so?\\\\\" He highlights his own inability to raise money by \\\\\"vile means\\\\\" as justification for needing Brutus\\'s help.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 50.576827985s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What is Cassius\\'s response to this situation?\",\\n    \"answer\": \"Cassius\\'s response is one of deep grievance and accusation. He feels Brutus has wronged him in two significant ways:\\\\n\\\\n1.  **Regarding Lucius Pella:** He states that Brutus wronged him by condemning Lucius Pella for taking bribes and, more importantly, by \\\\\"slight[ing] off\\\\\" Cassius\\'s letters that prayed on Pella\\'s side. Cassius feels personally disrespected and his influence disregarded.\\\\n2.  **Regarding Gold:** He vehemently complains that Brutus denied him the \\\\\"certain sums of gold\\\\\" he requested to pay his legions. Cassius feels this denial was a serious slight, questioning Brutus\\'s actions with \\\\\"Was that done like Cassius? Should I have answered Caius Cassius so?\\\\\" He highlights his own inability to raise money by \\\\\"vile means\\\\\" as justification for needing Brutus\\'s help.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 50.253292981s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Cassius\\'s response is one of deep grievance and accusation. He feels Brutus has wronged him in two significant ways:\\\\n\\\\n1.  **Regarding Lucius Pella:** He states that Brutus wronged him by condemning Lucius Pella for taking bribes and, more importantly, by \\\\\"slight[ing] off\\\\\" Cassius\\'s letters that prayed on Pella\\'s side. Cassius feels personally disrespected and his influence disregarded.\\\\n2.  **Regarding Gold:** He vehemently complains that Brutus denied him the \\\\\"certain sums of gold\\\\\" he requested to pay his legions. Cassius feels this denial was a serious slight, questioning Brutus\\'s actions with \\\\\"Was that done like Cassius? Should I have answered Caius Cassius so?\\\\\" He highlights his own inability to raise money by \\\\\"vile means\\\\\" as justification for needing Brutus\\'s help.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 50.253292981s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 50\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"Cassius\\'s response is one of deep grievance and accusation. He feels Brutus has wronged him in two significant ways:\\\\n\\\\n1.  **Regarding Lucius Pella:** He states that Brutus wronged him by condemning Lucius Pella for taking bribes and, more importantly, by \\\\\"slight[ing] off\\\\\" Cassius\\'s letters that prayed on Pella\\'s side. Cassius feels personally disrespected and his influence disregarded.\\\\n2.  **Regarding Gold:** He vehemently complains that Brutus denied him the \\\\\"certain sums of gold\\\\\" he requested to pay his legions. Cassius feels this denial was a serious slight, questioning Brutus\\'s actions with \\\\\"Was that done like Cassius? Should I have answered Caius Cassius so?\\\\\" He highlights his own inability to raise money by \\\\\"vile means\\\\\" as justification for needing Brutus\\'s help.\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 49.935119176s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What does Brutus do when he sees the battle is lost?\",\\n    \"answer\": \"When Brutus contemplates the possibility of losing the battle, he initially states his philosophy of *not* committing suicide like Cato, intending to \\\\\"arm myself with patience to stay the providence of some high powers that govern us below.\\\\\"\\\\n\\\\nHowever, immediately after, when Cassius asks if he is content to be led in triumph through the streets of Rome if they lose, Brutus emphatically declares: \\\\\"No, Cassius, no. Think not, thou noble Roman, That ever Brutus will go bound to Rome. He bears too great a mind.\\\\\"\\\\n\\\\nTherefore, when he sees the battle is lost (or considers the possibility), Brutus is determined **not to be captured and led as a prisoner in a Roman triumph.**\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 49.935119176s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What does Brutus do when he sees the battle is lost?\",\\n    \"answer\": \"When Brutus contemplates the possibility of losing the battle, he initially states his philosophy of *not* committing suicide like Cato, intending to \\\\\"arm myself with patience to stay the providence of some high powers that govern us below.\\\\\"\\\\n\\\\nHowever, immediately after, when Cassius asks if he is content to be led in triumph through the streets of Rome if they lose, Brutus emphatically declares: \\\\\"No, Cassius, no. Think not, thou noble Roman, That ever Brutus will go bound to Rome. He bears too great a mind.\\\\\"\\\\n\\\\nTherefore, when he sees the battle is lost (or considers the possibility), Brutus is determined **not to be captured and led as a prisoner in a Roman triumph.**\"\\n}\\nOutput: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[32]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[33]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[34]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[35]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[36]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[37]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[33]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[34]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[35]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[36]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[37]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[38]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[38]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[39]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[40]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[41]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[39]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[40]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[41]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[42]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[43]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[44]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[45]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[46]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[47]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[42]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[43]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[44]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[45]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[46]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[47]: TypeError(object list can't be used in 'await' expression)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 49.622095392s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"When Brutus contemplates the possibility of losing the battle, he initially states his philosophy of *not* committing suicide like Cato, intending to \\\\\"arm myself with patience to stay the providence of some high powers that govern us below.\\\\\"\\\\n\\\\nHowever, immediately after, when Cassius asks if he is content to be led in triumph through the streets of Rome if they lose, Brutus emphatically declares: \\\\\"No, Cassius, no. Think not, thou noble Roman, That ever Brutus will go bound to Rome. He bears too great a mind.\\\\\"\\\\n\\\\nTherefore, when he sees the battle is lost (or considers the possibility), Brutus is determined **not to be captured and led as a prisoner in a Roman triumph.**\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 49.206473517s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What does Antony call Brutus at the end?\",\\n    \"answer\": \"In Act 5, Scene 4, when Antony is told that Brutus has been taken, and after Lucilius reveals he is not Brutus but someone valuable pretending to be him, Antony refers to Lucilius (and by implication, Brutus himself, if he were captured) as \\\\\"**A prize no less in worth**.\\\\\"\"\\n}\\nOutput: '\n",
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 49.206473517s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"question\": \"Who was Albert Einstein and what is he best known for?\",\\n    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\\n}\\nOutput: {\\n    \"statements\": [\\n        \"Albert Einstein was a German-born theoretical physicist.\",\\n        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\\n        \"Albert Einstein was best known for developing the theory of relativity.\",\\n        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\\n    ]\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"question\": \"What does Antony call Brutus at the end?\",\\n    \"answer\": \"In Act 5, Scene 4, when Antony is told that Brutus has been taken, and after Lucilius reveals he is not Brutus but someone valuable pretending to be him, Antony refers to Lucilius (and by implication, Brutus himself, if he were captured) as \\\\\"**A prize no less in worth**.\\\\\"\"\\n}\\nOutput: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[48]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[49]: TypeError(object list can't be used in 'await' expression)\n",
      "Exception raised in Job[49]: TypeError(object list can't be used in 'await' expression)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed on prompt 0: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 48.871522556s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 48\n",
      "}\n",
      "]\n",
      "[DEBUG] Prompt type: <class 'langchain_core.prompt_values.StringPromptValue'>, Prompt value: text='Generate a question for the given answer and Identify if answer is noncommittal. Give noncommittal as 1 if the answer is noncommittal and 0 if the answer is committal. A noncommittal answer is one that is evasive, vague, or ambiguous. For example, \"I don\\'t know\" or \"I\\'m not sure\" are noncommittal answers\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema:\\n{\"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}, \"noncommittal\": {\"title\": \"Noncommittal\", \"type\": \"integer\"}}, \"required\": [\"question\", \"noncommittal\"], \"title\": \"ResponseRelevanceOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\\n\\n--------EXAMPLES-----------\\nExample 1\\nInput: {\\n    \"response\": \"Albert Einstein was born in Germany.\"\\n}\\nOutput: {\\n    \"question\": \"Where was Albert Einstein born?\",\\n    \"noncommittal\": 0\\n}\\n\\nExample 2\\nInput: {\\n    \"response\": \"I don\\'t know about the  groundbreaking feature of the smartphone invented in 2023 as am unaware of information beyond 2022. \"\\n}\\nOutput: {\\n    \"question\": \"What was the groundbreaking feature of the smartphone invented in 2023?\",\\n    \"noncommittal\": 1\\n}\\n-----------------------------\\n\\nNow perform the same with the following input\\ninput: {\\n    \"response\": \"In Act 5, Scene 4, when Antony is told that Brutus has been taken, and after Lucilius reveals he is not Brutus but someone valuable pretending to be him, Antony refers to Lucilius (and by implication, Brutus himself, if he were captured) as \\\\\"**A prize no less in worth**.\\\\\"\"\\n}\\nOutput: '\n",
      "\n",
      "✅ Evaluation complete! Metrics saved to '../RAG Results/multiquery_rag_metrics.txt'\n",
      "Faithfulness (avg): nan | Answer Relevancy (avg): nan\n",
      "\n",
      "✅ Evaluation complete! Metrics saved to '../RAG Results/multiquery_rag_metrics.txt'\n",
      "Faithfulness (avg): nan | Answer Relevancy (avg): nan\n"
     ]
    }
   ],
   "source": [
    "# === Gemini + RAG + RAGAS Evaluation (no LangChain) ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets google-generativeai tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import google.generativeai as genai\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# ==== CONFIG ====\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAoBDShyFr3QWxKiSVc59n_MkCjSNrZHKg\"\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# for m in genai.list_models():\n",
    "#     print(m.name, m.supported_generation_methods)\n",
    "\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "TOP_K = 3\n",
    "\n",
    "GEMINI_RAG_MODEL = \"gemini-2.5-flash\"\n",
    "GEMINI_RAGAS_MODEL = \"gemini-2.5-flash\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Rate limiting config (10 RPM = 6 seconds between requests)\n",
    "REQUEST_DELAY = 6.5  # seconds between API calls (adding buffer to be safe)\n",
    "\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_200 = f.read(200)\n",
    "print(\"First few characters:\\n\", first_200)\n",
    "\n",
    "\n",
    "# ==== 1️⃣ Load test data ====\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "\n",
    "# ==== 2️⃣ Gemini generation ====\n",
    "def generate_with_gemini(prompt, model_name=GEMINI_RAG_MODEL):\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        time.sleep(REQUEST_DELAY)  # Rate limit: 10 RPM\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Gemini API call failed: {e}\")\n",
    "        time.sleep(REQUEST_DELAY)  # Still wait even on error\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==== 3️⃣ Gemini wrapper for RAGAS ====\n",
    "class GeminiRagasLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def generate(self, prompts, **kwargs):  # ✅ Accept extra args like n=1\n",
    "        \"\"\"Generate outputs for one or many prompts.\n",
    "        RAGAS may pass a single prompt (StringPromptValue), a tuple like (\"text\", content),\n",
    "        or a list of such prompts. This wrapper normalizes them and extracts plain text.\"\"\"\n",
    "        # Normalize to a list of prompts\n",
    "        if isinstance(prompts, list):\n",
    "            prompts_list = prompts\n",
    "        elif isinstance(prompts, tuple) and len(prompts) == 2 and isinstance(prompts[0], str):\n",
    "            # A single (\"text\", content) tuple\n",
    "            prompts_list = [prompts]\n",
    "        else:\n",
    "            # Single prompt object (e.g., StringPromptValue) or other iterable\n",
    "            try:\n",
    "                # If it's an iterable but NOT a string-like prompt object, cast to list\n",
    "                if hasattr(prompts, \"to_string\") or hasattr(prompts, \"text\") or isinstance(prompts, (str, dict)):\n",
    "                    prompts_list = [prompts]\n",
    "                else:\n",
    "                    # Attempt converting to list; fallback to single-item list\n",
    "                    prompts_list = list(prompts)\n",
    "            except TypeError:\n",
    "                prompts_list = [prompts]\n",
    "\n",
    "        model = genai.GenerativeModel(self.model_name)\n",
    "        outputs = []\n",
    "\n",
    "        def to_text(p):\n",
    "            # LangChain prompt values\n",
    "            if hasattr(p, \"to_string\"):\n",
    "                try:\n",
    "                    return p.to_string()\n",
    "                except Exception:\n",
    "                    # Some PromptValue variants use .to_string with no args\n",
    "                    return str(p)\n",
    "            if hasattr(p, \"text\"):\n",
    "                try:\n",
    "                    return p.text\n",
    "                except Exception:\n",
    "                    return str(p)\n",
    "            # Tuple formats like (\"text\", content)\n",
    "            if isinstance(p, tuple):\n",
    "                if len(p) == 2 and isinstance(p[1], str):\n",
    "                    return p[1]\n",
    "                return \" \".join(str(x) for x in p)\n",
    "            # Dict formats\n",
    "            if isinstance(p, dict):\n",
    "                return p.get(\"text\") or p.get(\"content\") or json.dumps(p, ensure_ascii=False)\n",
    "            # Plain string\n",
    "            if isinstance(p, str):\n",
    "                return p\n",
    "            # Fallback\n",
    "            return str(p)\n",
    "\n",
    "        for i, p in enumerate(prompts_list):\n",
    "            try:\n",
    "                prompt_text = to_text(p)\n",
    "                r = model.generate_content(prompt_text)\n",
    "                outputs.append(r.text.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed on prompt {i}: {e}\")\n",
    "                print(f\"[DEBUG] Prompt type: {type(p)}, Prompt value: {p}\")\n",
    "                outputs.append(f\"[Error: {e}]\")\n",
    "            # Add delay between requests (except after the last one)\n",
    "            if i < len(prompts_list) - 1:\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# ==== 4️⃣ Check collection availability ====\n",
    "try:\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB) is NOT defined.\")\n",
    "    print(\"Please initialize your ChromaDB client/collection before running this cell.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# ==== 5️⃣ Generate records ====\n",
    "records = []\n",
    "for item in tqdm(test_data, desc=\"Generating Gemini RAG answers\"):\n",
    "    question = item[\"question\"]\n",
    "    ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "    retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "    retrieved_docs = retrieved[\"documents\"][0]\n",
    "    retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    generated_answer = generate_with_gemini(prompt)\n",
    "    if not generated_answer:\n",
    "        generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "    records.append({\n",
    "        \"question\": question,\n",
    "        \"contexts\": retrieved_docs,\n",
    "        \"answer\": generated_answer,\n",
    "        \"ground_truth\": ideal_answer,  # ✅ single string is fine\n",
    "    })\n",
    "\n",
    "\n",
    "# ==== 6️⃣ Convert to HF Dataset ====\n",
    "dataset = Dataset.from_list(records)\n",
    "\n",
    "\n",
    "# ==== 7️⃣ Custom HuggingFace Embedding Wrapper ====\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    \"\"\"Implements both sync + async embedding methods for latest RAGAS.\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        # ✅ Do not call super()\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    # --- Sync methods ---\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False).tolist()[0]\n",
    "\n",
    "    # --- Async methods ---\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "\n",
    "# ==== 8️⃣ Evaluate with RAGAS ====\n",
    "llm = GeminiRagasLLM(GEMINI_RAGAS_MODEL)\n",
    "embeddings = CustomHuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "print(f\"\\n[INFO] Evaluating with RAGAS using {GEMINI_RAGAS_MODEL} ...\")\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "\n",
    "# ==== 9️⃣ Save Results ====\n",
    "faithfulness_scores = results[\"faithfulness\"]\n",
    "answer_relevancy_scores = results[\"answer_relevancy\"]\n",
    "\n",
    "# ✅ Compute mean values\n",
    "faithfulness_mean = float(np.mean(faithfulness_scores))\n",
    "answer_relevancy_mean = float(np.mean(answer_relevancy_scores))\n",
    "\n",
    "os.makedirs(os.path.dirname(output_metrics_path), exist_ok=True)\n",
    "\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Gemini + RAGAS) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GEMINI_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GEMINI_RAGAS_MODEL}\\n\\n\")\n",
    "    f.write(f\"Faithfulness (avg): {faithfulness_mean:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy (avg): {answer_relevancy_mean:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness (avg): {faithfulness_mean:.4f} | Answer Relevancy (avg): {answer_relevancy_mean:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
