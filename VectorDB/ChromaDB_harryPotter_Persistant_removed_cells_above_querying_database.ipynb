{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd26bf01",
   "metadata": {},
   "source": [
    "### Imports and Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d607ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/harry_potter_chunks_hierarchical.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"harry_potter_collection\"\n",
    "# # Set API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b927c",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9b8be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: C:\\Users\\Gaming window\\Desktop\\ANLP_Assignment_2\\RAG-A2\\VectorDB\\chroma_Data\n",
      "Existing collections: ['harry_potter_collection']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788e6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 160 chunks from '../Chunking/Chunk_files/harry_potter_chunks_hierarchical.pkl'.\n",
      "\n",
      "Here is the metadata of a loaded chunk:\n",
      "{'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 5, 'chunk_type': 'section', 'chunk_level': 1, 'section_id': 'page_5_section_0', 'parent_id': 'page_5', 'chunk_index': 0, 'c': 'hierarchical_section', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# No need for fitz or RecursiveCharacterTextSplitter here, as we are loading from a file.\n",
    "\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f: # 'rb' mode for reading in binary\n",
    "        loaded_docs = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Now you can inspect the loaded documents to verify.\n",
    "print(\"\\nHere is the metadata of a loaded chunk:\")\n",
    "if loaded_docs:\n",
    "    print(loaded_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cbb00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use default SentenceTransformer for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928a1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gaming window\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function initialized with model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Install if needed\n",
    "# !pip install sentence_transformers\n",
    "\n",
    "# Set up embedding function\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding function initialized with model: all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f44b",
   "metadata": {},
   "source": [
    "### Creating new Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b34eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'harry_potter_collection' created or accessed successfully\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# Get or create the collection\n",
    "client.delete_collection(name=collection_name)  \n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\n",
    "        \"description\": \"Harry Potter book chunks\",\n",
    "        \"created\": str(datetime.now())\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' created or accessed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b449ba",
   "metadata": {},
   "source": [
    "### Add data to collection\n",
    "The chunks have to be given an id and added to the collection now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa83664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch: 0 to 159 (160 items)\n",
      "Successfully added 160 documents to collection 'harry_potter_collection'\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Prepare documents for ChromaDB\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "# Process each loaded document chunk\n",
    "for i, doc in enumerate(loaded_docs):\n",
    "    # Generate a unique ID (you could use a more deterministic approach if needed)\n",
    "    doc_id = f\"hp_chunk_{i}\"\n",
    "    \n",
    "    # Get the document text\n",
    "    document_text = doc.page_content\n",
    "    \n",
    "    # Get the document metadata\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Add to our lists\n",
    "    ids.append(doc_id)\n",
    "    documents.append(document_text)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Add documents in batches to avoid memory issues\n",
    "batch_size = 500\n",
    "total_added = 0\n",
    "\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    end_idx = min(i + batch_size, len(ids))\n",
    "    \n",
    "    # collection.update(\n",
    "    #     ids=ids[i:end_idx],\n",
    "    #     documents=documents[i:end_idx],\n",
    "    #     metadatas=metadatas[i:end_idx]\n",
    "    # )\n",
    "    collection.add(\n",
    "        ids=ids[i:end_idx],\n",
    "        documents=documents[i:end_idx],\n",
    "        metadatas=metadatas[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    total_added += end_idx - i\n",
    "    print(f\"Added batch: {i} to {end_idx-1} ({end_idx-i} items)\")\n",
    "\n",
    "print(f\"Successfully added {total_added} documents to collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fd6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 160\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "--- Document 1 ---\n",
      "ID: hp_chunk_0\n",
      "Text: Hamlet: “O farewell, honest soldier.  Who hath relieved/you?”). At\n",
      "any point in the text, you can ho...\n",
      "Metadata: {'section_id': 'page_5_section_0', 'page_number': 5, 'c': 'hierarchical_section', 'parent_id': 'page_5', 'chunk_level': 1, 'ischunk': True, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'chunk_type': 'section', 'chunk_index': 0}\n",
      "\n",
      "--- Document 2 ---\n",
      "ID: hp_chunk_1\n",
      "Text: Hamlet: “O farewell, honest soldier.  Who hath relieved/you?”). At\n",
      "any point in the text, you can ho...\n",
      "Metadata: {'section_id': 'page_5_section_0', 'parent_id': 'page_5_section_0', 'paragraph_id': 'page_5_section_0_para_0', 'chunk_index': 0, 'ischunk': True, 'c': 'hierarchical_paragraph', 'page_number': 5, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'chunk_type': 'paragraph', 'chunk_level': 2}\n",
      "\n",
      "--- Document 3 ---\n",
      "ID: hp_chunk_2\n",
      "Text: FLAVIUS\n",
      "CARPENTER\n",
      "MARULLUS\n",
      "COBBLER\n",
      "MARULLUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "Enter Flavius, Marullus, and certain Com...\n",
      "Metadata: {'chunk_type': 'paragraph', 'parent_id': 'page_9_section_0', 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 9, 'chunk_level': 2, 'paragraph_id': 'page_9_section_0_para_0', 'c': 'hierarchical_paragraph', 'chunk_index': 0, 'ischunk': True, 'section_id': 'page_9_section_0'}\n"
     ]
    }
   ],
   "source": [
    "# Check collection count\n",
    "count = collection.count()\n",
    "print(f\"Total documents in collection: {count}\")\n",
    "\n",
    "# Peek at the first few entries\n",
    "peek = collection.peek(limit=3)\n",
    "print(\"\\nSample entries:\")\n",
    "for i, (doc_id, doc_text, metadata) in enumerate(zip(\n",
    "    peek['ids'], peek['documents'], peek['metadatas']\n",
    ")):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Text: {doc_text[:100]}...\")\n",
    "    print(f\"Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ce07b",
   "metadata": {},
   "source": [
    "## Quantitative Analysis using RAGAs: Faithfulness and Answer Relevency: using OLLaMA for RAG and HuggingFace Zephyr for RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c56031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluate RAG pipeline using RAGAS\n",
    "# Faithfulness & Answer Relevancy\n",
    "# Ollama for RAG generation | Zephyr for offline RAGAS evaluation\n",
    "\n",
    "# ==== INSTALL DEPENDENCIES ====\n",
    "# !pip install ragas datasets transformers accelerate sentence-transformers tqdm\n",
    "# Make sure you have Ollama installed: https://ollama.ai/download\n",
    "# Example to pull a model: `ollama pull llama3` or `ollama pull mistral`\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.llms import HuggingfaceLLM\n",
    "from ragas.embeddings import HuggingfaceEmbeddings\n",
    "\n",
    "# ==== CONFIG ====\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "TOP_K = 3\n",
    "\n",
    "# --- LLMs ---\n",
    "OLLAMA_MODEL = \"llama3\"                     # for RAG generation (local via Ollama)\n",
    "LLM_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"  # for RAGAS evaluation (offline)\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # for embeddings\n",
    "# =================\n",
    "\n",
    "# 1️⃣ Load test data\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "# 2️⃣ Function to generate RAG answers using Ollama\n",
    "def generate_with_ollama(prompt, model_name=OLLAMA_MODEL):\n",
    "    \"\"\"\n",
    "    Generate a response using a local Ollama model.\n",
    "    Assumes Ollama is installed and the model is already pulled.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run the Ollama CLI\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=120\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.strip()\n",
    "        else:\n",
    "            print(f\"[WARN] Ollama returned error: {result.stderr}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ollama call failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# 3️⃣ Prepare evaluation records\n",
    "records = []\n",
    "for item in tqdm(test_data, desc=\"Generating Ollama RAG answers\"):\n",
    "    question = item[\"question\"]\n",
    "    ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "    # --- Retrieve from Chroma ---\n",
    "    retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "    retrieved_docs = retrieved[\"documents\"][0]\n",
    "    retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # --- Build RAG prompt ---\n",
    "    prompt = (\n",
    "        f\"You are a helpful assistant. \"\n",
    "        f\"Use only the information provided in the context below to answer the question.\\n\\n\"\n",
    "        f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # --- Generate answer using Ollama ---\n",
    "    generated_answer = generate_with_ollama(prompt)\n",
    "    if not generated_answer:\n",
    "        generated_answer = f\"[Fallback mock answer] {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "    # --- Add record for RAGAS evaluation ---\n",
    "    records.append({\n",
    "        \"question\": question,\n",
    "        \"contexts\": retrieved_docs,\n",
    "        \"answer\": generated_answer,\n",
    "        \"ground_truth\": [ideal_answer],\n",
    "    })\n",
    "\n",
    "# 4️⃣ Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(records)\n",
    "\n",
    "# 5️⃣ Initialize Zephyr & embedding models for RAGAS (offline)\n",
    "llm = HuggingfaceLLM(model=LLM_MODEL)\n",
    "embeddings = HuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# 6️⃣ Evaluate using RAGAS\n",
    "print(f\"\\n[INFO] Evaluating with RAGAS (Faithfulness & Answer Relevancy) using {LLM_MODEL} ...\")\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "# 7️⃣ Extract scores\n",
    "faithfulness_score = results[\"faithfulness\"]\n",
    "answer_relevancy_score = results[\"answer_relevancy\"]\n",
    "\n",
    "# 8️⃣ Save results to file\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Ollama + RAGAS Offline) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "    f.write(f\"Faithfulness: {faithfulness_score:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy: {answer_relevancy_score:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness: {faithfulness_score:.4f} | Answer Relevancy: {answer_relevancy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c47d8",
   "metadata": {},
   "source": [
    "## Using Gemini for RAG and RAGAs both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluate RAG pipeline using RAGAS\n",
    "# Faithfulness & Answer Relevancy\n",
    "# Gemini for RAG generation | Gemini for RAGAS evaluation (using RAGAS's API integration)\n",
    "\n",
    "# ==== INSTALL DEPENDENCIES ====\n",
    "# !pip install ragas datasets google-genai tqdm\n",
    "# Make sure you have your Gemini API key set as an environment variable (e.g., GEMINI_API_KEY)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "# New imports for Gemini/Google\n",
    "from google import genai\n",
    "from ragas.llms import RagasLLM\n",
    "from ragas.embeddings import HuggingfaceEmbeddings # Reusing a local embedding model as it's efficient\n",
    "\n",
    "# ==== CONFIG ====\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "TOP_K = 3\n",
    "\n",
    "# --- LLMs ---\n",
    "# Using specific Gemini models for their respective roles\n",
    "GEMINI_RAG_MODEL = \"gemini-2.5-flash\" # for RAG generation (Fast, modern LLM)\n",
    "GEMINI_RAGAS_MODEL = \"gemini-2.5-pro\"# for RAGAS evaluation (Prefer a more capable model for reasoning tasks)\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # for embeddings (RAGAS requires an embedding model)\n",
    "# =================\n",
    "\n",
    "# Initialize the Gemini client\n",
    "try:\n",
    "    client = genai.Client()\n",
    "except Exception:\n",
    "    print(\"[ERROR] Failed to initialize Gemini Client. Make sure the GEMINI_API_KEY environment variable is set.\")\n",
    "    exit()\n",
    "\n",
    "# 1️⃣ Load test data\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "# 2️⃣ Function to generate RAG answers using Gemini\n",
    "def generate_with_gemini(prompt, model_name=GEMINI_RAG_MODEL):\n",
    "    \"\"\"\n",
    "    Generate a response using the Gemini API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model_name,\n",
    "            contents=prompt,\n",
    "            config={\"system_instruction\": \"You are a helpful assistant. Use only the provided context to answer the question.\"}\n",
    "        )\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Gemini API call failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# 3️⃣ Prepare evaluation records\n",
    "records = []\n",
    "# NOTE: The 'collection' object (ChromaDB) is missing in this script, \n",
    "# assuming it exists in the user's environment before this cell runs.\n",
    "try:\n",
    "    # Attempt a mock query to ensure 'collection' exists, otherwise provide a warning.\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB client) is NOT defined.\")\n",
    "    print(\"This script will fail when trying to retrieve documents (collection.query).\")\n",
    "    print(\"Please ensure your ChromaDB client/collection is initialized before running this cell.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "for item in tqdm(test_data, desc=\"Generating Gemini RAG answers\"):\n",
    "    question = item[\"question\"]\n",
    "    ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "    # --- Retrieve from Chroma ---\n",
    "    # NOTE: Assuming 'collection' is defined in the execution environment\n",
    "    retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "    retrieved_docs = retrieved[\"documents\"][0]\n",
    "    retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # --- Build RAG prompt ---\n",
    "    prompt = (\n",
    "        f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # --- Generate answer using Gemini ---\n",
    "    generated_answer = generate_with_gemini(prompt)\n",
    "    if not generated_answer:\n",
    "        # Fallback in case of API failure\n",
    "        generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "    # --- Add record for RAGAS evaluation ---\n",
    "    records.append({\n",
    "        \"question\": question,\n",
    "        \"contexts\": retrieved_docs,\n",
    "        \"answer\": generated_answer,\n",
    "        \"ground_truth\": [ideal_answer],\n",
    "    })\n",
    "\n",
    "# 4️⃣ Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(records)\n",
    "\n",
    "# 5️⃣ Initialize RAGAS components with Gemini & embedding models\n",
    "# RagasLLM is used to interface with the google-genai SDK for RAGAS evaluation\n",
    "llm = RagasLLM(model=GEMINI_RAGAS_MODEL)\n",
    "embeddings = HuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# 6️⃣ Evaluate using RAGAS\n",
    "print(f\"\\n[INFO] Evaluating with RAGAS (Faithfulness & Answer Relevancy) using {GEMINI_RAGAS_MODEL} ...\")\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "# 7️⃣ Extract scores\n",
    "faithfulness_score = results[\"faithfulness\"]\n",
    "answer_relevancy_score = results[\"answer_relevancy\"]\n",
    "\n",
    "# 8️⃣ Save results to file\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Gemini + RAGAS with Gemini) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GEMINI_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GEMINI_RAGAS_MODEL}\\n\\n\")\n",
    "    f.write(f\"Faithfulness: {faithfulness_score:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy: {answer_relevancy_score:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness: {faithfulness_score:.4f} | Answer Relevancy: {answer_relevancy_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raga2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
