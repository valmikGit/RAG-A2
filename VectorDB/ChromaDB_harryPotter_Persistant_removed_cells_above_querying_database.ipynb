{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd26bf01",
   "metadata": {},
   "source": [
    "### Imports and Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d607ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/harry_potter_chunks_hierarchical.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"harry_potter_collection\"\n",
    "# # Set API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b927c",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# No need for fitz or RecursiveCharacterTextSplitter here, as we are loading from a file.\n",
    "\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f: # 'rb' mode for reading in binary\n",
    "        loaded_docs = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "\n",
    "# Now you can inspect the loaded documents to verify.\n",
    "print(\"\\nHere is the metadata of a loaded chunk:\")\n",
    "if loaded_docs:\n",
    "    print(loaded_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cbb00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use default SentenceTransformer for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Load model directly\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define embedding function compatible with Chroma\n",
    "def embedding_function(texts):\n",
    "    return model.encode(texts, show_progress_bar=False).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f44b",
   "metadata": {},
   "source": [
    "### Creating new Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get or create the collection\n",
    "client.delete_collection(name=collection_name)  \n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\n",
    "        \"description\": \"Harry Potter book chunks\",\n",
    "        \"created\": str(datetime.now())\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' created or accessed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b449ba",
   "metadata": {},
   "source": [
    "### Add data to collection\n",
    "The chunks have to be given an id and added to the collection now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa83664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Prepare documents for ChromaDB\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "# Process each loaded document chunk\n",
    "for i, doc in enumerate(loaded_docs):\n",
    "    # Generate a unique ID (you could use a more deterministic approach if needed)\n",
    "    doc_id = f\"hp_chunk_{i}\"\n",
    "    \n",
    "    # Get the document text\n",
    "    document_text = doc.page_content\n",
    "    \n",
    "    # Get the document metadata\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Add to our lists\n",
    "    ids.append(doc_id)\n",
    "    documents.append(document_text)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "# Add documents in batches to avoid memory issues\n",
    "batch_size = 500\n",
    "total_added = 0\n",
    "\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    end_idx = min(i + batch_size, len(ids))\n",
    "    \n",
    "    # collection.update(\n",
    "    #     ids=ids[i:end_idx],\n",
    "    #     documents=documents[i:end_idx],\n",
    "    #     metadatas=metadatas[i:end_idx]\n",
    "    # )\n",
    "    collection.add(\n",
    "        ids=ids[i:end_idx],\n",
    "        documents=documents[i:end_idx],\n",
    "        metadatas=metadatas[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    total_added += end_idx - i\n",
    "    print(f\"Added batch: {i} to {end_idx-1} ({end_idx-i} items)\")\n",
    "\n",
    "print(f\"Successfully added {total_added} documents to collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check collection count\n",
    "count = collection.count()\n",
    "print(f\"Total documents in collection: {count}\")\n",
    "\n",
    "# Peek at the first few entries\n",
    "peek = collection.peek(limit=3)\n",
    "print(\"\\nSample entries:\")\n",
    "for i, (doc_id, doc_text, metadata) in enumerate(zip(\n",
    "    peek['ids'], peek['documents'], peek['metadatas']\n",
    ")):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Text: {doc_text[:100]}...\")\n",
    "    print(f\"Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ce07b",
   "metadata": {},
   "source": [
    "## Quantitative Analysis using RAGAs: Faithfulness and Answer Relevency: using OLLaMA for RAG and HuggingFace Zephyr for RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c56031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8: Evaluate RAG pipeline using RAGAS\n",
    "# # Faithfulness & Answer Relevancy\n",
    "# # Ollama for RAG generation | Zephyr for offline RAGAS evaluation\n",
    "\n",
    "# # ==== INSTALL DEPENDENCIES ====\n",
    "# # !pip install ragas datasets transformers accelerate sentence-transformers tqdm\n",
    "# # Make sure you have Ollama installed: https://ollama.ai/download\n",
    "# # Example to pull a model: `ollama pull llama3` or `ollama pull mistral`\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import subprocess\n",
    "# from tqdm import tqdm\n",
    "# from datetime import datetime\n",
    "# from datasets import Dataset\n",
    "\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics import faithfulness, answer_relevancy\n",
    "# from ragas.llms import HuggingfaceLLM\n",
    "# from ragas.embeddings import HuggingfaceEmbeddings\n",
    "\n",
    "# # ==== CONFIG ====\n",
    "# testbed_path = \"../RAG Results/test_bed.json\"\n",
    "# output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "# TOP_K = 3\n",
    "\n",
    "# # --- LLMs ---\n",
    "# OLLAMA_MODEL = \"llama3\"                     # for RAG generation (local via Ollama)\n",
    "# LLM_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"  # for RAGAS evaluation (offline)\n",
    "# EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # for embeddings\n",
    "# # =================\n",
    "\n",
    "# # 1️⃣ Load test data\n",
    "# with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "# print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "# # 2️⃣ Function to generate RAG answers using Ollama\n",
    "# def generate_with_ollama(prompt, model_name=OLLAMA_MODEL):\n",
    "#     \"\"\"\n",
    "#     Generate a response using a local Ollama model.\n",
    "#     Assumes Ollama is installed and the model is already pulled.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Run the Ollama CLI\n",
    "#         result = subprocess.run(\n",
    "#             [\"ollama\", \"run\", model_name],\n",
    "#             input=prompt.encode(\"utf-8\"),\n",
    "#             capture_output=True,\n",
    "#             text=True,\n",
    "#             timeout=120\n",
    "#         )\n",
    "#         if result.returncode == 0:\n",
    "#             return result.stdout.strip()\n",
    "#         else:\n",
    "#             print(f\"[WARN] Ollama returned error: {result.stderr}\")\n",
    "#             return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Ollama call failed: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # 3️⃣ Prepare evaluation records\n",
    "# records = []\n",
    "# for item in tqdm(test_data, desc=\"Generating Ollama RAG answers\"):\n",
    "#     question = item[\"question\"]\n",
    "#     ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "#     # --- Retrieve from Chroma ---\n",
    "#     retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "#     retrieved_docs = retrieved[\"documents\"][0]\n",
    "#     retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "#     # --- Build RAG prompt ---\n",
    "#     prompt = (\n",
    "#         f\"You are a helpful assistant. \"\n",
    "#         f\"Use only the information provided in the context below to answer the question.\\n\\n\"\n",
    "#         f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "#         f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "#     )\n",
    "\n",
    "#     # --- Generate answer using Ollama ---\n",
    "#     generated_answer = generate_with_ollama(prompt)\n",
    "#     if not generated_answer:\n",
    "#         generated_answer = f\"[Fallback mock answer] {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "#     # --- Add record for RAGAS evaluation ---\n",
    "#     records.append({\n",
    "#         \"question\": question,\n",
    "#         \"contexts\": retrieved_docs,\n",
    "#         \"answer\": generated_answer,\n",
    "#         \"ground_truth\": [ideal_answer],\n",
    "#     })\n",
    "\n",
    "# # 4️⃣ Convert to Hugging Face Dataset\n",
    "# dataset = Dataset.from_list(records)\n",
    "\n",
    "# # 5️⃣ Initialize Zephyr & embedding models for RAGAS (offline)\n",
    "# llm = HuggingfaceLLM(model=LLM_MODEL)\n",
    "# embeddings = HuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# # 6️⃣ Evaluate using RAGAS\n",
    "# print(f\"\\n[INFO] Evaluating with RAGAS (Faithfulness & Answer Relevancy) using {LLM_MODEL} ...\")\n",
    "# results = evaluate(\n",
    "#     dataset=dataset,\n",
    "#     metrics=[faithfulness, answer_relevancy],\n",
    "#     llm=llm,\n",
    "#     embeddings=embeddings\n",
    "# )\n",
    "\n",
    "# # 7️⃣ Extract scores\n",
    "# faithfulness_score = results[\"faithfulness\"]\n",
    "# answer_relevancy_score = results[\"answer_relevancy\"]\n",
    "\n",
    "# # 8️⃣ Save results to file\n",
    "# with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"=== RAG Evaluation Metrics (Ollama + RAGAS Offline) ===\\n\")\n",
    "#     f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "#     f.write(f\"Faithfulness: {faithfulness_score:.4f}\\n\")\n",
    "#     f.write(f\"Answer Relevancy: {answer_relevancy_score:.4f}\\n\\n\")\n",
    "#     f.write(\"Full Results:\\n\")\n",
    "#     f.write(str(results))\n",
    "\n",
    "# print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "# print(f\"Faithfulness: {faithfulness_score:.4f} | Answer Relevancy: {answer_relevancy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c47d8",
   "metadata": {},
   "source": [
    "## Using Gemini for RAG and RAGAs both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 8: Evaluate RAG pipeline using RAGAS\n",
    "# # Faithfulness & Answer Relevancy\n",
    "# # Gemini for RAG generation | Gemini for RAGAS evaluation (using RAGAS's API integration)\n",
    "\n",
    "# # ==== INSTALL DEPENDENCIES ====\n",
    "# # !pip install ragas datasets google-genai tqdm\n",
    "# # Make sure you have your Gemini API key set as an environment variable (e.g., GEMINI_API_KEY)\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import subprocess\n",
    "# from tqdm import tqdm\n",
    "# from datetime import datetime\n",
    "# from datasets import Dataset\n",
    "\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics import faithfulness, answer_relevancy\n",
    "# # New imports for Gemini/Google\n",
    "# from google import genai\n",
    "# from ragas.llms import RagasLLM\n",
    "# from ragas.embeddings import HuggingfaceEmbeddings # Reusing a local embedding model as it's efficient\n",
    "\n",
    "# # ==== CONFIG ====\n",
    "# testbed_path = \"../RAG Results/test_bed.json\"\n",
    "# output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "# TOP_K = 3\n",
    "\n",
    "# # --- LLMs ---\n",
    "# # Using specific Gemini models for their respective roles\n",
    "# GEMINI_RAG_MODEL = \"gemini-2.5-flash\" # for RAG generation (Fast, modern LLM)\n",
    "# GEMINI_RAGAS_MODEL = \"gemini-2.5-pro\"# for RAGAS evaluation (Prefer a more capable model for reasoning tasks)\n",
    "# EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\" # for embeddings (RAGAS requires an embedding model)\n",
    "# # =================\n",
    "\n",
    "# # Initialize the Gemini client\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"sk-AIzaSyAoBDShyFr3QWxKiSVc59n_MkCjSNrZHKg\"\n",
    "\n",
    "# # Initialize the Gemini client — it will automatically pick up GEMINI_API_KEY\n",
    "# try:\n",
    "#     client = genai.Client()\n",
    "#     print(\"[INFO] Gemini client initialized successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"[ERROR] Failed to initialize Gemini Client: {e}\")\n",
    "#     raise\n",
    "\n",
    "# # 1️⃣ Load test data\n",
    "# with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "# print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "# # 2️⃣ Function to generate RAG answers using Gemini\n",
    "# def generate_with_gemini(prompt, model_name=GEMINI_RAG_MODEL):\n",
    "#     \"\"\"\n",
    "#     Generate a response using the Gemini API.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         response = client.models.generate_content(\n",
    "#             model=model_name,\n",
    "#             contents=prompt,\n",
    "#             config={\"system_instruction\": \"You are a helpful assistant. Use only the provided context to answer the question.\"}\n",
    "#         )\n",
    "#         return response.text.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] Gemini API call failed: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # 3️⃣ Prepare evaluation records\n",
    "# records = []\n",
    "# # NOTE: The 'collection' object (ChromaDB) is missing in this script, \n",
    "# # assuming it exists in the user's environment before this cell runs.\n",
    "# try:\n",
    "#     # Attempt a mock query to ensure 'collection' exists, otherwise provide a warning.\n",
    "#     collection.query(query_texts=[\"test\"], n_results=1)\n",
    "# except NameError:\n",
    "#     print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB client) is NOT defined.\")\n",
    "#     print(\"This script will fail when trying to retrieve documents (collection.query).\")\n",
    "#     print(\"Please ensure your ChromaDB client/collection is initialized before running this cell.\")\n",
    "#     exit()\n",
    "\n",
    "\n",
    "# for item in tqdm(test_data, desc=\"Generating Gemini RAG answers\"):\n",
    "#     question = item[\"question\"]\n",
    "#     ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "#     # --- Retrieve from Chroma ---\n",
    "#     # NOTE: Assuming 'collection' is defined in the execution environment\n",
    "#     retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "#     retrieved_docs = retrieved[\"documents\"][0]\n",
    "#     retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "#     # --- Build RAG prompt ---\n",
    "#     prompt = (\n",
    "#         f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "#         f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "#     )\n",
    "\n",
    "#     # --- Generate answer using Gemini ---\n",
    "#     generated_answer = generate_with_gemini(prompt)\n",
    "#     if not generated_answer:\n",
    "#         # Fallback in case of API failure\n",
    "#         generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "#     # --- Add record for RAGAS evaluation ---\n",
    "#     records.append({\n",
    "#         \"question\": question,\n",
    "#         \"contexts\": retrieved_docs,\n",
    "#         \"answer\": generated_answer,\n",
    "#         \"ground_truth\": [ideal_answer],\n",
    "#     })\n",
    "\n",
    "# # 4️⃣ Convert to Hugging Face Dataset\n",
    "# dataset = Dataset.from_list(records)\n",
    "\n",
    "# # 5️⃣ Initialize RAGAS components with Gemini & embedding models\n",
    "# # RagasLLM is used to interface with the google-genai SDK for RAGAS evaluation\n",
    "# llm = RagasLLM(model=GEMINI_RAGAS_MODEL)\n",
    "# embeddings = HuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# # 6️⃣ Evaluate using RAGAS\n",
    "# print(f\"\\n[INFO] Evaluating with RAGAS (Faithfulness & Answer Relevancy) using {GEMINI_RAGAS_MODEL} ...\")\n",
    "# results = evaluate(\n",
    "#     dataset=dataset,\n",
    "#     metrics=[faithfulness, answer_relevancy],\n",
    "#     llm=llm,\n",
    "#     embeddings=embeddings\n",
    "# )\n",
    "\n",
    "# # 7️⃣ Extract scores\n",
    "# faithfulness_score = results[\"faithfulness\"]\n",
    "# answer_relevancy_score = results[\"answer_relevancy\"]\n",
    "\n",
    "# # 8️⃣ Save results to file\n",
    "# with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"=== RAG Evaluation Metrics (Gemini + RAGAS with Gemini) ===\\n\")\n",
    "#     f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "#     f.write(f\"RAG Generation Model: {GEMINI_RAG_MODEL}\\n\")\n",
    "#     f.write(f\"RAGAS Evaluation Model: {GEMINI_RAGAS_MODEL}\\n\\n\")\n",
    "#     f.write(f\"Faithfulness: {faithfulness_score:.4f}\\n\")\n",
    "#     f.write(f\"Answer Relevancy: {answer_relevancy_score:.4f}\\n\\n\")\n",
    "#     f.write(\"Full Results:\\n\")\n",
    "#     f.write(str(results))\n",
    "\n",
    "# print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "# print(f\"Faithfulness: {faithfulness_score:.4f} | Answer Relevancy: {answer_relevancy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f3242",
   "metadata": {},
   "source": [
    "## Joshi Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21448fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Gemini + RAG + RAGAS Evaluation (no LangChain) ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets google-generativeai tqdm sentence-transformers\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import google.generativeai as genai\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings import HuggingfaceEmbeddings\n",
    "\n",
    "# ==== CONFIG ====\n",
    "genai.configure(api_key=\"sk-AIzaSyAoBDShyFr3QWxKiSVc59n_MkCjSNrZHKg\")  # ✅ <-- put your key here\n",
    "\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "TOP_K = 3\n",
    "\n",
    "GEMINI_RAG_MODEL = \"gemini-1.5-flash\"\n",
    "GEMINI_RAGAS_MODEL = \"gemini-1.5-pro\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_200 = f.read(200)\n",
    "print(\"First few characters:\\n\", first_200)\n",
    "\n",
    "# ==== 1️⃣ Load test data ====\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "# ==== 2️⃣ Gemini generation ====\n",
    "def generate_with_gemini(prompt, model_name=GEMINI_RAG_MODEL):\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Gemini API call failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==== 3️⃣ Create dummy RagasLLM wrapper ====\n",
    "class GeminiRagasLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def generate(self, prompts):\n",
    "        model = genai.GenerativeModel(self.model_name)\n",
    "        outputs = []\n",
    "        for p in prompts:\n",
    "            try:\n",
    "                r = model.generate_content(p)\n",
    "                outputs.append(r.text.strip())\n",
    "            except Exception as e:\n",
    "                outputs.append(f\"[Error: {e}]\")\n",
    "        return outputs\n",
    "\n",
    "# ==== 4️⃣ Check collection availability ====\n",
    "try:\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB) is NOT defined.\")\n",
    "    print(\"Please initialize your ChromaDB client/collection before running this cell.\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ==== 5️⃣ Generate records ====\n",
    "records = []\n",
    "for item in tqdm(test_data, desc=\"Generating Gemini RAG answers\"):\n",
    "    question = item[\"question\"]\n",
    "    ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "    retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "    retrieved_docs = retrieved[\"documents\"][0]\n",
    "    retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    generated_answer = generate_with_gemini(prompt)\n",
    "    if not generated_answer:\n",
    "        generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "    records.append({\n",
    "        \"question\": question,\n",
    "        \"contexts\": retrieved_docs,\n",
    "        \"answer\": generated_answer,\n",
    "        \"ground_truth\": [ideal_answer],\n",
    "    })\n",
    "\n",
    "# ==== 6️⃣ Convert to HF Dataset ====\n",
    "dataset = Dataset.from_list(records)\n",
    "\n",
    "# ==== 7️⃣ Evaluate with RAGAS ====\n",
    "llm = GeminiRagasLLM(GEMINI_RAGAS_MODEL)\n",
    "embeddings = HuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "print(f\"\\n[INFO] Evaluating with RAGAS using {GEMINI_RAGAS_MODEL} ...\")\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "# ==== 8️⃣ Save Results ====\n",
    "faithfulness_score = results[\"faithfulness\"]\n",
    "answer_relevancy_score = results[\"answer_relevancy\"]\n",
    "\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Gemini + RAGAS) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GEMINI_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GEMINI_RAGAS_MODEL}\\n\\n\")\n",
    "    f.write(f\"Faithfulness: {faithfulness_score:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy: {answer_relevancy_score:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness: {faithfulness_score:.4f} | Answer Relevancy: {answer_relevancy_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2201b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google import generativeai as genai\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "# print(llm.invoke(\"Say hello in one line\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
