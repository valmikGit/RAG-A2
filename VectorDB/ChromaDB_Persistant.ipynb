{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd26bf01",
   "metadata": {},
   "source": [
    "### Imports and Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0d607ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "import pickle\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "multiquery_rag_output_path = \"../RAG Results/multiquery_rag_results.txt\"\n",
    "Relative_Database_path = \"./chroma_Data_v_final\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "file_path = \"../Chunking/Chunk_files/julius-caesar_chunks_semantic.pkl\"\n",
    "# Create a new collection with a unique name\n",
    "collection_name = \"anlp_rag_collection\"\n",
    "# # Set API key\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b927c",
   "metadata": {},
   "source": [
    "### Chroma Setup and Chunk Loading\n",
    "Sets up persistant client and loads previously computed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b9b8be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: C:\\Users\\micro\\Desktop\\Abhinav college\\Resources\\Sem 7\\Advanced NLP\\Assignment 2\\RAG-A2\\VectorDB\\chroma_Data_v_final\n",
      "Existing collections: []\n"
     ]
    }
   ],
   "source": [
    "# Initialize the persistent client\n",
    "client = chromadb.PersistentClient(path=Absolute_Database_path)\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# List existing collections\n",
    "existing_collections = client.list_collections()\n",
    "print(f\"Existing collections: {[c.name for c in existing_collections]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "788e6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 126 chunks from '../Chunking/Chunk_files/julius-caesar_chunks_semantic.pkl'.\n",
      "\n",
      "Verification:\n",
      "‚úì Total chunks loaded: 126\n",
      "‚úì First chunk preview: Michael Witmore\n",
      "Director, Folger Shakespeare Library\n",
      "It is hard to imagine a world without Shakespea...\n",
      "‚úì Metadata: {'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'page_number': 3, 'c': 'semantic', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "# Load chunks with version compatibility handling\n",
    "\n",
    "loaded_docs = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Try loading normally first\n",
    "        try:\n",
    "            loaded_docs = pickle.load(f)\n",
    "            print(f\"Successfully loaded {len(loaded_docs)} chunks from '{file_path}'.\")\n",
    "        except AttributeError as e:\n",
    "            # Handle Pydantic v1 vs v2 incompatibility\n",
    "            if \"__fields_set__\" in str(e):\n",
    "                print(\"[WARN] Pickle file has version incompatibility. Attempting to reconstruct...\")\n",
    "                \n",
    "                # Reopen and load with custom unpickler\n",
    "                f.seek(0)\n",
    "                import pickle\n",
    "                \n",
    "                # Create a custom unpickler to handle missing attributes\n",
    "                class CompatibilityUnpickler(pickle.Unpickler):\n",
    "                    def find_class(self, module, name):\n",
    "                        # Map old LangChain classes to current ones\n",
    "                        if 'langchain' in module and 'schema' in module:\n",
    "                            # Import current Document class\n",
    "                            from langchain.schema import Document\n",
    "                            if name == 'Document':\n",
    "                                return Document\n",
    "                        return super().find_class(module, name)\n",
    "                \n",
    "                loaded_docs = CompatibilityUnpickler(f).load()\n",
    "                \n",
    "                # Reconstruct documents if needed\n",
    "                from langchain.schema import Document\n",
    "                reconstructed_docs = []\n",
    "                for doc in loaded_docs:\n",
    "                    if hasattr(doc, 'page_content') and hasattr(doc, 'metadata'):\n",
    "                        # Create fresh Document object\n",
    "                        new_doc = Document(\n",
    "                            page_content=doc.page_content,\n",
    "                            metadata=doc.metadata if isinstance(doc.metadata, dict) else {}\n",
    "                        )\n",
    "                        reconstructed_docs.append(new_doc)\n",
    "                    else:\n",
    "                        reconstructed_docs.append(doc)\n",
    "                \n",
    "                loaded_docs = reconstructed_docs\n",
    "                print(f\"[SUCCESS] Reconstructed {len(loaded_docs)} chunks successfully.\")\n",
    "            else:\n",
    "                raise\n",
    "                \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Last resort: regenerate the chunks\n",
    "    print(\"\\n[TIP] If error persists, regenerate the chunks using First_chunking_attempt.ipynb\")\n",
    "\n",
    "# Verify loaded documents\n",
    "print(\"\\nVerification:\")\n",
    "if loaded_docs:\n",
    "    print(f\"‚úì Total chunks loaded: {len(loaded_docs)}\")\n",
    "    print(f\"‚úì First chunk preview: {loaded_docs[0].page_content[:100]}...\")\n",
    "    print(f\"‚úì Metadata: {loaded_docs[0].metadata}\")\n",
    "else:\n",
    "    print(\"‚úó No documents loaded. Please regenerate chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cbb00",
   "metadata": {},
   "source": [
    "### Set up Embedding Function\n",
    "Will use default SentenceTransformer for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "928a1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function initialized with model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Install if needed\n",
    "# !pip install sentence_transformers\n",
    "\n",
    "# Set up embedding function\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding function initialized with model: all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f44b",
   "metadata": {},
   "source": [
    "### Creating new Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b34eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No existing collection named 'anlp_rag_collection' to delete.\n",
      "[SUCCESS] Fresh collection 'anlp_rag_collection' created successfully\n",
      "Current count in collection: 0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# FORCE DELETE the collection if it exists\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(f\"[INFO] Deleted existing collection '{collection_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] No existing collection named '{collection_name}' to delete.\")\n",
    "\n",
    "# Create a FRESH collection\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\n",
    "        \"description\": \"Julius Caesar Chunks collection for RAG\",\n",
    "        \"created\": str(datetime.now())\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"[SUCCESS] Fresh collection '{collection_name}' created successfully\")\n",
    "print(f\"Current count in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4bad9",
   "metadata": {},
   "source": [
    "### Text Cleaning Function\n",
    "Remove Folger edition artifacts: page headers, footers, line numbers (FTLN), act/scene markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa06f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED CLEANING TEST ===\n",
      "\n",
      "======================================================================\n",
      "TEST CASE 1\n",
      "======================================================================\n",
      "\n",
      "üìÑ ORIGINAL (90 chars):\n",
      "'79 Julius Caesar ACT 2. SC. 2 She kneels. CAESAR He lifts her up. DECIUS CAESAR CALPHURNIA'\n",
      "\n",
      "‚ú® CLEANED (60 chars):\n",
      "'She kneels. CAESAR He lifts her up. DECIUS CAESAR CALPHURNIA'\n",
      "\n",
      "üìä METADATA:\n",
      "   Act: 2\n",
      "   Scene: 2\n",
      "\n",
      "üìà STATS:\n",
      "   Retention: 66.7%\n",
      "   Removed: 30 chars\n",
      "\n",
      "======================================================================\n",
      "TEST CASE 2\n",
      "======================================================================\n",
      "\n",
      "üìÑ ORIGINAL (82 chars):\n",
      "\"115 Julius Caesar ACT 3. SC. 2 ANTONY SERVANT ANTONY They exit with Caesar's body.\"\n",
      "\n",
      "‚ú® CLEANED (51 chars):\n",
      "\"ANTONY SERVANT ANTONY They exit with Caesar's body.\"\n",
      "\n",
      "üìä METADATA:\n",
      "   Act: 3\n",
      "   Scene: 2\n",
      "\n",
      "üìà STATS:\n",
      "   Retention: 62.2%\n",
      "   Removed: 31 chars\n",
      "\n",
      "======================================================================\n",
      "TEST CASE 3\n",
      "======================================================================\n",
      "\n",
      "üìÑ ORIGINAL (261 chars):\n",
      "'\\nFTLN 0001 ACT 1. SC. 2\\nThe Tragedy of Julius Caesar\\n___________________________________________\\n\\nBRUTUS\\nFTLN 0123 What means this shouting? I do fear'\n",
      "\n",
      "‚ú® CLEANED (136 chars):\n",
      "'BRUTUS\\nWhat means this shouting? I do fear the people\\nChoose Caesar for their king.\\nCASSIUS\\nThe fault, dear Brutus, is not in our stars.'\n",
      "\n",
      "üìä METADATA:\n",
      "   Act: 1\n",
      "   Scene: 2\n",
      "\n",
      "üìà STATS:\n",
      "   Retention: 52.1%\n",
      "   Removed: 125 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_act_scene_metadata(text):\n",
    "    \"\"\"\n",
    "    Extract Act and Scene information from text.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'act': str or None, 'scene': str or None}\n",
    "    \"\"\"\n",
    "    metadata = {'act': None, 'scene': None}\n",
    "    \n",
    "    if not text or not isinstance(text, str):\n",
    "        return metadata\n",
    "    \n",
    "    # Pattern 1: \"ACT 1. SC. 2\" or \"ACT 1 SC 2\"\n",
    "    match = re.search(r'\\b(?:ACT|Act)\\s+([IVXLCDM0-9]+)[\\.,]?\\s+(?:SC|SCENE|Scene)[\\.,]?\\s+([IVXLCDM0-9]+)\\b', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        metadata['act'] = match.group(1)\n",
    "        metadata['scene'] = match.group(2)\n",
    "        return metadata\n",
    "    \n",
    "    # Pattern 2: \"ACT 1\" and \"SCENE 2\" separately\n",
    "    act_match = re.search(r'\\b(?:ACT|Act)\\s+([IVXLCDM0-9]+)\\b', text, re.IGNORECASE)\n",
    "    scene_match = re.search(r'\\b(?:SCENE|Scene)\\s+([IVXLCDM0-9]+)\\b', text, re.IGNORECASE)\n",
    "    \n",
    "    if act_match:\n",
    "        metadata['act'] = act_match.group(1)\n",
    "    if scene_match:\n",
    "        metadata['scene'] = scene_match.group(1)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def clean_folger_artifacts(text):\n",
    "    \"\"\"\n",
    "    Remove Folger Shakespeare Library edition artifacts from text.\n",
    "    \n",
    "    Removes:\n",
    "    - Leading page numbers (numbers at start of line)\n",
    "    - FTLN (Folger Through Line Numbers) like \"FTLN 0001\", \"FTLN 1234\"\n",
    "    - Act/Scene markers like \"ACT 1. SC. 2\", \"Act 1, Scene 2\"\n",
    "    - \"Julius Caesar\" title text\n",
    "    - Page headers/footers\n",
    "    - Stage directions in excessive brackets\n",
    "    - Multiple consecutive spaces/newlines\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string from document chunk\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (cleaned_text, metadata_dict)\n",
    "            - cleaned_text: Text with artifacts removed\n",
    "            - metadata_dict: {'act': str or None, 'scene': str or None}\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return text, {'act': None, 'scene': None}\n",
    "    \n",
    "    # Extract metadata BEFORE cleaning\n",
    "    metadata = extract_act_scene_metadata(text)\n",
    "    \n",
    "    # Store original for comparison\n",
    "    original_text = text\n",
    "    \n",
    "    # 1. Remove leading page numbers FIRST (numbers at start, before any text)\n",
    "    # This catches patterns like \"79 Julius Caesar ACT 2. SC. 2\"\n",
    "    text = re.sub(r'^\\s*\\d+\\s+', '', text)  # Remove leading number at start\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s+', '\\n', text)  # Remove numbers after newlines\n",
    "    \n",
    "    # 2. Remove \"Julius Caesar\" title (standalone or with \"The Tragedy of\")\n",
    "    # Order matters: remove longer pattern first\n",
    "    text = re.sub(r'\\bThe\\s+Tragedy\\s+of\\s+Julius\\s+Caesar\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bThe\\s+Tragedy\\s+of\\b', '', text, flags=re.IGNORECASE)  # Catch any leftover\n",
    "    text = re.sub(r'\\bJulius\\s+Caesar\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 3. Remove FTLN line numbers (various formats)\n",
    "    text = re.sub(r'\\bFTLN\\s*\\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 4. Remove ACT/SCENE markers (various formats)\n",
    "    # Pattern: \"ACT 1. SC. 2\", \"ACT 1 SC 2\", \"Act 1, Scene 2\", etc.\n",
    "    text = re.sub(r'\\b(?:ACT|Act)\\s+[IVXLCDM0-9]+[\\.,]?\\s+(?:SC|SCENE|Scene)[\\.,]?\\s+[IVXLCDM0-9]+\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(?:ACT|Act)\\s+[IVXLCDM0-9]+\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(?:SCENE|Scene)\\s+[IVXLCDM0-9]+\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 5. Remove standalone page numbers (numbers on their own line)\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 6. Remove header/footer separators (lines of dashes, equals, underscores)\n",
    "    text = re.sub(r'^[\\s\\-_=]{3,}$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 7. Clean up excessive whitespace\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    \n",
    "    # Replace multiple newlines with at most 2 newlines (preserve paragraph breaks)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace from each line\n",
    "    text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "    \n",
    "    # Remove empty lines that only contain whitespace\n",
    "    lines = [line for line in text.split('\\n') if line.strip()]\n",
    "    text = '\\n'.join(lines)\n",
    "    \n",
    "    # Final cleanup: ensure no leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text, metadata\n",
    "\n",
    "\n",
    "def validate_cleaning(original, cleaned):\n",
    "    \"\"\"\n",
    "    Validate that cleaning didn't remove too much content.\n",
    "    \n",
    "    Args:\n",
    "        original: Original text\n",
    "        cleaned: Cleaned text\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if cleaning is acceptable, False if too aggressive\n",
    "    \"\"\"\n",
    "    if not original:\n",
    "        return True\n",
    "    \n",
    "    # Calculate retention ratio\n",
    "    retention_ratio = len(cleaned) / len(original) if len(original) > 0 else 0\n",
    "    \n",
    "    # If we removed more than 70% of content, something might be wrong\n",
    "    # (increased from 50% since we're removing more artifacts)\n",
    "    if retention_ratio < 0.3:\n",
    "        return False\n",
    "    \n",
    "    # Check that we still have substantial text (at least 30 characters)\n",
    "    if len(cleaned) < 30 and len(original) > 100:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Test the cleaning function with your problematic examples\n",
    "test_cases = [\n",
    "    \"79 Julius Caesar ACT 2. SC. 2 She kneels. CAESAR He lifts her up. DECIUS CAESAR CALPHURNIA\",\n",
    "    \"115 Julius Caesar ACT 3. SC. 2 ANTONY SERVANT ANTONY They exit with Caesar's body.\",\n",
    "    \"\"\"\n",
    "FTLN 0001 ACT 1. SC. 2\n",
    "The Tragedy of Julius Caesar\n",
    "___________________________________________\n",
    "\n",
    "BRUTUS\n",
    "FTLN 0123 What means this shouting? I do fear the people\n",
    "FTLN 0124 Choose Caesar for their king.\n",
    "\n",
    "123\n",
    "\n",
    "CASSIUS\n",
    "The fault, dear Brutus, is not in our stars.\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "print(\"=== ENHANCED CLEANING TEST ===\\n\")\n",
    "for idx, test_text in enumerate(test_cases, 1):\n",
    "    cleaned_test, metadata = clean_folger_artifacts(test_text)\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TEST CASE {idx}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nüìÑ ORIGINAL ({len(test_text)} chars):\")\n",
    "    print(repr(test_text[:150]))\n",
    "    print(f\"\\n‚ú® CLEANED ({len(cleaned_test)} chars):\")\n",
    "    print(repr(cleaned_test[:150]))\n",
    "    print(f\"\\nüìä METADATA:\")\n",
    "    print(f\"   Act: {metadata['act']}\")\n",
    "    print(f\"   Scene: {metadata['scene']}\")\n",
    "    print(f\"\\nüìà STATS:\")\n",
    "    print(f\"   Retention: {len(cleaned_test)/len(test_text):.1%}\")\n",
    "    print(f\"   Removed: {len(test_text) - len(cleaned_test)} chars\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d791e",
   "metadata": {},
   "source": [
    "### Add data to collection\n",
    "The chunks have to be given an id and added to the collection now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eaa83664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating IDs with prefix: julius-caesar\n",
      "[INFO] Applying Folger artifact cleaning + metadata extraction to all chunks...\n",
      "\n",
      "[INFO] Cleaning Statistics:\n",
      "  Total chunks processed: 126\n",
      "  Successfully cleaned: 126\n",
      "  Skipped (validation failed): 0\n",
      "  Chunks with Act/Scene metadata: 82\n",
      "  Average text reduction: 5.45%\n",
      "\n",
      "[INFO] Prepared 126 documents with IDs like: julius-caesar_chunk_0, julius-caesar_chunk_1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Added batch: 0 to 125 (126 documents)\n",
      "\n",
      "[SUCCESS] Added 126 cleaned documents to collection 'anlp_rag_collection'\n",
      "[INFO] All chunks have IDs in format: julius-caesar_chunk_<number>\n",
      "[INFO] Act/Scene metadata stored separately from chunk text\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "# Extract document name from file path for ID generation\n",
    "# This will give us \"julius-caesar\" from the file path\n",
    "doc_name = file_path.split('/')[-1].split('_chunks')[0]\n",
    "\n",
    "# Prepare documents for ChromaDB\n",
    "ids = []\n",
    "documents = []\n",
    "metadatas = []\n",
    "\n",
    "print(f\"[INFO] Generating IDs with prefix: {doc_name}\")\n",
    "print(f\"[INFO] Applying Folger artifact cleaning + metadata extraction to all chunks...\")\n",
    "\n",
    "# Cleaning statistics\n",
    "cleaning_stats = {\n",
    "    'total': 0,\n",
    "    'cleaned': 0,\n",
    "    'skipped': 0,\n",
    "    'with_act_scene': 0,\n",
    "    'avg_reduction': []\n",
    "}\n",
    "\n",
    "# Process each loaded document chunk\n",
    "for i, doc in enumerate(loaded_docs):\n",
    "    # Generate a deterministic ID based on document name and index\n",
    "    # This ensures all chunks from the same document have consistent IDs\n",
    "    doc_id = f\"{doc_name}_chunk_{i}\"\n",
    "    \n",
    "    # Get the original document text\n",
    "    original_text = doc.page_content\n",
    "    \n",
    "    # Apply cleaning function (now returns cleaned text AND metadata)\n",
    "    cleaned_text, extracted_metadata = clean_folger_artifacts(original_text)\n",
    "    \n",
    "    # Validate cleaning\n",
    "    if validate_cleaning(original_text, cleaned_text):\n",
    "        document_text = cleaned_text\n",
    "        cleaning_stats['cleaned'] += 1\n",
    "        \n",
    "        # Calculate reduction\n",
    "        if len(original_text) > 0:\n",
    "            reduction = (len(original_text) - len(cleaned_text)) / len(original_text)\n",
    "            cleaning_stats['avg_reduction'].append(reduction)\n",
    "    else:\n",
    "        # If cleaning was too aggressive, use original but still extract metadata\n",
    "        document_text = original_text\n",
    "        cleaning_stats['skipped'] += 1\n",
    "        print(f\"[WARN] Chunk {i} cleaning too aggressive, using original text\")\n",
    "    \n",
    "    # Get the existing document metadata\n",
    "    metadata = doc.metadata.copy() if hasattr(doc, 'metadata') and doc.metadata else {}\n",
    "    \n",
    "    # Add Act/Scene metadata if extracted\n",
    "    if extracted_metadata['act']:\n",
    "        metadata['act'] = extracted_metadata['act']\n",
    "        cleaning_stats['with_act_scene'] += 1\n",
    "    if extracted_metadata['scene']:\n",
    "        metadata['scene'] = extracted_metadata['scene']\n",
    "    \n",
    "    # Add cleaning metadata\n",
    "    metadata['cleaned'] = (document_text != original_text)\n",
    "    metadata['original_length'] = len(original_text)\n",
    "    metadata['cleaned_length'] = len(document_text)\n",
    "    \n",
    "    # Add to our lists\n",
    "    ids.append(doc_id)\n",
    "    documents.append(document_text)\n",
    "    metadatas.append(metadata)\n",
    "    \n",
    "    cleaning_stats['total'] += 1\n",
    "\n",
    "# Calculate and display cleaning statistics\n",
    "avg_reduction = sum(cleaning_stats['avg_reduction']) / len(cleaning_stats['avg_reduction']) if cleaning_stats['avg_reduction'] else 0\n",
    "\n",
    "print(f\"\\n[INFO] Cleaning Statistics:\")\n",
    "print(f\"  Total chunks processed: {cleaning_stats['total']}\")\n",
    "print(f\"  Successfully cleaned: {cleaning_stats['cleaned']}\")\n",
    "print(f\"  Skipped (validation failed): {cleaning_stats['skipped']}\")\n",
    "print(f\"  Chunks with Act/Scene metadata: {cleaning_stats['with_act_scene']}\")\n",
    "print(f\"  Average text reduction: {avg_reduction:.2%}\")\n",
    "\n",
    "print(f\"\\n[INFO] Prepared {len(ids)} documents with IDs like: {ids[0]}, {ids[1] if len(ids) > 1 else 'N/A'}...\")\n",
    "\n",
    "# Add documents in batches to avoid memory issues\n",
    "batch_size = 500\n",
    "total_added = 0\n",
    "\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    end_idx = min(i + batch_size, len(ids))\n",
    "    \n",
    "    # Simply add all documents (collection is fresh, no need to update)\n",
    "    collection.add(\n",
    "        ids=ids[i:end_idx],\n",
    "        documents=documents[i:end_idx],\n",
    "        metadatas=metadatas[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    total_added += end_idx - i\n",
    "    print(f\"[INFO] Added batch: {i} to {end_idx-1} ({end_idx-i} documents)\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Added {total_added} cleaned documents to collection '{collection_name}'\")\n",
    "print(f\"[INFO] All chunks have IDs in format: {doc_name}_chunk_<number>\")\n",
    "print(f\"[INFO] Act/Scene metadata stored separately from chunk text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1fd6408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in collection: 126\n",
      "\n",
      "Sample entries:\n",
      "\n",
      "--- Document 1 ---\n",
      "ID: julius-caesar_chunk_0\n",
      "Text: Michael Witmore\n",
      "Director, Folger Shakespeare Library\n",
      "It is hard to imagine a world without Shakespea...\n",
      "Metadata: {'original_length': 2121, 'page_number': 3, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'c': 'semantic', 'cleaned_length': 2121, 'ischunk': True, 'cleaned': False}\n",
      "\n",
      "--- Document 2 ---\n",
      "ID: julius-caesar_chunk_1\n",
      "Text: Until now, with the release of The Folger Shakespeare (formerly\n",
      "Folger Digital Texts), readers in se...\n",
      "Metadata: {'ischunk': True, 'c': 'semantic', 'cleaned': False, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'original_length': 1674, 'page_number': 4, 'cleaned_length': 1674}\n",
      "\n",
      "--- Document 3 ---\n",
      "ID: julius-caesar_chunk_2\n",
      "Text: At\n",
      "any point in the text, you can hover your cursor over a bracket for\n",
      "more information. Because the...\n",
      "Metadata: {'cleaned': True, 'page_number': 5, 'original_length': 462, 'c': 'semantic', 'cleaned_length': 461, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'ischunk': True}\n"
     ]
    }
   ],
   "source": [
    "# Check collection count\n",
    "count = collection.count()\n",
    "print(f\"Total documents in collection: {count}\")\n",
    "\n",
    "# Peek at the first few entries\n",
    "peek = collection.peek(limit=3)\n",
    "print(\"\\nSample entries:\")\n",
    "for i, (doc_id, doc_text, metadata) in enumerate(zip(\n",
    "    peek['ids'], peek['documents'], peek['metadatas']\n",
    ")):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Text: {doc_text[:100]}...\")\n",
    "    print(f\"Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ad36bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SEARCHING FOR CHUNKS WITH ACT/SCENE METADATA ===\n",
      "\n",
      "Found 82 chunks with Act/Scene metadata\n",
      "\n",
      "First 5 examples:\n",
      "\n",
      "1. Chunk ID: julius-caesar_chunk_5\n",
      "   Act: 1, Scene: 1\n",
      "   Text: COBBLER\n",
      "MARULLUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "COBBLER\n",
      "MARULLUS\n",
      "Nay, I beseech you, sir, be not out with me. Yet if you be out, sir, I can mend you....\n",
      "   Metadata: {'cleaned': True, 'cleaned_length': 961, 'scene': '1', 'original_length': 1003, 'c': 'semantic', 'page_number': 10, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'ischunk': True, 'act': '1'}\n",
      "\n",
      "2. Chunk ID: julius-caesar_chunk_6\n",
      "   Act: 1, Scene: 1\n",
      "   Text: FLAVIUS\n",
      "All the Commoners exit. MARULLUS\n",
      "FLAVIUS\n",
      "They exit in different directions. To hear the replication of your sounds\n",
      "Made in her concave shores?...\n",
      "   Metadata: {'cleaned_length': 317, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'cleaned': True, 'ischunk': True, 'act': '1', 'original_length': 349, 'c': 'semantic', 'scene': '1', 'page_number': 11}\n",
      "\n",
      "3. Chunk ID: julius-caesar_chunk_8\n",
      "   Act: 1, Scene: 2\n",
      "   Text: CAESAR\n",
      "CASCA\n",
      "CAESAR\n",
      "CALPHURNIA\n",
      "CAESAR\n",
      "ANTONY\n",
      "CAESAR\n",
      "ANTONY\n",
      "CAESAR\n",
      "Sennet. SOOTHSAYER\n",
      "CAESAR\n",
      "CASCA\n",
      "CAESAR\n",
      "SOOTHSAYER\n",
      "CAESAR\n",
      "BRUTUS\n",
      "Enter Caesar, Antony...\n",
      "   Metadata: {'act': '1', 'original_length': 730, 'cleaned': True, 'ischunk': True, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'cleaned_length': 696, 'scene': '2', 'page_number': 12, 'c': 'semantic'}\n",
      "\n",
      "4. Chunk ID: julius-caesar_chunk_9\n",
      "   Act: 1, Scene: 2\n",
      "   Text: CAESAR\n",
      "CASSIUS\n",
      "The Soothsayer comes forward. CAESAR\n",
      "SOOTHSAYER\n",
      "CAESAR\n",
      "Sennet. All but Brutus and Cassius exit. CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "B...\n",
      "   Metadata: {'scene': '2', 'page_number': 13, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'cleaned': True, 'act': '1', 'cleaned_length': 349, 'ischunk': True, 'original_length': 382, 'c': 'semantic'}\n",
      "\n",
      "5. Chunk ID: julius-caesar_chunk_10\n",
      "   Act: 1, Scene: 2\n",
      "   Text: CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "Nor construe any further my neglect\n",
      "Than that poor Brutus, with himself at war,\n",
      "Forgets the shows of love to oth...\n",
      "   Metadata: {'scene': '2', 'cleaned_length': 1092, 'ischunk': True, 'act': '1', 'cleaned': True, 'c': 'semantic', 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'original_length': 1140, 'page_number': 14}\n",
      "\n",
      "\n",
      "=== VERIFYING ACT/SCENE TEXT REMOVED FROM CONTENT ===\n",
      "\n",
      "‚úÖ SUCCESS: No ACT/SCENE/Title artifacts found in chunk text!\n",
      "   Metadata is properly extracted and stored separately.\n",
      "Found 82 chunks with Act/Scene metadata\n",
      "\n",
      "First 5 examples:\n",
      "\n",
      "1. Chunk ID: julius-caesar_chunk_5\n",
      "   Act: 1, Scene: 1\n",
      "   Text: COBBLER\n",
      "MARULLUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "COBBLER\n",
      "MARULLUS\n",
      "Nay, I beseech you, sir, be not out with me. Yet if you be out, sir, I can mend you....\n",
      "   Metadata: {'cleaned': True, 'cleaned_length': 961, 'scene': '1', 'original_length': 1003, 'c': 'semantic', 'page_number': 10, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'ischunk': True, 'act': '1'}\n",
      "\n",
      "2. Chunk ID: julius-caesar_chunk_6\n",
      "   Act: 1, Scene: 1\n",
      "   Text: FLAVIUS\n",
      "All the Commoners exit. MARULLUS\n",
      "FLAVIUS\n",
      "They exit in different directions. To hear the replication of your sounds\n",
      "Made in her concave shores?...\n",
      "   Metadata: {'cleaned_length': 317, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'cleaned': True, 'ischunk': True, 'act': '1', 'original_length': 349, 'c': 'semantic', 'scene': '1', 'page_number': 11}\n",
      "\n",
      "3. Chunk ID: julius-caesar_chunk_8\n",
      "   Act: 1, Scene: 2\n",
      "   Text: CAESAR\n",
      "CASCA\n",
      "CAESAR\n",
      "CALPHURNIA\n",
      "CAESAR\n",
      "ANTONY\n",
      "CAESAR\n",
      "ANTONY\n",
      "CAESAR\n",
      "Sennet. SOOTHSAYER\n",
      "CAESAR\n",
      "CASCA\n",
      "CAESAR\n",
      "SOOTHSAYER\n",
      "CAESAR\n",
      "BRUTUS\n",
      "Enter Caesar, Antony...\n",
      "   Metadata: {'act': '1', 'original_length': 730, 'cleaned': True, 'ischunk': True, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'cleaned_length': 696, 'scene': '2', 'page_number': 12, 'c': 'semantic'}\n",
      "\n",
      "4. Chunk ID: julius-caesar_chunk_9\n",
      "   Act: 1, Scene: 2\n",
      "   Text: CAESAR\n",
      "CASSIUS\n",
      "The Soothsayer comes forward. CAESAR\n",
      "SOOTHSAYER\n",
      "CAESAR\n",
      "Sennet. All but Brutus and Cassius exit. CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "B...\n",
      "   Metadata: {'scene': '2', 'page_number': 13, 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'cleaned': True, 'act': '1', 'cleaned_length': 349, 'ischunk': True, 'original_length': 382, 'c': 'semantic'}\n",
      "\n",
      "5. Chunk ID: julius-caesar_chunk_10\n",
      "   Act: 1, Scene: 2\n",
      "   Text: CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "Nor construe any further my neglect\n",
      "Than that poor Brutus, with himself at war,\n",
      "Forgets the shows of love to oth...\n",
      "   Metadata: {'scene': '2', 'cleaned_length': 1092, 'ischunk': True, 'act': '1', 'cleaned': True, 'c': 'semantic', 'source': '../julius-caesar_PDF_FolgerShakespeare.pdf', 'original_length': 1140, 'page_number': 14}\n",
      "\n",
      "\n",
      "=== VERIFYING ACT/SCENE TEXT REMOVED FROM CONTENT ===\n",
      "\n",
      "‚úÖ SUCCESS: No ACT/SCENE/Title artifacts found in chunk text!\n",
      "   Metadata is properly extracted and stored separately.\n"
     ]
    }
   ],
   "source": [
    "# Find and display chunks with Act/Scene metadata\n",
    "print(\"=== SEARCHING FOR CHUNKS WITH ACT/SCENE METADATA ===\\n\")\n",
    "\n",
    "# Get all documents\n",
    "all_docs = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "chunks_with_act_scene = []\n",
    "for i, (doc_id, metadata, doc_text) in enumerate(zip(all_docs['ids'], all_docs['metadatas'], all_docs['documents'])):\n",
    "    if metadata.get('act') is not None:\n",
    "        chunks_with_act_scene.append({\n",
    "            'id': doc_id,\n",
    "            'act': metadata.get('act'),\n",
    "            'scene': metadata.get('scene'),\n",
    "            'text': doc_text[:150],\n",
    "            'full_metadata': metadata\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(chunks_with_act_scene)} chunks with Act/Scene metadata\\n\")\n",
    "\n",
    "# Display first 5 examples\n",
    "print(\"First 5 examples:\\n\")\n",
    "for i, chunk in enumerate(chunks_with_act_scene[:5], 1):\n",
    "    print(f\"{i}. Chunk ID: {chunk['id']}\")\n",
    "    print(f\"   Act: {chunk['act']}, Scene: {chunk['scene']}\")\n",
    "    print(f\"   Text: {chunk['text']}...\")\n",
    "    print(f\"   Metadata: {chunk['full_metadata']}\")\n",
    "    print()\n",
    "\n",
    "# Verify no ACT/SCENE text remains in the chunks\n",
    "print(\"\\n=== VERIFYING ACT/SCENE TEXT REMOVED FROM CONTENT ===\\n\")\n",
    "artifacts_remaining = []\n",
    "for chunk in chunks_with_act_scene[:10]:\n",
    "    text = chunk['text']\n",
    "    if re.search(r'\\bACT\\s+[IVXLCDM0-9]+', text, re.IGNORECASE):\n",
    "        artifacts_remaining.append((chunk['id'], 'ACT marker found'))\n",
    "    if re.search(r'\\bSCENE\\s+[IVXLCDM0-9]+', text, re.IGNORECASE):\n",
    "        artifacts_remaining.append((chunk['id'], 'SCENE marker found'))\n",
    "    if re.search(r'\\bJulius\\s+Caesar\\b', text, re.IGNORECASE):\n",
    "        artifacts_remaining.append((chunk['id'], 'Julius Caesar title found'))\n",
    "\n",
    "if artifacts_remaining:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Found {len(artifacts_remaining)} chunks with remaining artifacts:\")\n",
    "    for chunk_id, artifact in artifacts_remaining:\n",
    "        print(f\"  - {chunk_id}: {artifact}\")\n",
    "else:\n",
    "    print(\"‚úÖ SUCCESS: No ACT/SCENE/Title artifacts found in chunk text!\")\n",
    "    print(\"   Metadata is properly extracted and stored separately.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a51687a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QUICK VERIFICATION SUMMARY ===\n",
      "\n",
      "üìä Total chunks: 126\n",
      "üìå Chunks with Act/Scene metadata: 82\n",
      "üßπ Chunks with remaining 'ACT X. SC. Y' artifacts: 0\n",
      "\n",
      "‚úÖ SUCCESS! All ACT/SCENE markers removed from text!\n",
      "   Metadata properly extracted and stored separately.\n",
      "\n",
      "üìÑ Sample chunk with metadata:\n",
      "   ID: julius-caesar_chunk_5\n",
      "   Act: 1, Scene: 1\n",
      "   Text preview: COBBLER\n",
      "MARULLUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "COBBLER\n",
      "MARULLUS\n",
      "Nay, I beseech you, sir, be not out with me. Yet if yo...\n"
     ]
    }
   ],
   "source": [
    "# Quick summary verification\n",
    "print(\"=== QUICK VERIFICATION SUMMARY ===\\n\")\n",
    "\n",
    "# Get all documents\n",
    "all_docs = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "# Count chunks with metadata\n",
    "chunks_with_metadata = sum(1 for m in all_docs['metadatas'] if m.get('act') is not None)\n",
    "\n",
    "# Check for artifacts in ALL chunk texts\n",
    "artifacts_found = 0\n",
    "for doc_text in all_docs['documents']:\n",
    "    if re.search(r'\\bACT\\s+[0-9IVXLCDM]+', doc_text, re.IGNORECASE):\n",
    "        artifacts_found += 1\n",
    "    elif re.search(r'\\bSC\\.\\s+[0-9IVXLCDM]+', doc_text, re.IGNORECASE):\n",
    "        artifacts_found += 1\n",
    "\n",
    "print(f\"üìä Total chunks: {len(all_docs['ids'])}\")\n",
    "print(f\"üìå Chunks with Act/Scene metadata: {chunks_with_metadata}\")\n",
    "print(f\"üßπ Chunks with remaining 'ACT X. SC. Y' artifacts: {artifacts_found}\")\n",
    "\n",
    "if artifacts_found == 0:\n",
    "    print(\"\\n‚úÖ SUCCESS! All ACT/SCENE markers removed from text!\")\n",
    "    print(\"   Metadata properly extracted and stored separately.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {artifacts_found} chunks still contain ACT/SCENE artifacts\")\n",
    "\n",
    "# Show a sample chunk with metadata\n",
    "for i, (doc_id, metadata, doc_text) in enumerate(zip(all_docs['ids'], all_docs['metadatas'], all_docs['documents'])):\n",
    "    if metadata.get('act') is not None:\n",
    "        print(f\"\\nüìÑ Sample chunk with metadata:\")\n",
    "        print(f\"   ID: {doc_id}\")\n",
    "        print(f\"   Act: {metadata.get('act')}, Scene: {metadata.get('scene')}\")\n",
    "        print(f\"   Text preview: {doc_text[:120]}...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75225221",
   "metadata": {},
   "source": [
    "### Verify Cleaning Results\n",
    "Compare original vs cleaned text for sample chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1df43d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING VERIFICATION ===\n",
      "\n",
      "\n",
      "============================================================\n",
      "CHUNK 1 COMPARISON\n",
      "============================================================\n",
      "\n",
      "üìä Metadata:\n",
      "  - Act: N/A\n",
      "  - Scene: N/A\n",
      "  - Cleaned: False\n",
      "  - Original length: 2121 chars\n",
      "  - Cleaned length: 2121 chars\n",
      "  - Reduction: 0.0%\n",
      "\n",
      "üìÑ ORIGINAL TEXT (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "Michael Witmore\n",
      "Director, Folger Shakespeare Library\n",
      "It is hard to imagine a world without Shakespeare. Since their\n",
      "composition four hundred years ago, Shakespeare‚Äôs plays and poems\n",
      "have traveled the globe, inviting those who see and read his works to\n",
      "make them their own. Readers of the New Folger E\n",
      "...\n",
      "\n",
      "‚ú® CLEANED TEXT (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "Michael Witmore\n",
      "Director, Folger Shakespeare Library\n",
      "It is hard to imagine a world without Shakespeare. Since their\n",
      "composition four hundred years ago, Shakespeare‚Äôs plays and poems\n",
      "have traveled the globe, inviting those who see and read his works to\n",
      "make them their own. Readers of the New Folger E\n",
      "...\n",
      "\n",
      "üîç Artifacts Removed:\n",
      "  - No artifacts detected in this chunk\n",
      "\n",
      "============================================================\n",
      "CHUNK 2 COMPARISON\n",
      "============================================================\n",
      "\n",
      "üìä Metadata:\n",
      "  - Act: N/A\n",
      "  - Scene: N/A\n",
      "  - Cleaned: False\n",
      "  - Original length: 1674 chars\n",
      "  - Cleaned length: 1674 chars\n",
      "  - Reduction: 0.0%\n",
      "\n",
      "üìÑ ORIGINAL TEXT (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "Until now, with the release of The Folger Shakespeare (formerly\n",
      "Folger Digital Texts), readers in search of a free online text of\n",
      "Shakespeare‚Äôs plays had to be content primarily with using the\n",
      "Moby‚Ñ¢ Text, which reproduces a late-nineteenth century version of\n",
      "the plays. What is the difference? Many o\n",
      "...\n",
      "\n",
      "‚ú® CLEANED TEXT (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "Until now, with the release of The Folger Shakespeare (formerly\n",
      "Folger Digital Texts), readers in search of a free online text of\n",
      "Shakespeare‚Äôs plays had to be content primarily with using the\n",
      "Moby‚Ñ¢ Text, which reproduces a late-nineteenth century version of\n",
      "the plays. What is the difference? Many o\n",
      "...\n",
      "\n",
      "üîç Artifacts Removed:\n",
      "  - No artifacts detected in this chunk\n",
      "\n",
      "============================================================\n",
      "CHUNK 3 COMPARISON\n",
      "============================================================\n",
      "\n",
      "üìä Metadata:\n",
      "  - Act: N/A\n",
      "  - Scene: N/A\n",
      "  - Cleaned: True\n",
      "  - Original length: 462 chars\n",
      "  - Cleaned length: 461 chars\n",
      "  - Reduction: 0.2%\n",
      "\n",
      "üìÑ ORIGINAL TEXT (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "At\n",
      "any point in the text, you can hover your cursor over a bracket for\n",
      "more information. Because the Folger Shakespeare texts are edited in accord with\n",
      "twenty-first century knowledge about Shakespeare‚Äôs texts, the Folger\n",
      "here provides them to readers, scholars, teachers, actors, directors,\n",
      "and stude\n",
      "...\n",
      "\n",
      "‚ú® CLEANED TEXT (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "At\n",
      "any point in the text, you can hover your cursor over a bracket for\n",
      "more information. Because the Folger Shakespeare texts are edited in accord with\n",
      "twenty-first century knowledge about Shakespeare‚Äôs texts, the Folger\n",
      "here provides them to readers, scholars, teachers, actors, directors,\n",
      "and stude\n",
      "...\n",
      "\n",
      "üîç Artifacts Removed:\n",
      "  - FTLN references: 0\n",
      "  - ACT markers: 0\n",
      "  - SCENE markers: 0\n",
      "  - 'Julius Caesar' title: 0\n",
      "  - Leading page numbers: 0\n",
      "\n",
      "============================================================\n",
      "‚úÖ Cleaning verification complete!\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== CHUNKS WITH ACT/SCENE METADATA ===\n",
      "\n",
      "Found 82 chunks with Act/Scene metadata\n",
      "\n",
      "Chunk 5: Act 1, Scene 1\n",
      "  Text preview: COBBLER\n",
      "MARULLUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "COBBLER\n",
      "FLAVIUS\n",
      "COBBLER\n",
      "MARULLUS\n",
      "Nay, I beseech you, sir, be not ou...\n",
      "\n",
      "Chunk 6: Act 1, Scene 1\n",
      "  Text preview: FLAVIUS\n",
      "All the Commoners exit. MARULLUS\n",
      "FLAVIUS\n",
      "They exit in different directions. To hear the repl...\n",
      "\n",
      "Chunk 8: Act 1, Scene 2\n",
      "  Text preview: CAESAR\n",
      "CASCA\n",
      "CAESAR\n",
      "CALPHURNIA\n",
      "CAESAR\n",
      "ANTONY\n",
      "CAESAR\n",
      "ANTONY\n",
      "CAESAR\n",
      "Sennet. SOOTHSAYER\n",
      "CAESAR\n",
      "CASCA\n",
      "CA...\n",
      "\n",
      "Chunk 9: Act 1, Scene 2\n",
      "  Text preview: CAESAR\n",
      "CASSIUS\n",
      "The Soothsayer comes forward. CAESAR\n",
      "SOOTHSAYER\n",
      "CAESAR\n",
      "Sennet. All but Brutus and Cas...\n",
      "\n",
      "Chunk 10: Act 1, Scene 2\n",
      "  Text preview: CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "BRUTUS\n",
      "CASSIUS\n",
      "Nor construe any further my neglect\n",
      "Than that poor Brutus, wit...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display before/after examples for verification\n",
    "print(\"=== CLEANING VERIFICATION ===\\n\")\n",
    "\n",
    "# Show comparison for first 3 chunks\n",
    "for i in range(min(3, len(loaded_docs))):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHUNK {i+1} COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    original = loaded_docs[i].page_content\n",
    "    cleaned = documents[i]\n",
    "    metadata = metadatas[i]\n",
    "    \n",
    "    print(f\"\\nüìä Metadata:\")\n",
    "    print(f\"  - Act: {metadata.get('act', 'N/A')}\")\n",
    "    print(f\"  - Scene: {metadata.get('scene', 'N/A')}\")\n",
    "    print(f\"  - Cleaned: {metadata.get('cleaned', 'N/A')}\")\n",
    "    print(f\"  - Original length: {metadata.get('original_length', 0)} chars\")\n",
    "    print(f\"  - Cleaned length: {metadata.get('cleaned_length', 0)} chars\")\n",
    "    print(f\"  - Reduction: {(1 - metadata.get('cleaned_length', 0) / metadata.get('original_length', 1)):.1%}\")\n",
    "    \n",
    "    print(f\"\\nüìÑ ORIGINAL TEXT (first 300 chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(original[:300])\n",
    "    print(\"...\")\n",
    "    \n",
    "    print(f\"\\n‚ú® CLEANED TEXT (first 300 chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(cleaned[:300])\n",
    "    print(\"...\")\n",
    "    \n",
    "    print(f\"\\nüîç Artifacts Removed:\")\n",
    "    if original != cleaned:\n",
    "        # Show what was removed\n",
    "        removed_ftln = len(re.findall(r'FTLN\\s*\\d+', original, re.IGNORECASE))\n",
    "        removed_acts = len(re.findall(r'ACT\\s+[IVXLCDM0-9]+', original, re.IGNORECASE))\n",
    "        removed_scenes = len(re.findall(r'SCENE\\s+[IVXLCDM0-9]+', original, re.IGNORECASE))\n",
    "        removed_title = len(re.findall(r'Julius\\s+Caesar', original, re.IGNORECASE))\n",
    "        removed_page_nums = len(re.findall(r'^\\s*\\d+\\s+', original, re.MULTILINE))\n",
    "        \n",
    "        print(f\"  - FTLN references: {removed_ftln}\")\n",
    "        print(f\"  - ACT markers: {removed_acts}\")\n",
    "        print(f\"  - SCENE markers: {removed_scenes}\")\n",
    "        print(f\"  - 'Julius Caesar' title: {removed_title}\")\n",
    "        print(f\"  - Leading page numbers: {removed_page_nums}\")\n",
    "    else:\n",
    "        print(\"  - No artifacts detected in this chunk\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Cleaning verification complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show sample of chunks WITH Act/Scene metadata\n",
    "print(\"\\n\\n=== CHUNKS WITH ACT/SCENE METADATA ===\\n\")\n",
    "chunks_with_metadata = [(i, m) for i, m in enumerate(metadatas) if m.get('act') is not None]\n",
    "if chunks_with_metadata:\n",
    "    print(f\"Found {len(chunks_with_metadata)} chunks with Act/Scene metadata\\n\")\n",
    "    for idx, (chunk_idx, meta) in enumerate(chunks_with_metadata[:5]):\n",
    "        print(f\"Chunk {chunk_idx}: Act {meta.get('act')}, Scene {meta.get('scene')}\")\n",
    "        print(f\"  Text preview: {documents[chunk_idx][:100]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No chunks with Act/Scene metadata found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc47242",
   "metadata": {},
   "source": [
    "### Querying the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92ee0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rich table for displaying results (optional but nice)\n",
    "try:\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    \n",
    "    console = Console()\n",
    "    use_rich = True\n",
    "except ImportError:\n",
    "    use_rich = False\n",
    "    print(\"Rich package not found. Using standard print.\")\n",
    "\n",
    "# Function to display query results\n",
    "def print_results(results, use_rich=use_rich):\n",
    "    if use_rich:\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Rank\", width=6)\n",
    "        table.add_column(\"Document ID\")\n",
    "        table.add_column(\"Document Text\", width=60)\n",
    "        table.add_column(\"Page\")\n",
    "        table.add_column(\"Distance\")\n",
    "        \n",
    "        docs = results['documents'][0]\n",
    "        ids = results['ids'][0]\n",
    "        metas = results['metadatas'][0]\n",
    "        distances = results['distances'][0]\n",
    "        \n",
    "        for i, (doc, doc_id, meta, dist) in enumerate(zip(docs, ids, metas, distances)):\n",
    "            table.add_row(\n",
    "                str(i+1),\n",
    "                doc_id,\n",
    "                (doc[:100] + \"...\") if len(doc) > 100 else doc,\n",
    "                str(meta.get('page_number', 'N/A')),\n",
    "                f\"{dist:.4f}\"\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "    else:\n",
    "        # Standard print version\n",
    "        for i, (doc, meta, dist) in enumerate(zip(\n",
    "            results['documents'][0], \n",
    "            results['metadatas'][0], \n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            print(f\"\\n--- Result {i+1} ---\")\n",
    "            print(f\"Text: {doc[:100]}...\")\n",
    "            print(f\"Metadata: {meta}\")\n",
    "            print(f\"Distance: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ac82c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for query: 'What themes are explored in Julius Caesar?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Rank   </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Document ID           </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Document Text                                                </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Page </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Distance </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ 1      ‚îÇ julius-caesar_chunk_‚Ä¶ ‚îÇ CAESAR                                                       ‚îÇ 50   ‚îÇ 0.3766   ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ SOOTHSAYER                                                   ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ ARTEMIDORUS                                                  ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ DECIUS                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ ARTEMIDORUS                                                  ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CAESAR                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ ARTEMIDORUS                                                  ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CAESAR                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ PUBLIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ Caesar go...                                                 ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ 2      ‚îÇ julius-caesar_chunk_‚Ä¶ ‚îÇ POPILIUS                                                     ‚îÇ 51   ‚îÇ 0.3850   ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ POPILIUS                                                     ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ He walks away. BRUTUS                                        ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ BRUTUS                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ BRUTUS                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ Trebonius and ...                                            ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ 3      ‚îÇ julius-caesar_chunk_‚Ä¶ ‚îÇ Bear back! If you have tears, prepare to shed them now. You  ‚îÇ 68   ‚îÇ 0.3935   ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ all do know this mantle. I remember                          ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ The ...                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mRank  \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mDocument ID          \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mDocument Text                                               \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mPage\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mDistance\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ 1      ‚îÇ julius-caesar_chunk_‚Ä¶ ‚îÇ CAESAR                                                       ‚îÇ 50   ‚îÇ 0.3766   ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ SOOTHSAYER                                                   ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ ARTEMIDORUS                                                  ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ DECIUS                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ ARTEMIDORUS                                                  ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CAESAR                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ ARTEMIDORUS                                                  ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CAESAR                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ PUBLIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ Caesar go...                                                 ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ 2      ‚îÇ julius-caesar_chunk_‚Ä¶ ‚îÇ POPILIUS                                                     ‚îÇ 51   ‚îÇ 0.3850   ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ POPILIUS                                                     ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ He walks away. BRUTUS                                        ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ BRUTUS                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ BRUTUS                                                       ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ CASSIUS                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ Trebonius and ...                                            ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ 3      ‚îÇ julius-caesar_chunk_‚Ä¶ ‚îÇ Bear back! If you have tears, prepare to shed them now. You  ‚îÇ 68   ‚îÇ 0.3935   ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ all do know this mantle. I remember                          ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îÇ        ‚îÇ                       ‚îÇ The ...                                                      ‚îÇ      ‚îÇ          ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run a sample query\n",
    "query = \"What themes are explored in Julius Caesar?\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nResults for query: '{query}'\")\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb314ce",
   "metadata": {},
   "source": [
    "### Natural Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install groq --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497764f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix version conflicts - downgrade to compatible versions\n",
    "# !pip uninstall langchain-groq -y\n",
    "# !pip install \"langchain-core<0.3.0\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20e7f9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq LLM initialized with model: llama-3.3-70b-versatile (using native Groq SDK)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set the API key for Groq\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "# Create a simple LLM wrapper that works like LangChain's ChatGroq\n",
    "class GroqLLM:\n",
    "    def __init__(self, model, temperature=0.7):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.client = groq_client\n",
    "    \n",
    "    def invoke(self, prompt):\n",
    "        \"\"\"Invoke the LLM with a prompt and return response\"\"\"\n",
    "        # Handle both string prompts and LangChain PromptValue objects\n",
    "        if hasattr(prompt, 'to_string'):\n",
    "            prompt_text = prompt.to_string()\n",
    "        elif hasattr(prompt, 'text'):\n",
    "            prompt_text = prompt.text\n",
    "        else:\n",
    "            prompt_text = str(prompt)\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            model=self.model,\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        \n",
    "        # Return an object with .content attribute for compatibility\n",
    "        class Response:\n",
    "            def __init__(self, content):\n",
    "                self.content = content\n",
    "            def __str__(self):\n",
    "                return self.content\n",
    "        \n",
    "        return Response(response.choices[0].message.content)\n",
    "\n",
    "# Initialize our custom LLM wrapper\n",
    "llm = GroqLLM(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Groq LLM initialized with model: llama-3.3-70b-versatile (using native Groq SDK)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dab6b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Better prompt template for Julius Caesar\n",
    "rag_prompt_template = \"\"\"\n",
    "You are an expert on Shakespeare's Julius Caesar. Answer questions using ONLY the context below.\n",
    "If you can't find a complete answer in the context but see partial information, try to provide what you can find and acknowledge the limitations of the available information.\n",
    "If there is NO relevant information at all in the context, respond with \"I don't have enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (based only on the context provided):\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb979b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "def answer_with_hybrid_rag(query, n_results=5):\n",
    "    # 1. Semantic search with ChromaDB\n",
    "    semantic_results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # 2. Perform keyword search with BM25\n",
    "    # First get all documents to search across\n",
    "    all_docs = collection.get(\n",
    "        limit=100,  # Adjust based on your collection size\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    # Tokenize for BM25\n",
    "    tokenized_docs = [doc.split() for doc in all_docs[\"documents\"]]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top BM25 results\n",
    "    top_bm25_indices = np.argsort(bm25_scores)[-n_results:][::-1]\n",
    "    \n",
    "    # 3. Combine results (simple union)\n",
    "    combined_docs = []\n",
    "    combined_meta = []\n",
    "    combined_ids = [] \n",
    "    seen_ids = set()\n",
    "    \n",
    "    # Add semantic results\n",
    "    for doc, meta, doc_id in zip(\n",
    "        semantic_results[\"documents\"][0], \n",
    "        semantic_results[\"metadatas\"][0],\n",
    "        semantic_results[\"ids\"][0]\n",
    "    ):\n",
    "        if doc_id not in seen_ids:\n",
    "            combined_docs.append(doc)\n",
    "            combined_meta.append(meta)\n",
    "            combined_ids.append(doc_id)\n",
    "            seen_ids.add(doc_id)\n",
    "    \n",
    "    # Add keyword results\n",
    "    for idx in top_bm25_indices:\n",
    "        doc_id = all_docs[\"ids\"][idx]\n",
    "        if doc_id not in seen_ids:\n",
    "            combined_docs.append(all_docs[\"documents\"][idx])\n",
    "            combined_meta.append(all_docs[\"metadatas\"][idx])\n",
    "            combined_ids.append(doc_id)\n",
    "            seen_ids.add(doc_id)\n",
    "    \n",
    "    # Limit to n_results total\n",
    "    combined_docs = combined_docs[:n_results]\n",
    "    combined_meta = combined_meta[:n_results]\n",
    "    combined_ids = combined_ids[:n_results]\n",
    "    \n",
    "    # Format context and complete RAG as before\n",
    "    formatted_docs = []\n",
    "    for doc, meta in zip(combined_docs, combined_meta):\n",
    "        page_num = meta.get(\"page_number\", \"unknown\")\n",
    "        formatted_docs.append(f\"[Page {page_num}]: {doc}\")\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(formatted_docs)\n",
    "    filled_prompt = prompt.format(context=context, query=query)\n",
    "    response = llm.invoke(filled_prompt)\n",
    "    \n",
    "    # Create a mock results object for print_results compatibility\n",
    "    mock_results = {\n",
    "        \"documents\": [combined_docs],\n",
    "        \"metadatas\": [combined_meta],\n",
    "        \"distances\": [[0.0] * len(combined_docs)],  # Placeholder distances\n",
    "        \"ids\": [combined_ids]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": response.content if hasattr(response, 'content') else str(response),\n",
    "        \"source_documents\": mock_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "960bba69",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test our RAG pipeline with a question\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the relationship between Brutus and Caesar?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manswer_with_hybrid_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 70\u001b[0m, in \u001b[0;36manswer_with_hybrid_rag\u001b[1;34m(query, n_results)\u001b[0m\n\u001b[0;32m     68\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(formatted_docs)\n\u001b[0;32m     69\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(context\u001b[38;5;241m=\u001b[39mcontext, query\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m---> 70\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilled_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Create a mock results object for print_results compatibility\u001b[39;00m\n\u001b[0;32m     73\u001b[0m mock_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: [combined_docs],\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m: [combined_meta],\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(combined_docs)],  \u001b[38;5;66;03m# Placeholder distances\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m: [combined_ids]\n\u001b[0;32m     78\u001b[0m }\n",
      "Cell \u001b[1;32mIn[27], line 27\u001b[0m, in \u001b[0;36mGroqLLM.invoke\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     prompt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(prompt)\n\u001b[1;32m---> 27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Return an object with .content attribute for compatibility\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mResponse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:464\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[0;32m    304\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcitation_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompound_custom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisable_tool_validation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_reasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1043\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# Test our RAG pipeline with a question\n",
    "test_query = \"What is the relationship between Brutus and Caesar?\"\n",
    "response = answer_with_hybrid_rag(test_query)\n",
    "\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"\\nAnswer: {response['answer']}\")\n",
    "print(\"\\nSources:\")\n",
    "print_results(response[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d52efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Question: What are the main themes in Julius Caesar?\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manswer_with_hybrid_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop source:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 70\u001b[0m, in \u001b[0;36manswer_with_hybrid_rag\u001b[1;34m(query, n_results)\u001b[0m\n\u001b[0;32m     68\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(formatted_docs)\n\u001b[0;32m     69\u001b[0m filled_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(context\u001b[38;5;241m=\u001b[39mcontext, query\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m---> 70\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilled_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Create a mock results object for print_results compatibility\u001b[39;00m\n\u001b[0;32m     73\u001b[0m mock_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: [combined_docs],\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m: [combined_meta],\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(combined_docs)],  \u001b[38;5;66;03m# Placeholder distances\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m: [combined_ids]\n\u001b[0;32m     78\u001b[0m }\n",
      "Cell \u001b[1;32mIn[27], line 27\u001b[0m, in \u001b[0;36mGroqLLM.invoke\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     prompt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(prompt)\n\u001b[1;32m---> 27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Return an object with .content attribute for compatibility\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mResponse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:464\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[0;32m    304\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcitation_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompound_custom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisable_tool_validation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_reasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1043\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions to evaluate system\n",
    "results_for_export = []\n",
    "\n",
    "test_questions = [\n",
    "    \"What are the main themes in Julius Caesar?\",\n",
    "    \"How does Brutus justify killing Caesar?\",\n",
    "    \"What role does Cassius play in the conspiracy?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Question: {question}\")\n",
    "    response = answer_with_hybrid_rag(question)\n",
    "    print(f\"\\nAnswer: {response['answer']}\")\n",
    "    print(\"\\nTop source:\")\n",
    "    if len(response[\"source_documents\"][\"documents\"][0]) > 0:\n",
    "        top_doc = response[\"source_documents\"][\"documents\"][0][0]\n",
    "        top_meta = response[\"source_documents\"][\"metadatas\"][0][0]\n",
    "        page = top_meta.get(\"page_number\", \"N/A\")\n",
    "        print(f\"[Page {page}]:\\n{top_doc[:200]}...\")  # Print first 200 chars\n",
    "        # Save for export\n",
    "        results_for_export.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response['answer'],\n",
    "            \"page\": page,\n",
    "            \"chunk\": top_doc\n",
    "        })\n",
    "    else:\n",
    "        print(\"No sources found.\")\n",
    "        results_for_export.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response['answer'],\n",
    "            \"page\": None,\n",
    "            \"chunk\": None\n",
    "        })\n",
    "\n",
    "# Export results to a well-formatted text file\n",
    "with open(multiquery_rag_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"RAG Multi-Query Evaluation Results\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    for idx, res in enumerate(results_for_export, 1):\n",
    "        f.write(f\"Question {idx}: {res['question']}\\n\")\n",
    "        f.write(f\"Answer:\\n{res['answer']}\\n\\n\")\n",
    "        if res[\"chunk\"]:\n",
    "            f.write(f\"Top Source Chunk (Page {res['page']}):\\n{res['chunk']}\\n\")\n",
    "        else:\n",
    "            f.write(\"Top Source Chunk: No sources found.\\n\")\n",
    "        f.write(\"-\"*60 + \"\\n\\n\")\n",
    "print(f\"\\nResults exported to {multiquery_rag_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c3905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install langchain-community==0.2.17, langchain-core==0.2.38 and langchain==0.2.16 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"ragas>=0.2.0,<0.3.0\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c56031",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_data_content_block' from 'langchain_core.messages' (c:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\messages\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m faithfulness, answer_relevancy\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingfaceEmbeddings\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\ragas\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CacheInterface, DiskCacheBackend, cacher\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset, MultiTurnSample, SingleTurnSample\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\ragas\\evaluation.py:20\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChainType, RagasTracer, new_group\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     EvaluationDataset,\n\u001b[0;32m     16\u001b[0m     EvaluationResult,\n\u001b[0;32m     17\u001b[0m     MultiTurnSample,\n\u001b[0;32m     18\u001b[0m     SingleTurnSample,\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     BaseRagasEmbeddings,\n\u001b[0;32m     22\u001b[0m     LangchainEmbeddingsWrapper,\n\u001b[0;32m     23\u001b[0m     embedding_factory,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExceptionInRunner\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Executor\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\ragas\\embeddings\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     BaseRagasEmbeddings,\n\u001b[0;32m      3\u001b[0m     HuggingfaceEmbeddings,\n\u001b[0;32m      4\u001b[0m     LangchainEmbeddingsWrapper,\n\u001b[0;32m      5\u001b[0m     LlamaIndexEmbeddingsWrapper,\n\u001b[0;32m      6\u001b[0m     embedding_factory,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhaystack_wrapper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HaystackEmbeddingsWrapper\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseRagasEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaystackEmbeddingsWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\ragas\\embeddings\\base.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embeddings\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoreSchema, core_schema\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_openai\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Module for OpenAI integrations.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Module for OpenAI chat models.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field, SecretStr, model_validator\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Self\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m _BM \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BM\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mBaseModel)\n",
      "File \u001b[1;32mc:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:48\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModelInput\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     BaseChatModel,\n\u001b[0;32m     46\u001b[0m     LangSmithParams,\n\u001b[0;32m     47\u001b[0m )\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     49\u001b[0m     AIMessage,\n\u001b[0;32m     50\u001b[0m     AIMessageChunk,\n\u001b[0;32m     51\u001b[0m     BaseMessage,\n\u001b[0;32m     52\u001b[0m     BaseMessageChunk,\n\u001b[0;32m     53\u001b[0m     ChatMessage,\n\u001b[0;32m     54\u001b[0m     ChatMessageChunk,\n\u001b[0;32m     55\u001b[0m     FunctionMessage,\n\u001b[0;32m     56\u001b[0m     FunctionMessageChunk,\n\u001b[0;32m     57\u001b[0m     HumanMessage,\n\u001b[0;32m     58\u001b[0m     HumanMessageChunk,\n\u001b[0;32m     59\u001b[0m     InvalidToolCall,\n\u001b[0;32m     60\u001b[0m     SystemMessage,\n\u001b[0;32m     61\u001b[0m     SystemMessageChunk,\n\u001b[0;32m     62\u001b[0m     ToolCall,\n\u001b[0;32m     63\u001b[0m     ToolMessage,\n\u001b[0;32m     64\u001b[0m     ToolMessageChunk,\n\u001b[0;32m     65\u001b[0m     is_data_content_block,\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m content \u001b[38;5;28;01mas\u001b[39;00m types\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     InputTokenDetails,\n\u001b[0;32m     70\u001b[0m     OutputTokenDetails,\n\u001b[0;32m     71\u001b[0m     UsageMetadata,\n\u001b[0;32m     72\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_data_content_block' from 'langchain_core.messages' (c:\\Users\\micro\\Anaconda3\\envs\\NLP_2\\Lib\\site-packages\\langchain_core\\messages\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# === Groq + RAG + RAGAS Evaluation ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets groq tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from groq import Groq\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "\n",
    "\n",
    "# ==== CONFIG ====\n",
    "# Use the API key already set in previous cell\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "cached_answers_path = \"../RAG Results/cached_rag_answers.json\"  # NEW: Cache file\n",
    "TOP_K = 3\n",
    "\n",
    "GROQ_RAG_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_RAGAS_MODEL = \"llama-3.3-70b-versatile\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Rate limiting config for llama-3.3-70b-versatile:\n",
    "# RPM: 30 (requests per minute)\n",
    "# RPD: 1,000 (requests per day)\n",
    "# TPM: 12,000 (tokens per minute)\n",
    "# TPD: 100,000 (tokens per day)\n",
    "REQUEST_DELAY = 2.5  # seconds between requests (allows ~24 RPM, safe margin below 30 RPM)\n",
    "BATCH_SIZE = 5  # Process in small batches to avoid hitting token limits\n",
    "MAX_RETRIES = 3  # Retry failed requests\n",
    "\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_200 = f.read(200)\n",
    "print(\"First few characters:\\n\", first_200)\n",
    "\n",
    "\n",
    "# ==== 1Ô∏è‚É£ Load test data ====\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "\n",
    "# ==== 2Ô∏è‚É£ Groq generation with retry logic ====\n",
    "def generate_with_groq(prompt, model_name=GROQ_RAG_MODEL, retries=MAX_RETRIES):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                wait_time = REQUEST_DELAY * (attempt + 2)  # Exponential backoff\n",
    "                print(f\"[WARN] Rate limit hit. Waiting {wait_time}s before retry {attempt + 1}/{retries}\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] Groq API call failed (attempt {attempt + 1}): {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    time.sleep(REQUEST_DELAY)\n",
    "                    return None\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==== 3Ô∏è‚É£ Groq wrapper for RAGAS following BaseRagasLLM interface ====\n",
    "from ragas.llms.base import BaseRagasLLM as RagasBaseLLM\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "class GroqRagasLLM(RagasBaseLLM):\n",
    "    \"\"\"Groq LLM wrapper implementing RAGAS BaseRagasLLM interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(run_config=RunConfig())\n",
    "        self.model_name = model_name\n",
    "        self.client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "        self.request_count = 0\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def _rate_limit_check(self):\n",
    "        \"\"\"Ensure we don't exceed rate limits\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        \n",
    "        # Ensure minimum delay between requests\n",
    "        if time_since_last < REQUEST_DELAY:\n",
    "            sleep_time = REQUEST_DELAY - time_since_last\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "        self.request_count += 1\n",
    "\n",
    "    def _extract_text_from_prompt(self, prompt: PromptValue) -> str:\n",
    "        \"\"\"Extract text from PromptValue object.\"\"\"\n",
    "        # PromptValue has .to_string() method\n",
    "        if hasattr(prompt, \"to_string\"):\n",
    "            return prompt.to_string()\n",
    "        # Fallback to string conversion\n",
    "        return str(prompt)\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Synchronous generation - required by BaseRagasLLM.\"\"\"\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Rate limit check\n",
    "                    self._rate_limit_check()\n",
    "                    \n",
    "                    chat_completion = self.client.chat.completions.create(\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break  # Success\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    async def agenerate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Asynchronous generation - required by BaseRagasLLM.\"\"\"\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Rate limit check\n",
    "                    await asyncio.sleep(REQUEST_DELAY)\n",
    "                    \n",
    "                    # Run blocking SDK call in thread\n",
    "                    chat_completion = await asyncio.to_thread(\n",
    "                        self.client.chat.completions.create,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break  # Success\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            await asyncio.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    def is_finished(self, response: LLMResult) -> bool:\n",
    "        \"\"\"Check if response is complete - required by BaseRagasLLM.\"\"\"\n",
    "        return True\n",
    "\n",
    "\n",
    "# ==== 4Ô∏è‚É£ Check collection availability ====\n",
    "try:\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB) is NOT defined.\")\n",
    "    print(\"Please initialize your ChromaDB client/collection before running this cell.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# ==== 5Ô∏è‚É£ Generate records with caching and rate limiting ====\n",
    "records = []\n",
    "\n",
    "# Check if cached answers exist\n",
    "if os.path.exists(cached_answers_path):\n",
    "    print(f\"[INFO] Found cached answers at '{cached_answers_path}'\")\n",
    "    try:\n",
    "        with open(cached_answers_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cached_data = json.load(f)\n",
    "        \n",
    "        # Validate cache matches current test data\n",
    "        if len(cached_data) == len(test_data):\n",
    "            questions_match = all(\n",
    "                cached_data[i][\"question\"] == test_data[i][\"question\"] \n",
    "                for i in range(len(test_data))\n",
    "            )\n",
    "            \n",
    "            if questions_match:\n",
    "                print(f\"[INFO] Loading {len(cached_data)} cached answers (skipping generation)\")\n",
    "                records = cached_data\n",
    "            else:\n",
    "                print(\"[WARN] Cached questions don't match test data. Regenerating...\")\n",
    "        else:\n",
    "            print(f\"[WARN] Cache size mismatch ({len(cached_data)} vs {len(test_data)}). Regenerating...\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load cache: {e}. Regenerating...\")\n",
    "\n",
    "# Generate new answers if cache not usable\n",
    "if not records:\n",
    "    print(f\"[INFO] Generating RAG answers with rate limiting (max 30 RPM)...\")\n",
    "    print(f\"[INFO] Request delay: {REQUEST_DELAY}s | Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    for item in tqdm(test_data, desc=\"Generating Groq RAG answers\"):\n",
    "        question = item[\"question\"]\n",
    "        ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "        retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "        retrieved_docs = retrieved[\"documents\"][0]\n",
    "        retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        prompt = (\n",
    "            f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        generated_answer = generate_with_groq(prompt)\n",
    "        if not generated_answer:\n",
    "            generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "        records.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": retrieved_docs,\n",
    "            \"answer\": generated_answer,\n",
    "            \"ground_truth\": ideal_answer,\n",
    "        })\n",
    "        \n",
    "        # Progress update every 5 questions\n",
    "        if len(records) % 5 == 0:\n",
    "            print(f\"[INFO] Processed {len(records)}/{len(test_data)} questions\")\n",
    "    \n",
    "    # Save generated answers to cache\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(cached_answers_path), exist_ok=True)\n",
    "        with open(cached_answers_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[SUCCESS] Cached {len(records)} answers to '{cached_answers_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to save cache: {e}\")\n",
    "\n",
    "\n",
    "# ==== 6Ô∏è‚É£ Convert to HF Dataset ====\n",
    "dataset = Dataset.from_list(records)\n",
    "print(f\"[INFO] Created dataset with {len(dataset)} samples\")\n",
    "\n",
    "\n",
    "# ==== 7Ô∏è‚É£ Custom HuggingFace Embedding Wrapper ====\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    \"\"\"Implements both sync + async embedding methods for latest RAGAS.\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        # ‚úÖ Do not call super()\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    # --- Sync methods ---\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False).tolist()[0]\n",
    "\n",
    "    # --- Async methods ---\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "\n",
    "# ==== 8Ô∏è‚É£ Evaluate with RAGAS ====\n",
    "llm = GroqRagasLLM(GROQ_RAGAS_MODEL)\n",
    "embeddings = CustomHuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "print(f\"\\n[INFO] Starting RAGAS evaluation with {GROQ_RAGAS_MODEL}...\")\n",
    "print(f\"[INFO] Rate limits: 30 RPM | 12K TPM | Using {REQUEST_DELAY}s delays\")\n",
    "print(f\"[INFO] Estimated time: ~{len(dataset) * REQUEST_DELAY / 60:.1f} minutes\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n[SUCCESS] Evaluation completed in {elapsed_time / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "# ==== 9Ô∏è‚É£ Save Results ====\n",
    "faithfulness_scores = results[\"faithfulness\"]\n",
    "answer_relevancy_scores = results[\"answer_relevancy\"]\n",
    "\n",
    "# ‚úÖ Compute mean values\n",
    "faithfulness_mean = float(np.mean(faithfulness_scores))\n",
    "answer_relevancy_mean = float(np.mean(answer_relevancy_scores))\n",
    "\n",
    "os.makedirs(os.path.dirname(output_metrics_path), exist_ok=True)\n",
    "\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Groq + RAGAS) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "    f.write(f\"Evaluation Duration: {elapsed_time / 60:.2f} minutes\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GROQ_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GROQ_RAGAS_MODEL}\\n\")\n",
    "    f.write(f\"Rate Limiting: {REQUEST_DELAY}s delay between requests\\n\")\n",
    "    f.write(f\"Cached Answers: {os.path.basename(cached_answers_path)}\\n\\n\")\n",
    "    f.write(f\"Faithfulness (avg): {faithfulness_mean:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy (avg): {answer_relevancy_mean:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness (avg): {faithfulness_mean:.4f} | Answer Relevancy (avg): {answer_relevancy_mean:.4f}\")\n",
    "print(f\"\\n[TIP] To regenerate answers, delete: {cached_answers_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
