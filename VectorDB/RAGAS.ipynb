{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "041aa939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: C:\\Users\\micro\\Desktop\\Abhinav college\\Resources\\Sem 7\\Advanced NLP\\Assignment 2\\RAG-A2\\VectorDB\\chroma_Data_v_final\n",
      "[SUCCESS] Loaded collection 'anlp_rag_collection'\n",
      "[INFO] Count: 126\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Path\n",
    "Relative_Database_path = \"./chroma_Data_v_final\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "collection_name = \"anlp_rag_collection\"\n",
    "\n",
    "# Initialize Chroma\n",
    "client = chromadb.PersistentClient(path=str(Absolute_Database_path))\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# Correct embedding function: use model_name (primitive), not a model instance\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Load existing collection\n",
    "collection = client.get_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "print(f\"[SUCCESS] Loaded collection '{collection_name}'\")\n",
    "print(f\"[INFO] Count: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e5eba",
   "metadata": {},
   "source": [
    "5 Groq API calls => trying to handle rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90bbe660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Size: 7215 bytes\n",
      "[INFO] Loaded 37 QA pairs.\n",
      "[INFO] Generating new RAG answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [08:18<00:00, 13.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running RAGAS evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL LIMIT] Waiting 9.1s...\n",
      "[GLOBAL LIMIT] Waiting 0.8s...\n",
      "[GLOBAL LIMIT] Waiting 0.5s...\n",
      "[GLOBAL LIMIT] Waiting 0.6s...\n",
      "[GLOBAL LIMIT] Waiting 1.0s...\n",
      "[GLOBAL LIMIT] Waiting 1.1s...\n",
      "[GLOBAL LIMIT] Waiting 8.7s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|â–ˆâ–ˆâ–       | 18/74 [03:20<07:54,  8.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL LIMIT] Waiting 8.0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [08:55<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "Faithfulness: 0.7333547833547833\n",
      "Answer Relevancy: 0.7745587596295099\n"
     ]
    }
   ],
   "source": [
    "# === Groq + RAG + RAGAS Evaluation (Fully Patched Version) ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets groq tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from datasets import Dataset\n",
    "\n",
    "from groq import Groq\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "from ragas.llms.base import BaseRagasLLM as RagasBaseLLM\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "# ============================================================\n",
    "# NEW: LOG FILE\n",
    "# ============================================================\n",
    "GROQ_LOG_PATH = \"../RAG Results/groq_logs.txt\"\n",
    "\n",
    "def log_groq_call(model, prompt, response):\n",
    "    \"\"\"Append Groq prompt/response to log file.\"\"\"\n",
    "    with open(GROQ_LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n===============================================\\n\")\n",
    "        f.write(f\"TIMESTAMP: {datetime.now()}\\n\")\n",
    "        f.write(f\"MODEL: {model}\\n\\n\")\n",
    "        f.write(\"PROMPT:\\n\")\n",
    "        f.write(prompt + \"\\n\\n\")\n",
    "        f.write(\"RESPONSE:\\n\")\n",
    "        f.write(response + \"\\n\")\n",
    "        f.write(\"===============================================\\n\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¥ 5 GROQ API KEYS - ROUND ROBIN\n",
    "# ============================================================\n",
    "API_KEYS = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "]\n",
    "\n",
    "rr_index = 0\n",
    "def get_next_client():\n",
    "    global rr_index\n",
    "    client = Groq(api_key=API_KEYS[rr_index])\n",
    "    rr_index = (rr_index + 1) % len(API_KEYS)\n",
    "    return client\n",
    "\n",
    "# ============================================================\n",
    "# GLOBAL SAFE RATE LIMITER\n",
    "# ============================================================\n",
    "MAX_RPM = 25\n",
    "WINDOW = 60\n",
    "global_request_times = deque()\n",
    "\n",
    "def wait_for_slot():\n",
    "    now = time.time()\n",
    "\n",
    "    while global_request_times and now - global_request_times[0] > WINDOW:\n",
    "        global_request_times.popleft()\n",
    "\n",
    "    if len(global_request_times) >= MAX_RPM:\n",
    "        sleep_time = WINDOW - (now - global_request_times[0]) + 0.5\n",
    "        print(f\"[GLOBAL LIMIT] Waiting {sleep_time:.1f}s...\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    global_request_times.append(time.time())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "cached_answers_path = \"../RAG Results/cached_rag_answers.json\"\n",
    "\n",
    "TOP_K = 3\n",
    "\n",
    "GROQ_RAG_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_RAGAS_MODEL = \"llama-3.1-8b-instant\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "REQUEST_DELAY = 12.0\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load testbed\n",
    "# ============================================================\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Groq generation helper\n",
    "# ============================================================\n",
    "def generate_with_groq(prompt, model_name=GROQ_RAG_MODEL):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            wait_for_slot()\n",
    "            client = get_next_client()\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                temperature=0.7,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt + \"\\n\\nIMPORTANT: Respond in plain text. No lists. No JSON.\"\n",
    "                }]\n",
    "            )\n",
    "            text = response.choices[0].message.content.strip()\n",
    "\n",
    "            log_groq_call(model_name, prompt, text)\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                sleep_time = REQUEST_DELAY * (attempt + 2)\n",
    "                print(f\"[WARN] Rate limit. Sleeping {sleep_time}s\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] {e}\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Groq wrapper for RAGAS scoring (JSON FORCING FIX APPLIED)\n",
    "# ============================================================\n",
    "class GroqRagasLLM(RagasBaseLLM):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(run_config=RunConfig())\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def _extract_text(self, prompt: PromptValue):\n",
    "        if hasattr(prompt, \"to_string\"):\n",
    "            return prompt.to_string()\n",
    "        return str(prompt)\n",
    "\n",
    "    def generate_text(self, prompt: PromptValue, n=1, temperature=0.0, stop=None, callbacks=None):\n",
    "        raw_prompt = self._extract_text(prompt)\n",
    "\n",
    "        # STRICT JSON WRAPPER â€” FIXES ALL PARSER ERRORS\n",
    "        json_prompt = f\"\"\"\n",
    "You MUST respond ONLY with valid JSON.\n",
    "No explanations. No natural language. No commentary. No markdown.\n",
    "If an array is expected, return an array.\n",
    "If a dictionary is expected, return a dictionary.\n",
    "Your entire response MUST be valid JSON.\n",
    "\n",
    "Task:\n",
    "{raw_prompt}\n",
    "\"\"\"\n",
    "\n",
    "        generations = []\n",
    "\n",
    "        for _ in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    wait_for_slot()\n",
    "                    client = get_next_client()\n",
    "\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                        messages=[{\"role\": \"user\", \"content\": json_prompt}]\n",
    "                    )\n",
    "\n",
    "                    text = response.choices[0].message.content.strip()\n",
    "                    log_groq_call(self.model_name, json_prompt, text)\n",
    "\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break\n",
    "\n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower():\n",
    "                        time.sleep(REQUEST_DELAY)\n",
    "                    else:\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text='{\"error\": \"LLM failed\"}')])\n",
    "                        else:\n",
    "                            time.sleep(REQUEST_DELAY)\n",
    "\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    async def agenerate_text(self, *args, **kwargs):\n",
    "        return self.generate_text(*args, **kwargs)\n",
    "\n",
    "    def is_finished(self, response):\n",
    "        return True\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Generate RAG answers (with caching)\n",
    "# ============================================================\n",
    "records = []\n",
    "\n",
    "if os.path.exists(cached_answers_path):\n",
    "    print(\"[INFO] Loading cached answers...\")\n",
    "    with open(cached_answers_path, \"r\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "if not records:\n",
    "    print(\"[INFO] Generating new RAG answers...\")\n",
    "    for item in tqdm(test_data):\n",
    "        q = item[\"question\"]\n",
    "        gt = item[\"ideal_answer\"]\n",
    "\n",
    "        retrieved = collection.query(query_texts=[q], n_results=TOP_K)\n",
    "        docs = retrieved[\"documents\"][0]\n",
    "        context = \"\\n\".join(docs)\n",
    "\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{q}\\n\\nAnswer:\"\n",
    "        ans = generate_with_groq(prompt)\n",
    "        if not ans:\n",
    "            ans = \"[Error: failed generation]\"\n",
    "\n",
    "        records.append({\n",
    "            \"question\": q,\n",
    "            \"contexts\": docs,\n",
    "            \"answer\": ans,\n",
    "            \"ground_truth\": gt\n",
    "        })\n",
    "\n",
    "    with open(cached_answers_path, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Convert to HF dataset\n",
    "# ============================================================\n",
    "dataset = Dataset.from_list(records)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Embedding wrapper (ASYNC METHODS FIXED)\n",
    "# ============================================================\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False)[0].tolist()\n",
    "\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "embeddings = CustomHuggingfaceEmbeddings(EMBED_MODEL)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run RAGAS evaluation\n",
    "# ============================================================\n",
    "llm = GroqRagasLLM(GROQ_RAGAS_MODEL)\n",
    "\n",
    "print(\"[INFO] Running RAGAS evaluation...\")\n",
    "start = time.time()\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "elapsed = (end - start) / 60\n",
    "\n",
    "faith_avg = float(np.mean(results[\"faithfulness\"]))\n",
    "rel_avg = float(np.mean(results[\"answer_relevancy\"]))\n",
    "\n",
    "with open(output_metrics_path, \"w\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics ===\\n\")\n",
    "    f.write(f\"Time: {datetime.now()}\\n\")\n",
    "    f.write(f\"Duration: {elapsed:.2f} minutes\\n\")\n",
    "    f.write(f\"Faithfulness: {faith_avg:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy: {rel_avg:.4f}\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(\"DONE!\")\n",
    "print(\"Faithfulness:\", faith_avg)\n",
    "print(\"Answer Relevancy:\", rel_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "918a0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../RAG Results/cached_rag_answers.json\", \"w\") as f:\n",
    "    json.dump(records, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
