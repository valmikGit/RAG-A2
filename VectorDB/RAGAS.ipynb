{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041aa939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: C:\\Users\\Gaming window\\Desktop\\ANLP_Assignment_2\\RAG-A2\\VectorDB\\chroma_Data_v5\n",
      "[SUCCESS] Loaded collection 'anlp_rag_collection'\n",
      "[INFO] Count: 126\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Path\n",
    "Relative_Database_path = \"./chroma_Data_v5\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "collection_name = \"anlp_rag_collection\"\n",
    "\n",
    "# Initialize Chroma\n",
    "client = chromadb.PersistentClient(path=str(Absolute_Database_path))\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# Correct embedding function: use model_name (primitive), not a model instance\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Load existing collection\n",
    "collection = client.get_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "print(f\"[SUCCESS] Loaded collection '{collection_name}'\")\n",
    "print(f\"[INFO] Count: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cd077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Size: 3632 bytes\n",
      "First few characters:\n",
      " [\n",
      "    {\n",
      "        \"question\": \"How does Caesar first enter the play?\",\n",
      "        \"ideal_answer\": \"In a triumphal procession; he has defeated the sons of his deceased rival, Pompey\"\n",
      "    },\n",
      "{\n",
      "\"question\": \"W\n",
      "[INFO] Loaded 25 QA pairs from testbed.\n",
      "[INFO] Found cached answers at '../RAG Results/cached_rag_answers.json'\n",
      "[INFO] Loading 25 cached answers (skipping generation)\n",
      "[INFO] Created dataset with 25 samples\n",
      "\n",
      "[INFO] Starting RAGAS evaluation with llama-3.3-70b-versatile...\n",
      "[INFO] Rate limits: 30 RPM | 12K TPM | Using 7.5s delays\n",
      "[INFO] Estimated time: ~3.1 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 476. Please try again in 3m55.008s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 396. Please try again in 2m45.888s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 466. Please try again in 3m46.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 383. Please try again in 2m34.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 411. Please try again in 2m58.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 501. Please try again in 4m16.608s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 419. Please try again in 3m5.76s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 648. Please try again in 6m23.616s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 383. Please try again in 2m34.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 385. Please try again in 2m36.384s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 475. Please try again in 3m54.144s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 408. Please try again in 2m56.256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 422. Please try again in 3m8.352s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 408. Please try again in 2m56.256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 408. Please try again in 2m56.256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 385. Please try again in 2m36.384s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 385. Please try again in 2m36.384s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 634. Please try again in 6m11.52s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 396. Please try again in 2m45.888s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 487. Please try again in 4m4.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 466. Please try again in 3m46.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 455. Please try again in 3m36.864s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 411. Please try again in 2m58.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 383. Please try again in 2m34.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 411. Please try again in 2m58.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 487. Please try again in 4m4.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 487. Please try again in 4m4.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 634. Please try again in 6m11.52s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 466. Please try again in 3m46.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 455. Please try again in 3m36.864s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 634. Please try again in 6m11.52s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99796, Requested 455. Please try again in 3m36.864s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[14]: TimeoutError()\n",
      "Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[6]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Exception raised in Job[12]: TimeoutError()\n",
      "Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[15]: TimeoutError()\n",
      "Exception raised in Job[3]: TimeoutError()\n",
      "Evaluating:   2%|â–         | 1/50 [03:00<2:27:01, 180.04s/it]Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[9]: TimeoutError()\n",
      "Exception raised in Job[1]: TimeoutError()\n",
      "Exception raised in Job[7]: TimeoutError()\n",
      "Exception raised in Job[13]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 451. Please try again in 5m52.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 517. Please try again in 6m49.536s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 493. Please try again in 6m28.799999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 439. Please try again in 5m42.144s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 525. Please try again in 6m56.448s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 439. Please try again in 5m42.144s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 504. Please try again in 6m38.304s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 525. Please try again in 6m56.448s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 465. Please try again in 6m4.608s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 525. Please try again in 6m56.448s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 448. Please try again in 5m49.919999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 504. Please try again in 6m38.304s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 537. Please try again in 7m6.816s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 448. Please try again in 5m49.919999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 445. Please try again in 5m47.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 479. Please try again in 6m16.704s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 439. Please try again in 5m42.144s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 504. Please try again in 6m38.304s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 445. Please try again in 5m47.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 479. Please try again in 6m16.704s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 445. Please try again in 5m47.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 457. Please try again in 5m57.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 440. Please try again in 5m43.008s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 479. Please try again in 6m16.704s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 448. Please try again in 5m49.919999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99928, Requested 388. Please try again in 4m33.024s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[18]: TimeoutError()\n",
      "Exception raised in Job[22]: TimeoutError()\n",
      "Exception raised in Job[16]: TimeoutError()\n",
      "Exception raised in Job[20]: TimeoutError()\n",
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [06:00<10:04, 18.32s/it]  Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[21]: TimeoutError()\n",
      "Exception raised in Job[19]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[24]: TimeoutError()\n",
      "Exception raised in Job[28]: TimeoutError()\n",
      "Exception raised in Job[26]: TimeoutError()\n",
      "Exception raised in Job[30]: TimeoutError()\n",
      "Exception raised in Job[27]: TimeoutError()\n",
      "Exception raised in Job[25]: TimeoutError()\n",
      "Exception raised in Job[31]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99749, Requested 381. Please try again in 1m52.32s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99749, Requested 369. Please try again in 1m41.952s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99749, Requested 485. Please try again in 3m22.176s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99749, Requested 439. Please try again in 2m42.432s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99749, Requested 447. Please try again in 2m49.344s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99749, Requested 496. Please try again in 3m31.68s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99749, Requested 439. Please try again in 2m42.432s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99749, Requested 369. Please try again in 1m41.952s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 472. Please try again in 3m10.08s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 471. Please try again in 3m9.216s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 369. Please try again in 1m41.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 485. Please try again in 3m21.312s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 439. Please try again in 2m41.567999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 471. Please try again in 3m9.216s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 510. Please try again in 3m42.911999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 455. Please try again in 2m55.392s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 464. Please try again in 3m3.168s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 485. Please try again in 3m21.312s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 389. Please try again in 1m58.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 377. Please try again in 1m48s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 455. Please try again in 2m55.392s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 510. Please try again in 3m42.911999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 522. Please try again in 3m53.28s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 464. Please try again in 3m3.168s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 455. Please try again in 2m55.392s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 510. Please try again in 3m42.911999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 464. Please try again in 3m3.168s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 485. Please try again in 3m21.312s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 377. Please try again in 1m48s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 479. Please try again in 3m16.128s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 377. Please try again in 1m48s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 5): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka4d73smemtbxk93ytgs691k` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 471. Please try again in 3m9.216s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[34]: TimeoutError()\n",
      "Exception raised in Job[32]: TimeoutError()\n",
      "Exception raised in Job[33]: TimeoutError()\n",
      "Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [09:00<04:02, 14.24s/it]Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[38]: TimeoutError()\n",
      "Exception raised in Job[36]: TimeoutError()\n",
      "Exception raised in Job[39]: TimeoutError()\n",
      "Exception raised in Job[37]: TimeoutError()\n",
      "Exception raised in Job[42]: TimeoutError()\n",
      "Exception raised in Job[40]: TimeoutError()\n",
      "Exception raised in Job[44]: TimeoutError()\n",
      "Exception raised in Job[46]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[43]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[45]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 15.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [10:05<00:09,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 22.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 30.0s (attempt 3)\n",
      "[WARN] Rate limit hit. Waiting 37.5s (attempt 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[48]: TimeoutError()\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [12:00<00:00, 14.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Evaluation completed in 12.06 minutes\n",
      "\n",
      "âœ… Evaluation complete! Metrics saved to '../RAG Results/multiquery_rag_metrics.txt'\n",
      "Faithfulness (avg): nan | Answer Relevancy (avg): nan\n",
      "\n",
      "[TIP] To regenerate answers, delete: ../RAG Results/cached_rag_answers.json\n"
     ]
    }
   ],
   "source": [
    "# === Groq + RAG + RAGAS Evaluation ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets groq tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from groq import Groq\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set the API key for Groq\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"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\" abhinav key 1\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "# ==== CONFIG ====\n",
    "# Use the API key already set in previous cell\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "cached_answers_path = \"../RAG Results/cached_rag_answers.json\"  # NEW: Cache file\n",
    "TOP_K = 3\n",
    "\n",
    "GROQ_RAG_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_RAGAS_MODEL = \"llama-3.3-70b-versatile\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Rate limiting config for llama-3.3-70b-versatile:\n",
    "# RPM: 30 (requests per minute)\n",
    "# RPD: 1,000 (requests per day)\n",
    "# TPM: 12,000 (tokens per minute)\n",
    "# TPD: 100,000 (tokens per day)\n",
    "REQUEST_DELAY = 7.5  # seconds between requests (allows ~24 RPM, safe margin below 30 RPM)\n",
    "BATCH_SIZE = 5  # Process in small batches to avoid hitting token limits\n",
    "MAX_RETRIES = 5  # Retry failed requests\n",
    "\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_200 = f.read(200)\n",
    "print(\"First few characters:\\n\", first_200)\n",
    "\n",
    "\n",
    "# ==== 1ï¸âƒ£ Load test data ====\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "\n",
    "# ==== 2ï¸âƒ£ Groq generation with retry logic ====\n",
    "def generate_with_groq(prompt, model_name=GROQ_RAG_MODEL, retries=MAX_RETRIES):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                wait_time = REQUEST_DELAY * (attempt + 2)  # Exponential backoff\n",
    "                print(f\"[WARN] Rate limit hit. Waiting {wait_time}s before retry {attempt + 1}/{retries}\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] Groq API call failed (attempt {attempt + 1}): {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    time.sleep(REQUEST_DELAY)\n",
    "                    return None\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==== 3ï¸âƒ£ Groq wrapper for RAGAS following BaseRagasLLM interface ====\n",
    "from ragas.llms.base import BaseRagasLLM as RagasBaseLLM\n",
    "from ragas.run_config import RunConfig\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "MAX_RPM = 30       # requests per minute\n",
    "MAX_TPM = 12000    # tokens per minute\n",
    "WINDOW = 60        # 60 seconds\n",
    "\n",
    "class GroqRagasLLM(RagasBaseLLM):\n",
    "    \"\"\"Groq LLM wrapper implementing RAGAS BaseRagasLLM interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(run_config=RunConfig())\n",
    "        self.model_name = model_name\n",
    "        self.client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "        \n",
    "        # sliding windows\n",
    "        self.request_times = deque()\n",
    "        self.token_counts = deque()\n",
    "\n",
    "    def _rate_limit_check(self, estimated_tokens=500):\n",
    "        \"\"\"Block until we are below RPM and TPM.\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        # drop events older than 60 seconds\n",
    "        while self.request_times and now - self.request_times[0] > WINDOW:\n",
    "            self.request_times.popleft()\n",
    "\n",
    "        while self.token_counts and now - self.token_counts[0][0] > WINDOW:\n",
    "            self.token_counts.popleft()\n",
    "\n",
    "        # compute current usage in window\n",
    "        current_rpm = len(self.request_times)\n",
    "        current_tpm = sum(tokens for _, tokens in self.token_counts)\n",
    "\n",
    "        # compute wait time if needed\n",
    "        while current_rpm >= MAX_RPM or current_tpm + estimated_tokens >= MAX_TPM:\n",
    "            # sleep until oldest entry expires\n",
    "            oldest_req = self.request_times[0] if self.request_times else now\n",
    "            oldest_tok = self.token_counts[0][0] if self.token_counts else now\n",
    "            wait_until = min(oldest_req, oldest_tok) + WINDOW\n",
    "            sleep_time = max(wait_until - time.time(), 0.1)\n",
    "            print(f\"[RATE LIMIT] Waiting {sleep_time:.1f}s (RPM={current_rpm}, TPM={current_tpm})\")\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "            # refresh window\n",
    "            now = time.time()\n",
    "            while self.request_times and now - self.request_times[0] > WINDOW:\n",
    "                self.request_times.popleft()\n",
    "            while self.token_counts and now - self.token_counts[0][0] > WINDOW:\n",
    "                self.token_counts.popleft()\n",
    "\n",
    "            current_rpm = len(self.request_times)\n",
    "            current_tpm = sum(tokens for _, tokens in self.token_counts)\n",
    "\n",
    "        # Register this request\n",
    "        self.request_times.append(time.time())\n",
    "        self.token_counts.append((time.time(), estimated_tokens))\n",
    "\n",
    "    def _extract_text_from_prompt(self, prompt: PromptValue) -> str:\n",
    "        \"\"\"Extract text from PromptValue object.\"\"\"\n",
    "        # PromptValue has .to_string() method\n",
    "        if hasattr(prompt, \"to_string\"):\n",
    "            return prompt.to_string()\n",
    "        # Fallback to string conversion\n",
    "        return str(prompt)\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Synchronous generation - required by BaseRagasLLM.\"\"\"\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Rate limit check\n",
    "                    estimated_tokens = len(prompt_text.split()) * 1.3\n",
    "                    self._rate_limit_check(int(estimated_tokens))\n",
    "                    \n",
    "                    chat_completion = self.client.chat.completions.create(\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break  # Success\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    async def agenerate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Asynchronous generation - required by BaseRagasLLM.\"\"\"\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Rate limit check\n",
    "                    await asyncio.sleep(REQUEST_DELAY)\n",
    "                    \n",
    "                    # Run blocking SDK call in thread\n",
    "                    chat_completion = await asyncio.to_thread(\n",
    "                        self.client.chat.completions.create,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break  # Success\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            await asyncio.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    def is_finished(self, response: LLMResult) -> bool:\n",
    "        \"\"\"Check if response is complete - required by BaseRagasLLM.\"\"\"\n",
    "        return True\n",
    "\n",
    "\n",
    "# ==== 4ï¸âƒ£ Check collection availability ====\n",
    "try:\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB) is NOT defined.\")\n",
    "    print(\"Please initialize your ChromaDB client/collection before running this cell.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# ==== 5ï¸âƒ£ Generate records with caching and rate limiting ====\n",
    "records = []\n",
    "\n",
    "# Check if cached answers exist\n",
    "if os.path.exists(cached_answers_path):\n",
    "    print(f\"[INFO] Found cached answers at '{cached_answers_path}'\")\n",
    "    try:\n",
    "        with open(cached_answers_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cached_data = json.load(f)\n",
    "        \n",
    "        # Validate cache matches current test data\n",
    "        if len(cached_data) == len(test_data):\n",
    "            questions_match = all(\n",
    "                cached_data[i][\"question\"] == test_data[i][\"question\"] \n",
    "                for i in range(len(test_data))\n",
    "            )\n",
    "            \n",
    "            if questions_match:\n",
    "                print(f\"[INFO] Loading {len(cached_data)} cached answers (skipping generation)\")\n",
    "                records = cached_data\n",
    "            else:\n",
    "                print(\"[WARN] Cached questions don't match test data. Regenerating...\")\n",
    "        else:\n",
    "            print(f\"[WARN] Cache size mismatch ({len(cached_data)} vs {len(test_data)}). Regenerating...\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load cache: {e}. Regenerating...\")\n",
    "\n",
    "# Generate new answers if cache not usable\n",
    "if not records:\n",
    "    print(f\"[INFO] Generating RAG answers with rate limiting (max 30 RPM)...\")\n",
    "    print(f\"[INFO] Request delay: {REQUEST_DELAY}s | Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    for item in tqdm(test_data, desc=\"Generating Groq RAG answers\"):\n",
    "        question = item[\"question\"]\n",
    "        ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "        retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "        retrieved_docs = retrieved[\"documents\"][0]\n",
    "        retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        prompt = (\n",
    "            f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        generated_answer = generate_with_groq(prompt)\n",
    "        if not generated_answer:\n",
    "            generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "        records.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": retrieved_docs,\n",
    "            \"answer\": generated_answer,\n",
    "            \"ground_truth\": ideal_answer,\n",
    "        })\n",
    "        \n",
    "        # Progress update every 5 questions\n",
    "        if len(records) % 5 == 0:\n",
    "            print(f\"[INFO] Processed {len(records)}/{len(test_data)} questions\")\n",
    "    \n",
    "    # Save generated answers to cache\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(cached_answers_path), exist_ok=True)\n",
    "        with open(cached_answers_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[SUCCESS] Cached {len(records)} answers to '{cached_answers_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to save cache: {e}\")\n",
    "\n",
    "\n",
    "# ==== 6ï¸âƒ£ Convert to HF Dataset ====\n",
    "dataset = Dataset.from_list(records)\n",
    "print(f\"[INFO] Created dataset with {len(dataset)} samples\")\n",
    "\n",
    "\n",
    "# ==== 7ï¸âƒ£ Custom HuggingFace Embedding Wrapper ====\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    \"\"\"Implements both sync + async embedding methods for latest RAGAS.\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        # âœ… Do not call super()\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    # --- Sync methods ---\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False).tolist()[0]\n",
    "\n",
    "    # --- Async methods ---\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "\n",
    "# ==== 8ï¸âƒ£ Evaluate with RAGAS ====\n",
    "llm = GroqRagasLLM(GROQ_RAGAS_MODEL)\n",
    "embeddings = CustomHuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "print(f\"\\n[INFO] Starting RAGAS evaluation with {GROQ_RAGAS_MODEL}...\")\n",
    "print(f\"[INFO] Rate limits: 30 RPM | 12K TPM | Using {REQUEST_DELAY}s delays\")\n",
    "print(f\"[INFO] Estimated time: ~{len(dataset) * REQUEST_DELAY / 60:.1f} minutes\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n[SUCCESS] Evaluation completed in {elapsed_time / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "# ==== 9ï¸âƒ£ Save Results ====\n",
    "faithfulness_scores = results[\"faithfulness\"]\n",
    "answer_relevancy_scores = results[\"answer_relevancy\"]\n",
    "\n",
    "# âœ… Compute mean values\n",
    "faithfulness_mean = float(np.mean(faithfulness_scores))\n",
    "answer_relevancy_mean = float(np.mean(answer_relevancy_scores))\n",
    "\n",
    "os.makedirs(os.path.dirname(output_metrics_path), exist_ok=True)\n",
    "\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Groq + RAGAS) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "    f.write(f\"Evaluation Duration: {elapsed_time / 60:.2f} minutes\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GROQ_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GROQ_RAGAS_MODEL}\\n\")\n",
    "    f.write(f\"Rate Limiting: {REQUEST_DELAY}s delay between requests\\n\")\n",
    "    f.write(f\"Cached Answers: {os.path.basename(cached_answers_path)}\\n\\n\")\n",
    "    f.write(f\"Faithfulness (avg): {faithfulness_mean:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy (avg): {answer_relevancy_mean:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness (avg): {faithfulness_mean:.4f} | Answer Relevancy (avg): {answer_relevancy_mean:.4f}\")\n",
    "print(f\"\\n[TIP] To regenerate answers, delete: {cached_answers_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0f49d",
   "metadata": {},
   "source": [
    "5 Groq API keys round robin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Groq + RAG + RAGAS Evaluation ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets groq tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from groq import Groq\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# ============================================================\n",
    "#             ðŸ”¥ 5 GROQ API KEYS - ROUND ROBIN\n",
    "# ============================================================\n",
    "API_KEYS = [\n",
    "    \"API_KEY_1\",\n",
    "    \"API_KEY_2\",\n",
    "    \"API_KEY_3\",\n",
    "    \"API_KEY_4\",\n",
    "    \"API_KEY_5\",\n",
    "]\n",
    "\n",
    "rr_index = 0\n",
    "\n",
    "def get_next_client():\n",
    "    global rr_index\n",
    "    key = API_KEYS[rr_index]\n",
    "    rr_index = (rr_index + 1) % len(API_KEYS)\n",
    "    return Groq(api_key=key)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "# Load first client from round robin\n",
    "groq_client = get_next_client()\n",
    "\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "cached_answers_path = \"../RAG Results/cached_rag_answers.json\"  # NEW: Cache file\n",
    "TOP_K = 3\n",
    "\n",
    "GROQ_RAG_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_RAGAS_MODEL = \"llama-3.3-70b-versatile\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "REQUEST_DELAY = 7.5  \n",
    "BATCH_SIZE = 5  \n",
    "MAX_RETRIES = 5  \n",
    "\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_200 = f.read(200)\n",
    "print(\"First few characters:\\n\", first_200)\n",
    "\n",
    "\n",
    "# ==== 1ï¸âƒ£ Load test data ====\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "\n",
    "# ==== 2ï¸âƒ£ Groq generation with retry logic ====\n",
    "def generate_with_groq(prompt, model_name=GROQ_RAG_MODEL, retries=MAX_RETRIES):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            client = get_next_client()  # ðŸ”¥ USE NEXT API KEY\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                print(f\"[WARN] Rate limit hit. Waiting {wait_time}s before retry {attempt + 1}/{retries}\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] Groq API call failed (attempt {attempt + 1}): {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    time.sleep(REQUEST_DELAY)\n",
    "                    return None\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==== 3ï¸âƒ£ Groq wrapper for RAGAS following BaseRagasLLM interface ====\n",
    "from ragas.llms.base import BaseRagasLLM as RagasBaseLLM\n",
    "from ragas.run_config import RunConfig\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "MAX_RPM = 30       \n",
    "MAX_TPM = 12000    \n",
    "WINDOW = 60        \n",
    "\n",
    "class GroqRagasLLM(RagasBaseLLM):\n",
    "    \"\"\"Groq LLM wrapper implementing RAGAS BaseRagasLLM interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(run_config=RunConfig())\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # ðŸ”¥ USE ROUND ROBIN CLIENT\n",
    "        self.client = get_next_client()\n",
    "        \n",
    "        self.request_times = deque()\n",
    "        self.token_counts = deque()\n",
    "\n",
    "    def _rate_limit_check(self, estimated_tokens=500):\n",
    "        now = time.time()\n",
    "        \n",
    "        while self.request_times and now - self.request_times[0] > WINDOW:\n",
    "            self.request_times.popleft()\n",
    "\n",
    "        while self.token_counts and now - self.token_counts[0][0] > WINDOW:\n",
    "            self.token_counts.popleft()\n",
    "\n",
    "        current_rpm = len(self.request_times)\n",
    "        current_tpm = sum(tokens for _, tokens in self.token_counts)\n",
    "\n",
    "        while current_rpm >= MAX_RPM or current_tpm + estimated_tokens >= MAX_TPM:\n",
    "            oldest_req = self.request_times[0] if self.request_times else now\n",
    "            oldest_tok = self.token_counts[0][0] if self.token_counts else now\n",
    "            wait_until = min(oldest_req, oldest_tok) + WINDOW\n",
    "            sleep_time = max(wait_until - time.time(), 0.1)\n",
    "            print(f\"[RATE LIMIT] Waiting {sleep_time:.1f}s (RPM={current_rpm}, TPM={current_tpm})\")\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "            now = time.time()\n",
    "            while self.request_times and now - self.request_times[0] > WINDOW:\n",
    "                self.request_times.popleft()\n",
    "            while self.token_counts and now - self.token_counts[0][0] > WINDOW:\n",
    "                self.token_counts.popleft()\n",
    "\n",
    "            current_rpm = len(self.request_times)\n",
    "            current_tpm = sum(tokens for _, tokens in self.token_counts)\n",
    "\n",
    "        self.request_times.append(time.time())\n",
    "        self.token_counts.append((time.time(), estimated_tokens))\n",
    "\n",
    "    def _extract_text_from_prompt(self, prompt: PromptValue) -> str:\n",
    "        if hasattr(prompt, \"to_string\"):\n",
    "            return prompt.to_string()\n",
    "        return str(prompt)\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    estimated_tokens = len(prompt_text.split()) * 1.3\n",
    "                    self._rate_limit_check(int(estimated_tokens))\n",
    "                    \n",
    "                    self.client = get_next_client()  # ðŸ”¥ switch API key each call\n",
    "                    \n",
    "                    chat_completion = self.client.chat.completions.create(\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    async def agenerate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    await asyncio.sleep(REQUEST_DELAY)\n",
    "                    \n",
    "                    self.client = get_next_client()  # ðŸ”¥ round robin\n",
    "                    \n",
    "                    chat_completion = await asyncio.to_thread(\n",
    "                        self.client.chat.completions.create,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            await asyncio.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    def is_finished(self, response: LLMResult) -> bool:\n",
    "        return True\n",
    "\n",
    "\n",
    "# ==== 4ï¸âƒ£ Check collection availability ====\n",
    "try:\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB) is NOT defined.\")\n",
    "    print(\"Please initialize your ChromaDB client/collection before running this cell.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# ==== 5ï¸âƒ£ Generate records with caching and rate limiting ====\n",
    "records = []\n",
    "\n",
    "if os.path.exists(cached_answers_path):\n",
    "    print(f\"[INFO] Found cached answers at '{cached_answers_path}'\")\n",
    "    try:\n",
    "        with open(cached_answers_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cached_data = json.load(f)\n",
    "        \n",
    "        if len(cached_data) == len(test_data):\n",
    "            questions_match = all(\n",
    "                cached_data[i][\"question\"] == test_data[i][\"question\"] \n",
    "                for i in range(len(test_data))\n",
    "            )\n",
    "            \n",
    "            if questions_match:\n",
    "                print(f\"[INFO] Loading {len(cached_data)} cached answers (skipping generation)\")\n",
    "                records = cached_data\n",
    "            else:\n",
    "                print(\"[WARN] Cached questions don't match test data. Regenerating...\")\n",
    "        else:\n",
    "            print(f\"[WARN] Cache size mismatch ({len(cached_data)} vs {len(test_data)}). Regenerating...\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load cache: {e}. Regenerating...\")\n",
    "\n",
    "if not records:\n",
    "    print(f\"[INFO] Generating RAG answers with rate limiting (max 30 RPM)...\")\n",
    "    print(f\"[INFO] Request delay: {REQUEST_DELAY}s | Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    for item in tqdm(test_data, desc=\"Generating Groq RAG answers\"):\n",
    "        question = item[\"question\"]\n",
    "        ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "        retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "        retrieved_docs = retrieved[\"documents\"][0]\n",
    "        retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        prompt = (\n",
    "            f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        generated_answer = generate_with_groq(prompt)\n",
    "        if not generated_answer:\n",
    "            generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "        records.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": retrieved_docs,\n",
    "            \"answer\": generated_answer,\n",
    "            \"ground_truth\": ideal_answer,\n",
    "        })\n",
    "        \n",
    "        if len(records) % 5 == 0:\n",
    "            print(f\"[INFO] Processed {len(records)}/{len(test_data)} questions\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(cached_answers_path), exist_ok=True)\n",
    "        with open(cached_answers_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[SUCCESS] Cached {len(records)} answers to '{cached_answers_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to save cache: {e}\")\n",
    "\n",
    "\n",
    "# ==== 6ï¸âƒ£ Convert to HF Dataset ====\n",
    "dataset = Dataset.from_list(records)\n",
    "print(f\"[INFO] Created dataset with {len(dataset)} samples\")\n",
    "\n",
    "\n",
    "# ==== 7ï¸âƒ£ Custom HuggingFace Embedding Wrapper ====\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False).tolist()[0]\n",
    "\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "\n",
    "# ==== 8ï¸âƒ£ Evaluate with RAGAS ====\n",
    "llm = GroqRagasLLM(GROQ_RAGAS_MODEL)\n",
    "embeddings = CustomHuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "print(f\"\\n[INFO] Starting RAGAS evaluation with {GROQ_RAGAS_MODEL}...\")\n",
    "print(f\"[INFO] Rate limits: 30 RPM | 12K TPM | Using {REQUEST_DELAY}s delays\")\n",
    "print(f\"[INFO] Estimated time: ~{len(dataset) * REQUEST_DELAY / 60:.1f} minutes\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n[SUCCESS] Evaluation completed in {elapsed_time / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "# ==== 9ï¸âƒ£ Save Results ====\n",
    "faithfulness_scores = results[\"faithfulness\"]\n",
    "answer_relevancy_scores = results[\"answer_relevancy\"]\n",
    "\n",
    "faithfulness_mean = float(np.mean(faithfulness_scores))\n",
    "answer_relevancy_mean = float(np.mean(answer_relevancy_scores))\n",
    "\n",
    "os.makedirs(os.path.dirname(output_metrics_path), exist_ok=True)\n",
    "\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Groq + RAGAS) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "    f.write(f\"Evaluation Duration: {elapsed_time / 60:.2f} minutes\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GROQ_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GROQ_RAGAS_MODEL}\\n\")\n",
    "    f.write(f\"Rate Limiting: {REQUEST_DELAY}s delay between requests\\n\")\n",
    "    f.write(f\"Cached Answers: {os.path.basename(cached_answers_path)}\\n\\n\")\n",
    "    f.write(f\"Faithfulness (avg): {faithfulness_mean:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy (avg): {answer_relevancy_mean:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness (avg): {faithfulness_mean:.4f} | Answer Relevancy (avg): {answer_relevancy_mean:.4f}\")\n",
    "print(f\"\\n[TIP] To regenerate answers, delete: {cached_answers_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb860d",
   "metadata": {},
   "source": [
    "5 Groq API Round Robin Fashion => With Groq responses stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d40dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Groq + RAG + RAGAS Evaluation ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets groq tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from groq import Groq\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# ============================================================\n",
    "#             ðŸ”¥ 5 GROQ API KEYS - ROUND ROBIN\n",
    "# ============================================================\n",
    "API_KEYS = [\n",
    "    \"API_KEY_1\",\n",
    "    \"API_KEY_2\",\n",
    "    \"API_KEY_3\",\n",
    "    \"API_KEY_4\",\n",
    "    \"API_KEY_5\",\n",
    "]\n",
    "\n",
    "rr_index = 0\n",
    "\n",
    "def get_next_client():\n",
    "    global rr_index\n",
    "    key = API_KEYS[rr_index]\n",
    "    rr_index = (rr_index + 1) % len(API_KEYS)\n",
    "    return Groq(api_key=key)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "# Load first client from round robin\n",
    "groq_client = get_next_client()\n",
    "\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "cached_answers_path = \"../RAG Results/cached_rag_answers.json\"\n",
    "\n",
    "# ðŸ”¥ NEW: Log file to store queries + responses\n",
    "query_response_log_path = \"../RAG Results/query_response_log.txt\"\n",
    "\n",
    "TOP_K = 3\n",
    "\n",
    "GROQ_RAG_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_RAGAS_MODEL = \"llama-3.3-70b-versatile\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "REQUEST_DELAY = 7.5  \n",
    "BATCH_SIZE = 5  \n",
    "MAX_RETRIES = 5  \n",
    "\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_200 = f.read(200)\n",
    "print(\"First few characters:\\n\", first_200)\n",
    "\n",
    "\n",
    "# ==== 1ï¸âƒ£ Load test data ====\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "\n",
    "# ==== 2ï¸âƒ£ Groq generation with retry logic ====\n",
    "def generate_with_groq(prompt, model_name=GROQ_RAG_MODEL, retries=MAX_RETRIES):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            client = get_next_client()\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                print(f\"[WARN] Rate limit hit. Waiting {wait_time}s before retry {attempt + 1}/{retries}\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] Groq API call failed (attempt {attempt + 1}): {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    time.sleep(REQUEST_DELAY)\n",
    "                    return None\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==== 3ï¸âƒ£ Groq wrapper for RAGAS following BaseRagasLLM interface ====\n",
    "from ragas.llms.base import BaseRagasLLM as RagasBaseLLM\n",
    "from ragas.run_config import RunConfig\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "MAX_RPM = 30       \n",
    "MAX_TPM = 12000    \n",
    "WINDOW = 60        \n",
    "\n",
    "class GroqRagasLLM(RagasBaseLLM):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(run_config=RunConfig())\n",
    "        self.model_name = model_name\n",
    "        self.client = get_next_client()\n",
    "        \n",
    "        self.request_times = deque()\n",
    "        self.token_counts = deque()\n",
    "\n",
    "    def _rate_limit_check(self, estimated_tokens=500):\n",
    "        now = time.time()\n",
    "        \n",
    "        while self.request_times and now - self.request_times[0] > WINDOW:\n",
    "            self.request_times.popleft()\n",
    "\n",
    "        while self.token_counts and now - self.token_counts[0][0] > WINDOW:\n",
    "            self.token_counts.popleft()\n",
    "\n",
    "        current_rpm = len(self.request_times)\n",
    "        current_tpm = sum(tokens for _, tokens in self.token_counts)\n",
    "\n",
    "        while current_rpm >= MAX_RPM or current_tpm + estimated_tokens >= MAX_TPM:\n",
    "            oldest_req = self.request_times[0] if self.request_times else now\n",
    "            oldest_tok = self.token_counts[0][0] if self.token_counts else now\n",
    "            wait_until = min(oldest_req, oldest_tok) + WINDOW\n",
    "            sleep_time = max(wait_until - time.time(), 0.1)\n",
    "            print(f\"[RATE LIMIT] Waiting {sleep_time:.1f}s (RPM={current_rpm}, TPM={current_tpm})\")\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "            now = time.time()\n",
    "            while self.request_times and now - self.request_times[0] > WINDOW:\n",
    "                self.request_times.popleft()\n",
    "            while self.token_counts and now - self.token_counts[0][0] > WINDOW:\n",
    "                self.token_counts.popleft()\n",
    "\n",
    "            current_rpm = len(self.request_times)\n",
    "            current_tpm = sum(tokens for _, tokens in self.token_counts)\n",
    "\n",
    "        self.request_times.append(time.time())\n",
    "        self.token_counts.append((time.time(), estimated_tokens))\n",
    "\n",
    "    def _extract_text_from_prompt(self, prompt: PromptValue) -> str:\n",
    "        if hasattr(prompt, \"to_string\"):\n",
    "            return prompt.to_string()\n",
    "        return str(prompt)\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    estimated_tokens = len(prompt_text.split()) * 1.3\n",
    "                    self._rate_limit_check(int(estimated_tokens))\n",
    "                    \n",
    "                    self.client = get_next_client()\n",
    "                    \n",
    "                    chat_completion = self.client.chat.completions.create(\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    async def agenerate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    await asyncio.sleep(REQUEST_DELAY)\n",
    "                    \n",
    "                    self.client = get_next_client()\n",
    "                    \n",
    "                    chat_completion = await asyncio.to_thread(\n",
    "                        self.client.chat.completions.create,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            await asyncio.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    def is_finished(self, response: LLMResult) -> bool:\n",
    "        return True\n",
    "\n",
    "\n",
    "# ==== 4ï¸âƒ£ Check collection availability ====\n",
    "try:\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB) is NOT defined.\")\n",
    "    print(\"Please initialize your ChromaDB client/collection before running this cell.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# ==== 5ï¸âƒ£ Generate records with caching and rate limiting ====\n",
    "records = []\n",
    "\n",
    "if os.path.exists(cached_answers_path):\n",
    "    print(f\"[INFO] Found cached answers at '{cached_answers_path}'\")\n",
    "    try:\n",
    "        with open(cached_answers_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cached_data = json.load(f)\n",
    "        \n",
    "        if len(cached_data) == len(test_data):\n",
    "            questions_match = all(\n",
    "                cached_data[i][\"question\"] == test_data[i][\"question\"] \n",
    "                for i in range(len(test_data))\n",
    "            )\n",
    "            \n",
    "            if questions_match:\n",
    "                print(f\"[INFO] Loading {len(cached_data)} cached answers (skipping generation)\")\n",
    "                records = cached_data\n",
    "            else:\n",
    "                print(\"[WARN] Cached questions don't match test data. Regenerating...\")\n",
    "        else:\n",
    "            print(f\"[WARN] Cache size mismatch ({len(cached_data)} vs {len(test_data)}). Regenerating...\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load cache: {e}. Regenerating...\")\n",
    "\n",
    "if not records:\n",
    "    print(f\"[INFO] Generating RAG answers with rate limiting (max 30 RPM)...\")\n",
    "    print(f\"[INFO] Request delay: {REQUEST_DELAY}s | Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    for item in tqdm(test_data, desc=\"Generating Groq RAG answers\"):\n",
    "        question = item[\"question\"]\n",
    "        ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "        retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "        retrieved_docs = retrieved[\"documents\"][0]\n",
    "        retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        prompt = (\n",
    "            f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        generated_answer = generate_with_groq(prompt)\n",
    "        if not generated_answer:\n",
    "            generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "        # ====================================================\n",
    "        # ðŸ”¥ NEW: APPEND QUERY + CONTEXT + ANSWER TO LOG FILE\n",
    "        # ====================================================\n",
    "        with open(query_response_log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"=== QUERY ===\\n\")\n",
    "            f.write(question + \"\\n\\n\")\n",
    "            f.write(\"=== CONTEXT ===\\n\")\n",
    "            f.write(retrieved_context + \"\\n\\n\")\n",
    "            f.write(\"=== GENERATED ANSWER ===\\n\")\n",
    "            f.write(generated_answer + \"\\n\\n\")\n",
    "            f.write(\"=============================================\\n\\n\")\n",
    "\n",
    "        records.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": retrieved_docs,\n",
    "            \"answer\": generated_answer,\n",
    "            \"ground_truth\": ideal_answer,\n",
    "        })\n",
    "        \n",
    "        if len(records) % 5 == 0:\n",
    "            print(f\"[INFO] Processed {len(records)}/{len(test_data)} questions\")\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(cached_answers_path), exist_ok=True)\n",
    "        with open(cached_answers_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[SUCCESS] Cached {len(records)} answers to '{cached_answers_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to save cache: {e}\")\n",
    "\n",
    "\n",
    "# ==== 6ï¸âƒ£ Convert to HF Dataset ====\n",
    "dataset = Dataset.from_list(records)\n",
    "print(f\"[INFO] Created dataset with {len(dataset)} samples\")\n",
    "\n",
    "\n",
    "# ==== 7ï¸âƒ£ Custom HuggingFace Embedding Wrapper ====\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False).tolist()[0]\n",
    "\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "\n",
    "# ==== 8ï¸âƒ£ Evaluate with RAGAS ====\n",
    "llm = GroqRagasLLM(GROQ_RAGAS_MODEL)\n",
    "embeddings = CustomHuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "print(f\"\\n[INFO] Starting RAGAS evaluation with {GROQ_RAGAS_MODEL}...\")\n",
    "print(f\"[INFO] Rate limits: 30 RPM | 12K TPM | Using {REQUEST_DELAY}s delays\")\n",
    "print(f\"[INFO] Estimated time: ~{len(dataset) * REQUEST_DELAY / 60:.1f} minutes\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n[SUCCESS] Evaluation completed in {elapsed_time / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "# ==== 9ï¸âƒ£ Save Results ====\n",
    "faithfulness_scores = results[\"faithfulness\"]\n",
    "answer_relevancy_scores = results[\"answer_relevancy\"]\n",
    "\n",
    "faithfulness_mean = float(np.mean(faithfulness_scores))\n",
    "answer_relevancy_mean = float(np.mean(answer_relevancy_scores))\n",
    "\n",
    "os.makedirs(os.path.dirname(output_metrics_path), exist_ok=True)\n",
    "\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Groq + RAGAS) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "    f.write(f\"Evaluation Duration: {elapsed_time / 60:.2f} minutes\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GROQ_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GROQ_RAGAS_MODEL}\\n\")\n",
    "    f.write(f\"Rate Limiting: {REQUEST_DELAY}s delay between requests\\n\")\n",
    "    f.write(f\"Cached Answers: {os.path.basename(cached_answers_path)}\\n\\n\")\n",
    "    f.write(f\"Faithfulness (avg): {faithfulness_mean:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy (avg): {answer_relevancy_mean:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness (avg): {faithfulness_mean:.4f} | Answer Relevancy (avg): {answer_relevancy_mean:.4f}\")\n",
    "print(f\"\\n[TIP] To regenerate answers, delete: {cached_answers_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e5eba",
   "metadata": {},
   "source": [
    "5 Groq API calls => trying to handle rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90bbe660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Size: 7364 bytes\n",
      "[INFO] Loaded 37 QA pairs.\n",
      "[INFO] Loading cached answers...\n",
      "[INFO] Running RAGAS evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL LIMIT] Waiting 22.7s...\n",
      "[GLOBAL LIMIT] Waiting 0.5s...\n",
      "[GLOBAL LIMIT] Waiting 0.5s...\n",
      "[GLOBAL LIMIT] Waiting 0.7s...\n",
      "[GLOBAL LIMIT] Waiting 7.0s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|â–         | 1/50 [01:19<1:05:16, 79.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL LIMIT] Waiting 2.3s...\n",
      "[GLOBAL LIMIT] Waiting 4.9s...\n",
      "[GLOBAL LIMIT] Waiting 0.5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [05:36<00:00,  6.74s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "Faithfulness: 0.7504112554112555\n",
      "Answer Relevancy: 0.6912702492121244\n"
     ]
    }
   ],
   "source": [
    "# === Groq + RAG + RAGAS Evaluation (Fully Patched Version) ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets groq tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from datasets import Dataset\n",
    "\n",
    "from groq import Groq\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "from ragas.llms.base import BaseRagasLLM as RagasBaseLLM\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "# ============================================================\n",
    "# NEW: LOG FILE\n",
    "# ============================================================\n",
    "GROQ_LOG_PATH = \"../RAG Results/groq_logs.txt\"\n",
    "\n",
    "def log_groq_call(model, prompt, response):\n",
    "    \"\"\"Append Groq prompt/response to log file.\"\"\"\n",
    "    with open(GROQ_LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n===============================================\\n\")\n",
    "        f.write(f\"TIMESTAMP: {datetime.now()}\\n\")\n",
    "        f.write(f\"MODEL: {model}\\n\\n\")\n",
    "        f.write(\"PROMPT:\\n\")\n",
    "        f.write(prompt + \"\\n\\n\")\n",
    "        f.write(\"RESPONSE:\\n\")\n",
    "        f.write(response + \"\\n\")\n",
    "        f.write(\"===============================================\\n\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¥ 5 GROQ API KEYS - ROUND ROBIN\n",
    "# ============================================================\n",
    "API_KEYS = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "]\n",
    "\n",
    "rr_index = 0\n",
    "def get_next_client():\n",
    "    global rr_index\n",
    "    client = Groq(api_key=API_KEYS[rr_index])\n",
    "    rr_index = (rr_index + 1) % len(API_KEYS)\n",
    "    return client\n",
    "\n",
    "# ============================================================\n",
    "# GLOBAL SAFE RATE LIMITER\n",
    "# ============================================================\n",
    "MAX_RPM = 25\n",
    "WINDOW = 60\n",
    "global_request_times = deque()\n",
    "\n",
    "def wait_for_slot():\n",
    "    now = time.time()\n",
    "\n",
    "    while global_request_times and now - global_request_times[0] > WINDOW:\n",
    "        global_request_times.popleft()\n",
    "\n",
    "    if len(global_request_times) >= MAX_RPM:\n",
    "        sleep_time = WINDOW - (now - global_request_times[0]) + 0.5\n",
    "        print(f\"[GLOBAL LIMIT] Waiting {sleep_time:.1f}s...\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    global_request_times.append(time.time())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "cached_answers_path = \"../RAG Results/cached_rag_answers.json\"\n",
    "\n",
    "TOP_K = 3\n",
    "\n",
    "GROQ_RAG_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_RAGAS_MODEL = \"llama-3.1-8b-instant\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "REQUEST_DELAY = 12.0\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load testbed\n",
    "# ============================================================\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Groq generation helper\n",
    "# ============================================================\n",
    "def generate_with_groq(prompt, model_name=GROQ_RAG_MODEL):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            wait_for_slot()\n",
    "            client = get_next_client()\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                temperature=0.7,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt + \"\\n\\nIMPORTANT: Respond in plain text. No lists. No JSON.\"\n",
    "                }]\n",
    "            )\n",
    "            text = response.choices[0].message.content.strip()\n",
    "\n",
    "            log_groq_call(model_name, prompt, text)\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                sleep_time = REQUEST_DELAY * (attempt + 2)\n",
    "                print(f\"[WARN] Rate limit. Sleeping {sleep_time}s\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] {e}\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Groq wrapper for RAGAS scoring (JSON FORCING FIX APPLIED)\n",
    "# ============================================================\n",
    "class GroqRagasLLM(RagasBaseLLM):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(run_config=RunConfig())\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def _extract_text(self, prompt: PromptValue):\n",
    "        if hasattr(prompt, \"to_string\"):\n",
    "            return prompt.to_string()\n",
    "        return str(prompt)\n",
    "\n",
    "    def generate_text(self, prompt: PromptValue, n=1, temperature=0.0, stop=None, callbacks=None):\n",
    "        raw_prompt = self._extract_text(prompt)\n",
    "\n",
    "        # STRICT JSON WRAPPER â€” FIXES ALL PARSER ERRORS\n",
    "        json_prompt = f\"\"\"\n",
    "You MUST respond ONLY with valid JSON.\n",
    "No explanations. No natural language. No commentary. No markdown.\n",
    "If an array is expected, return an array.\n",
    "If a dictionary is expected, return a dictionary.\n",
    "Your entire response MUST be valid JSON.\n",
    "\n",
    "Task:\n",
    "{raw_prompt}\n",
    "\"\"\"\n",
    "\n",
    "        generations = []\n",
    "\n",
    "        for _ in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    wait_for_slot()\n",
    "                    client = get_next_client()\n",
    "\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                        messages=[{\"role\": \"user\", \"content\": json_prompt}]\n",
    "                    )\n",
    "\n",
    "                    text = response.choices[0].message.content.strip()\n",
    "                    log_groq_call(self.model_name, json_prompt, text)\n",
    "\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break\n",
    "\n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower():\n",
    "                        time.sleep(REQUEST_DELAY)\n",
    "                    else:\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text='{\"error\": \"LLM failed\"}')])\n",
    "                        else:\n",
    "                            time.sleep(REQUEST_DELAY)\n",
    "\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    async def agenerate_text(self, *args, **kwargs):\n",
    "        return self.generate_text(*args, **kwargs)\n",
    "\n",
    "    def is_finished(self, response):\n",
    "        return True\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Generate RAG answers (with caching)\n",
    "# ============================================================\n",
    "records = []\n",
    "\n",
    "if os.path.exists(cached_answers_path):\n",
    "    print(\"[INFO] Loading cached answers...\")\n",
    "    with open(cached_answers_path, \"r\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "if not records:\n",
    "    print(\"[INFO] Generating new RAG answers...\")\n",
    "    for item in tqdm(test_data):\n",
    "        q = item[\"question\"]\n",
    "        gt = item[\"ideal_answer\"]\n",
    "\n",
    "        retrieved = collection.query(query_texts=[q], n_results=TOP_K)\n",
    "        docs = retrieved[\"documents\"][0]\n",
    "        context = \"\\n\".join(docs)\n",
    "\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{q}\\n\\nAnswer:\"\n",
    "        ans = generate_with_groq(prompt)\n",
    "        if not ans:\n",
    "            ans = \"[Error: failed generation]\"\n",
    "\n",
    "        records.append({\n",
    "            \"question\": q,\n",
    "            \"contexts\": docs,\n",
    "            \"answer\": ans,\n",
    "            \"ground_truth\": gt\n",
    "        })\n",
    "\n",
    "    with open(cached_answers_path, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Convert to HF dataset\n",
    "# ============================================================\n",
    "dataset = Dataset.from_list(records)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Embedding wrapper (ASYNC METHODS FIXED)\n",
    "# ============================================================\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False)[0].tolist()\n",
    "\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "embeddings = CustomHuggingfaceEmbeddings(EMBED_MODEL)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run RAGAS evaluation\n",
    "# ============================================================\n",
    "llm = GroqRagasLLM(GROQ_RAGAS_MODEL)\n",
    "\n",
    "print(\"[INFO] Running RAGAS evaluation...\")\n",
    "start = time.time()\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "elapsed = (end - start) / 60\n",
    "\n",
    "faith_avg = float(np.mean(results[\"faithfulness\"]))\n",
    "rel_avg = float(np.mean(results[\"answer_relevancy\"]))\n",
    "\n",
    "with open(output_metrics_path, \"w\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics ===\\n\")\n",
    "    f.write(f\"Time: {datetime.now()}\\n\")\n",
    "    f.write(f\"Duration: {elapsed:.2f} minutes\\n\")\n",
    "    f.write(f\"Faithfulness: {faith_avg:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy: {rel_avg:.4f}\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(\"DONE!\")\n",
    "print(\"Faithfulness:\", faith_avg)\n",
    "print(\"Answer Relevancy:\", rel_avg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raga2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
