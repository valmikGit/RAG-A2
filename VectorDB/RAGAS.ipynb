{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041aa939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChromaDB client initialized at: C:\\Users\\micro\\Desktop\\Abhinav college\\Resources\\Sem 7\\Advanced NLP\\Assignment 2\\RAG-A2\\VectorDB\\chroma_Data_v4\n",
      "[SUCCESS] Loaded collection 'anlp_rag_collection'\n",
      "[INFO] Count: 126\n",
      "[SUCCESS] Loaded collection 'anlp_rag_collection'\n",
      "[INFO] Count: 126\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Path\n",
    "Relative_Database_path = \"./chroma_Data_v4\"\n",
    "Absolute_Database_path = Path(Relative_Database_path).resolve()\n",
    "collection_name = \"anlp_rag_collection\"\n",
    "\n",
    "# Initialize Chroma\n",
    "client = chromadb.PersistentClient(path=str(Absolute_Database_path))\n",
    "print(f\"[INFO] ChromaDB client initialized at: {Absolute_Database_path}\")\n",
    "\n",
    "# Correct embedding function: use model_name (primitive), not a model instance\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Load existing collection\n",
    "collection = client.get_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "print(f\"[SUCCESS] Loaded collection '{collection_name}'\")\n",
    "print(f\"[INFO] Count: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00cd077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Size: 3530 bytes\n",
      "First few characters:\n",
      " [\n",
      "    {\n",
      "        \"question\": \"How does Caesar first enter the play?\",\n",
      "        \"ideal_answer\": \"In a triumphal procession; he has defeated the sons of his deceased rival, Pompey\"\n",
      "    },\n",
      "{\n",
      "\"question\": \"W\n",
      "[INFO] Loaded 25 QA pairs from testbed.\n",
      "[INFO] Found cached answers at '../RAG Results/cached_rag_answers.json'\n",
      "[INFO] Loading 25 cached answers (skipping generation)\n",
      "[INFO] Created dataset with 25 samples\n",
      "[INFO] Found cached answers at '../RAG Results/cached_rag_answers.json'\n",
      "[INFO] Loading 25 cached answers (skipping generation)\n",
      "[INFO] Created dataset with 25 samples\n",
      "\n",
      "[INFO] Starting RAGAS evaluation with llama-3.3-70b-versatile...\n",
      "[INFO] Rate limits: 30 RPM | 12K TPM | Using 2.5s delays\n",
      "[INFO] Estimated time: ~1.0 minutes\n",
      "\n",
      "[INFO] Starting RAGAS evaluation with llama-3.3-70b-versatile...\n",
      "[INFO] Rate limits: 30 RPM | 12K TPM | Using 2.5s delays\n",
      "[INFO] Estimated time: ~1.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|█         | 5/50 [00:07<00:52,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|█▏        | 6/50 [00:18<02:34,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11780, Requested 634. Please try again in 2.07s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11780, Requested 634. Please try again in 2.07s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11986, Requested 455. Please try again in 2.205s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11986, Requested 455. Please try again in 2.205s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11779, Requested 487. Please try again in 1.33s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11779, Requested 487. Please try again in 1.33s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11870, Requested 517. Please try again in 1.935s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11870, Requested 517. Please try again in 1.935s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  14%|█▍        | 7/50 [00:42<06:11,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11787, Requested 667. Please try again in 2.269999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11753, Requested 626. Please try again in 1.894999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|█▌        | 8/50 [00:48<05:25,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  18%|█▊        | 9/50 [00:55<05:14,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11751, Requested 1547. Please try again in 6.489999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11751, Requested 1547. Please try again in 6.489999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[16]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. The task requires analyzing the complexity of each sentence in the answer and breaking it down into fully understandable statements without using pronouns. However, the provided output string contains an error message related to a rate limit being reached for a specific model, which does not comply with the required JSON schema. To fix this, the output should be reformatted according to the specified schema, focusing on generating statements that are free of pronouns and adhere to the given JSON structure. For instance, given the input question and answer, the output should be a JSON object containing an array of statements that are direct, understandable, and pronoun-free, such as: {\"statements\": [\"The subject of the question was a historical figure.\", \"This figure was known for specific achievements.\"]}. Ensuring the output complies with the {\"properties\": {\"statements\": {\"description\": \"The generated statements\", \"items\": {\"type\": \"string\"}, \"title\": \"Statements\", \"type\": \"array\"}}, \"required\": [\"statements\"], \"title\": \"StatementGeneratorOutput\", \"type\": \"object\"} schema is crucial. The error message provided in the output string does not align with these requirements and thus needs to be adjusted to fit the specified format and content guidelines.\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating:  20%|██        | 10/50 [01:03<05:10,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11984, Requested 1492. Please try again in 7.38s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11984, Requested 1492. Please try again in 7.38s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11962, Requested 1544. Please try again in 7.53s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11962, Requested 1544. Please try again in 7.53s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|██▏       | 11/50 [01:09<04:41,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11412, Requested 1733. Please try again in 5.725s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11412, Requested 1733. Please try again in 5.725s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11299, Requested 1764. Please try again in 5.315s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11299, Requested 1764. Please try again in 5.315s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[15]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. The input contains an error message indicating a rate limit has been reached for the model. To generate a question for the given answer and identify if the answer is noncommittal, the actual response is needed, not an error message. However, based on the provided error message, it is not possible to generate a question or determine noncommittal status as the error does not contain a response that can be analyzed. The error message is: {\"error\": {\"message\": \"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11986, Requested 455. Please try again in 2.205s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\", \"type\": \"tokens\", \"code\": \"rate_limit_exceeded\"}}. Given this context, the question could be related to the error itself or the need to wait before making another request, but without a clear answer, the noncommittal status cannot be accurately determined. For the purpose of this exercise, let's assume a hypothetical scenario where the question could be about the error or the wait time, and the answer provided in the error message is considered noncommittal because it does not directly address a specific query but rather provides a reason for not being able to process the request.\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating:  24%|██▍       | 12/50 [01:17<04:48,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11610, Requested 939. Please try again in 2.745s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11746, Requested 1836. Please try again in 7.91s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11746, Requested 1836. Please try again in 7.91s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11599, Requested 1394. Please try again in 4.965s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11599, Requested 1394. Please try again in 4.965s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11333, Requested 796. Please try again in 644.999999ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11333, Requested 796. Please try again in 644.999999ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11228, Requested 994. Please try again in 1.109999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11228, Requested 994. Please try again in 1.109999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11456, Requested 1479. Please try again in 4.675s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11944, Requested 1039. Please try again in 4.915s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11456, Requested 1479. Please try again in 4.675s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11944, Requested 1039. Please try again in 4.915s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|██▌       | 13/50 [01:26<04:48,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[11]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. Fix the output string and return it.\\nPlease return the output in a JSON format that complies with the following schema as specified in JSON Schema: \\{\\\"properties\\\": {\\\"text\\\": {\\\"title\\\": \\\"Text\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"text\\\"], \\\"title\\\": \\\"StringIO\\\", \\\"type\\\": \\\"object\\\"}\\}\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating:  28%|██▊       | 14/50 [01:37<05:14,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|███       | 15/50 [01:45<04:57,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11099, Requested 1329. Please try again in 2.14s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11099, Requested 1329. Please try again in 2.14s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11870, Requested 1842. Please try again in 8.56s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11870, Requested 1842. Please try again in 8.56s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  32%|███▏      | 16/50 [01:55<05:04,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  34%|███▍      | 17/50 [02:03<04:46,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11104, Requested 1305. Please try again in 2.045s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11104, Requested 1305. Please try again in 2.045s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[6]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  36%|███▌      | 18/50 [02:12<04:41,  8.81s/it]Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[6]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  36%|███▌      | 18/50 [02:12<04:41,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11910, Requested 1438. Please try again in 6.739999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11816, Requested 1960. Please try again in 8.879999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11714, Requested 1600. Please try again in 6.57s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11816, Requested 1960. Please try again in 8.879999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11714, Requested 1600. Please try again in 6.57s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[4]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  38%|███▊      | 19/50 [02:16<03:53,  7.54s/it]Exception raised in Job[4]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Evaluating:  38%|███▊      | 19/50 [02:16<03:53,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11802, Requested 1992. Please try again in 8.969999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11765, Requested 2010. Please try again in 8.875s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|████      | 20/50 [02:21<03:20,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11549, Requested 1818. Please try again in 6.835s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  42%|████▏     | 21/50 [02:33<04:02,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11176, Requested 1926. Please try again in 5.51s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11176, Requested 1926. Please try again in 5.51s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|████▍     | 22/50 [02:38<03:20,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11430, Requested 2186. Please try again in 8.08s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11789, Requested 2225. Please try again in 10.07s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11789, Requested 2225. Please try again in 10.07s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11476, Requested 1344. Please try again in 4.099999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11476, Requested 1344. Please try again in 4.099999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10701, Requested 2307. Please try again in 5.04s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10701, Requested 2307. Please try again in 5.04s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  46%|████▌     | 23/50 [02:52<04:12,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10843, Requested 1784. Please try again in 3.135s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10843, Requested 1784. Please try again in 3.135s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating:  48%|████▊     | 24/50 [03:00<03:47,  8.77s/it]Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[14]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Evaluating:  48%|████▊     | 24/50 [03:00<03:47,  8.77s/it]Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[14]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Exception raised in Job[12]: TimeoutError()\n",
      "Exception raised in Job[13]: TimeoutError()\n",
      "Evaluating:  60%|██████    | 30/50 [03:00<00:49,  2.47s/it]Exception raised in Job[12]: TimeoutError()\n",
      "Exception raised in Job[13]: TimeoutError()\n",
      "Evaluating:  60%|██████    | 30/50 [03:00<00:49,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11378, Requested 2311. Please try again in 8.444999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[18]: TimeoutError()\n",
      "Evaluating:  64%|██████▍   | 32/50 [03:06<00:48,  2.69s/it]Exception raised in Job[20]: TimeoutError()\n",
      "Exception raised in Job[20]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  68%|██████▊   | 34/50 [03:10<00:39,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11803, Requested 1472. Please try again in 6.375s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11803, Requested 1472. Please try again in 6.375s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11834, Requested 2038. Please try again in 9.36s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11834, Requested 2038. Please try again in 9.36s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11918, Requested 471. Please try again in 1.945s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11918, Requested 471. Please try again in 1.945s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  70%|███████   | 35/50 [03:33<01:26,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11882, Requested 472. Please try again in 1.77s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11802, Requested 471. Please try again in 1.365s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11802, Requested 471. Please try again in 1.365s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11871, Requested 522. Please try again in 1.965s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11877, Requested 510. Please try again in 1.935s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11871, Requested 522. Please try again in 1.965s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11877, Requested 510. Please try again in 1.935s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11784, Requested 510. Please try again in 1.47s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11784, Requested 510. Please try again in 1.47s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  72%|███████▏  | 36/50 [03:39<01:22,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  74%|███████▍  | 37/50 [03:40<01:02,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11594, Requested 1714. Please try again in 6.54s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[22]: TimeoutError()\n",
      "Evaluating:  76%|███████▌  | 38/50 [03:42<00:50,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[42]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. The correct output should be in a JSON format that complies with the specified schema. The input string should be analyzed and broken down into fully understandable statements without pronouns. The output should be formatted according to the provided JSON schema.\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating:  78%|███████▊  | 39/50 [03:44<00:40,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[39]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. The task requires generating a question for the given answer and identifying if the answer is noncommittal. The provided answer is: \"The provided text does not explicitly mention the news Brutus and Cassius receive from Rome. However, based on the context of Act 4, Scene 3, it appears that they receive some unfavorable news or reports, as Cassius mentions that Brutus has wronged him by condemning Lucius Pella for taking bribes, despite Cassius' letters in his favor. This suggests that Brutus and Cassius may have received news of discontent or disapproval from Rome, but the exact details are not specified in the given text.\" The question for this answer could be: \"What news do Brutus and Cassius receive from Rome in Act 4, Scene 3?\" The answer is noncommittal because it does not provide a clear or direct response to the question, instead offering an interpretation based on context. Therefore, the noncommittal value is 1.\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating:  80%|████████  | 40/50 [03:49<00:38,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11833, Requested 1776. Please try again in 8.045s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[24]: TimeoutError()\n",
      "Evaluating:  82%|████████▏ | 41/50 [03:55<00:40,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[45]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. The correct output should be in the following format: {\"question\": \"What is Cassius's response to Brutus's actions?\", \"noncommittal\": 0}. The given answer is committal as it provides a clear explanation of Cassius's response, therefore noncommittal is 0.\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating:  84%|████████▍ | 42/50 [03:59<00:33,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n",
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  86%|████████▌ | 43/50 [04:06<00:36,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[26]: TimeoutError()\n",
      "Evaluating:  88%|████████▊ | 44/50 [04:09<00:26,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[44]: OutputParserException(Invalid json output: The output string did not satisfy the constraints given in the prompt. The correct output should be in a JSON format that complies with the specified schema. The input string contains an error message indicating a rate limit exceeded for the model \"llama-3.3-70b-versatile\" in organization \"org_01k9ys8bswepfshkmm14afrk4c\" service tier \"on_demand\" on tokens per minute (TPM). The error message suggests trying again in 1.965 seconds or upgrading to the Dev Tier for more tokens.\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE )\n",
      "Evaluating:  90%|█████████ | 45/50 [04:15<00:24,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11333, Requested 1515. Please try again in 4.24s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  92%|█████████▏| 46/50 [04:24<00:24,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed (attempt 3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k9ys8bswepfshkmm14afrk4c` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11485, Requested 1914. Please try again in 6.995s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n",
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  94%|█████████▍| 47/50 [04:34<00:21,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 7.5s (attempt 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[30]: TimeoutError()\n",
      "Evaluating:  98%|█████████▊| 49/50 [04:47<00:06,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Rate limit hit. Waiting 5.0s (attempt 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[32]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 50/50 [05:03<00:00,  6.06s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Evaluation completed in 5.17 minutes\n",
      "\n",
      "✅ Evaluation complete! Metrics saved to '../RAG Results/multiquery_rag_metrics.txt'\n",
      "Faithfulness (avg): nan | Answer Relevancy (avg): nan\n",
      "\n",
      "[TIP] To regenerate answers, delete: ../RAG Results/cached_rag_answers.json\n"
     ]
    }
   ],
   "source": [
    "# === Groq + RAG + RAGAS Evaluation ===\n",
    "# Prereqs:\n",
    "# pip install ragas datasets groq tqdm sentence-transformers numpy\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from groq import Groq\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas.embeddings.base import HuggingfaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set the API key for Groq\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "# ==== CONFIG ====\n",
    "# Use the API key already set in previous cell\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "testbed_path = \"../RAG Results/test_bed.json\"\n",
    "output_metrics_path = \"../RAG Results/multiquery_rag_metrics.txt\"\n",
    "cached_answers_path = \"../RAG Results/cached_rag_answers.json\"  # NEW: Cache file\n",
    "TOP_K = 3\n",
    "\n",
    "GROQ_RAG_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GROQ_RAGAS_MODEL = \"llama-3.3-70b-versatile\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Rate limiting config for llama-3.3-70b-versatile:\n",
    "# RPM: 30 (requests per minute)\n",
    "# RPD: 1,000 (requests per day)\n",
    "# TPM: 12,000 (tokens per minute)\n",
    "# TPD: 100,000 (tokens per day)\n",
    "REQUEST_DELAY = 2.5  # seconds between requests (allows ~24 RPM, safe margin below 30 RPM)\n",
    "BATCH_SIZE = 5  # Process in small batches to avoid hitting token limits\n",
    "MAX_RETRIES = 3  # Retry failed requests\n",
    "\n",
    "print(\"Exists:\", os.path.exists(testbed_path))\n",
    "print(\"Size:\", os.path.getsize(testbed_path), \"bytes\")\n",
    "\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_200 = f.read(200)\n",
    "print(\"First few characters:\\n\", first_200)\n",
    "\n",
    "\n",
    "# ==== 1️⃣ Load test data ====\n",
    "with open(testbed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(test_data)} QA pairs from testbed.\")\n",
    "\n",
    "\n",
    "# ==== 2️⃣ Groq generation with retry logic ====\n",
    "def generate_with_groq(prompt, model_name=GROQ_RAG_MODEL, retries=MAX_RETRIES):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            chat_completion = groq_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                wait_time = REQUEST_DELAY * (attempt + 2)  # Exponential backoff\n",
    "                print(f\"[WARN] Rate limit hit. Waiting {wait_time}s before retry {attempt + 1}/{retries}\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] Groq API call failed (attempt {attempt + 1}): {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    time.sleep(REQUEST_DELAY)\n",
    "                    return None\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==== 3️⃣ Groq wrapper for RAGAS following BaseRagasLLM interface ====\n",
    "from ragas.llms.base import BaseRagasLLM as RagasBaseLLM\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "class GroqRagasLLM(RagasBaseLLM):\n",
    "    \"\"\"Groq LLM wrapper implementing RAGAS BaseRagasLLM interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(run_config=RunConfig())\n",
    "        self.model_name = model_name\n",
    "        self.client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "        self.request_count = 0\n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def _rate_limit_check(self):\n",
    "        \"\"\"Ensure we don't exceed rate limits\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        \n",
    "        # Ensure minimum delay between requests\n",
    "        if time_since_last < REQUEST_DELAY:\n",
    "            sleep_time = REQUEST_DELAY - time_since_last\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "        self.request_count += 1\n",
    "\n",
    "    def _extract_text_from_prompt(self, prompt: PromptValue) -> str:\n",
    "        \"\"\"Extract text from PromptValue object.\"\"\"\n",
    "        # PromptValue has .to_string() method\n",
    "        if hasattr(prompt, \"to_string\"):\n",
    "            return prompt.to_string()\n",
    "        # Fallback to string conversion\n",
    "        return str(prompt)\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Synchronous generation - required by BaseRagasLLM.\"\"\"\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Rate limit check\n",
    "                    self._rate_limit_check()\n",
    "                    \n",
    "                    chat_completion = self.client.chat.completions.create(\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break  # Success\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    async def agenerate_text(\n",
    "        self,\n",
    "        prompt: PromptValue,\n",
    "        n: int = 1,\n",
    "        temperature: float = 0.01,\n",
    "        stop=None,\n",
    "        callbacks=None,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Asynchronous generation - required by BaseRagasLLM.\"\"\"\n",
    "        prompt_text = self._extract_text_from_prompt(prompt)\n",
    "        generations = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    # Rate limit check\n",
    "                    await asyncio.sleep(REQUEST_DELAY)\n",
    "                    \n",
    "                    # Run blocking SDK call in thread\n",
    "                    chat_completion = await asyncio.to_thread(\n",
    "                        self.client.chat.completions.create,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                        model=self.model_name,\n",
    "                        temperature=temperature,\n",
    "                    )\n",
    "                    \n",
    "                    text = chat_completion.choices[0].message.content.strip()\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break  # Success\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"rate_limit\" in str(e).lower() and attempt < MAX_RETRIES - 1:\n",
    "                        wait_time = REQUEST_DELAY * (attempt + 2)\n",
    "                        print(f\"[WARN] Rate limit hit. Waiting {wait_time}s (attempt {attempt + 1})\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"[ERROR] Failed (attempt {attempt + 1}): {e}\")\n",
    "                        if attempt == MAX_RETRIES - 1:\n",
    "                            generations.append([Generation(text=f\"[Error: {e}]\")])\n",
    "                        else:\n",
    "                            await asyncio.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "    def is_finished(self, response: LLMResult) -> bool:\n",
    "        \"\"\"Check if response is complete - required by BaseRagasLLM.\"\"\"\n",
    "        return True\n",
    "\n",
    "\n",
    "# ==== 4️⃣ Check collection availability ====\n",
    "try:\n",
    "    collection.query(query_texts=[\"test\"], n_results=1)\n",
    "except NameError:\n",
    "    print(\"\\n[CRITICAL WARNING] The 'collection' object (ChromaDB) is NOT defined.\")\n",
    "    print(\"Please initialize your ChromaDB client/collection before running this cell.\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# ==== 5️⃣ Generate records with caching and rate limiting ====\n",
    "records = []\n",
    "\n",
    "# Check if cached answers exist\n",
    "if os.path.exists(cached_answers_path):\n",
    "    print(f\"[INFO] Found cached answers at '{cached_answers_path}'\")\n",
    "    try:\n",
    "        with open(cached_answers_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cached_data = json.load(f)\n",
    "        \n",
    "        # Validate cache matches current test data\n",
    "        if len(cached_data) == len(test_data):\n",
    "            questions_match = all(\n",
    "                cached_data[i][\"question\"] == test_data[i][\"question\"] \n",
    "                for i in range(len(test_data))\n",
    "            )\n",
    "            \n",
    "            if questions_match:\n",
    "                print(f\"[INFO] Loading {len(cached_data)} cached answers (skipping generation)\")\n",
    "                records = cached_data\n",
    "            else:\n",
    "                print(\"[WARN] Cached questions don't match test data. Regenerating...\")\n",
    "        else:\n",
    "            print(f\"[WARN] Cache size mismatch ({len(cached_data)} vs {len(test_data)}). Regenerating...\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load cache: {e}. Regenerating...\")\n",
    "\n",
    "# Generate new answers if cache not usable\n",
    "if not records:\n",
    "    print(f\"[INFO] Generating RAG answers with rate limiting (max 30 RPM)...\")\n",
    "    print(f\"[INFO] Request delay: {REQUEST_DELAY}s | Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    for item in tqdm(test_data, desc=\"Generating Groq RAG answers\"):\n",
    "        question = item[\"question\"]\n",
    "        ideal_answer = item[\"ideal_answer\"]\n",
    "\n",
    "        retrieved = collection.query(query_texts=[question], n_results=TOP_K)\n",
    "        retrieved_docs = retrieved[\"documents\"][0]\n",
    "        retrieved_context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        prompt = (\n",
    "            f\"Context:\\n{retrieved_context}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        generated_answer = generate_with_groq(prompt)\n",
    "        if not generated_answer:\n",
    "            generated_answer = f\"[Fallback mock answer] Context excerpt: {retrieved_docs[0][:150]}...\"\n",
    "\n",
    "        records.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": retrieved_docs,\n",
    "            \"answer\": generated_answer,\n",
    "            \"ground_truth\": ideal_answer,\n",
    "        })\n",
    "        \n",
    "        # Progress update every 5 questions\n",
    "        if len(records) % 5 == 0:\n",
    "            print(f\"[INFO] Processed {len(records)}/{len(test_data)} questions\")\n",
    "    \n",
    "    # Save generated answers to cache\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(cached_answers_path), exist_ok=True)\n",
    "        with open(cached_answers_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"[SUCCESS] Cached {len(records)} answers to '{cached_answers_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to save cache: {e}\")\n",
    "\n",
    "\n",
    "# ==== 6️⃣ Convert to HF Dataset ====\n",
    "dataset = Dataset.from_list(records)\n",
    "print(f\"[INFO] Created dataset with {len(dataset)} samples\")\n",
    "\n",
    "\n",
    "# ==== 7️⃣ Custom HuggingFace Embedding Wrapper ====\n",
    "class CustomHuggingfaceEmbeddings(HuggingfaceEmbeddings):\n",
    "    \"\"\"Implements both sync + async embedding methods for latest RAGAS.\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        # ✅ Do not call super()\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    # --- Sync methods ---\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, show_progress_bar=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], show_progress_bar=False).tolist()[0]\n",
    "\n",
    "    # --- Async methods ---\n",
    "    async def aembed_documents(self, texts):\n",
    "        return self.embed_documents(texts)\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        return self.embed_query(text)\n",
    "\n",
    "\n",
    "# ==== 8️⃣ Evaluate with RAGAS ====\n",
    "llm = GroqRagasLLM(GROQ_RAGAS_MODEL)\n",
    "embeddings = CustomHuggingfaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "print(f\"\\n[INFO] Starting RAGAS evaluation with {GROQ_RAGAS_MODEL}...\")\n",
    "print(f\"[INFO] Rate limits: 30 RPM | 12K TPM | Using {REQUEST_DELAY}s delays\")\n",
    "print(f\"[INFO] Estimated time: ~{len(dataset) * REQUEST_DELAY / 60:.1f} minutes\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n[SUCCESS] Evaluation completed in {elapsed_time / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "# ==== 9️⃣ Save Results ====\n",
    "faithfulness_scores = results[\"faithfulness\"]\n",
    "answer_relevancy_scores = results[\"answer_relevancy\"]\n",
    "\n",
    "# ✅ Compute mean values\n",
    "faithfulness_mean = float(np.mean(faithfulness_scores))\n",
    "answer_relevancy_mean = float(np.mean(answer_relevancy_scores))\n",
    "\n",
    "os.makedirs(os.path.dirname(output_metrics_path), exist_ok=True)\n",
    "\n",
    "with open(output_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== RAG Evaluation Metrics (Groq + RAGAS) ===\\n\")\n",
    "    f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "    f.write(f\"Evaluation Duration: {elapsed_time / 60:.2f} minutes\\n\\n\")\n",
    "    f.write(f\"RAG Generation Model: {GROQ_RAG_MODEL}\\n\")\n",
    "    f.write(f\"RAGAS Evaluation Model: {GROQ_RAGAS_MODEL}\\n\")\n",
    "    f.write(f\"Rate Limiting: {REQUEST_DELAY}s delay between requests\\n\")\n",
    "    f.write(f\"Cached Answers: {os.path.basename(cached_answers_path)}\\n\\n\")\n",
    "    f.write(f\"Faithfulness (avg): {faithfulness_mean:.4f}\\n\")\n",
    "    f.write(f\"Answer Relevancy (avg): {answer_relevancy_mean:.4f}\\n\\n\")\n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(str(results))\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete! Metrics saved to '{output_metrics_path}'\")\n",
    "print(f\"Faithfulness (avg): {faithfulness_mean:.4f} | Answer Relevancy (avg): {answer_relevancy_mean:.4f}\")\n",
    "print(f\"\\n[TIP] To regenerate answers, delete: {cached_answers_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
